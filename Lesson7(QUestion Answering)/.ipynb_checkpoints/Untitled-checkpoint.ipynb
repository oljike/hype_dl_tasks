{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import nltk\n",
    "import copy\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "code_folding": [
     0,
     38,
     50,
     57,
     65
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, counter, sos, eos, pad, unk, min_freq=None):\n",
    "        self.sos = sos\n",
    "        self.eos = eos\n",
    "        self.pad = pad\n",
    "        self.unk = unk\n",
    "        \n",
    "        self.pad_idx = 0\n",
    "        self.unk_idx = 1\n",
    "\n",
    "        \n",
    "        self._token2idx = {\n",
    "            self.pad: self.pad_idx,\n",
    "            self.unk: self.unk_idx,\n",
    "        }\n",
    "        self._idx2token = {idx:token for token, idx in self._token2idx.items()}\n",
    "        \n",
    "        idx = len(self._token2idx)\n",
    "        min_freq = 0 if min_freq is None else min_freq\n",
    "        \n",
    "        for token, count in counter.items():\n",
    "            if count > min_freq:\n",
    "                self._token2idx[token] = idx\n",
    "                self._idx2token[idx]   = token\n",
    "                idx += 1\n",
    "        \n",
    "        self.vocab_size = len(self._token2idx)\n",
    "        self.tokens     = list(self._token2idx.keys())\n",
    "    \n",
    "    def token2idx(self, token):\n",
    "        return self._token2idx.get(token, self.pad_idx)\n",
    "    \n",
    "    def idx2token(self, idx):\n",
    "        return self._idx2token.get(idx, self.pad)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token2idx)\n",
    "\n",
    "def pad_contexts(list_):\n",
    "    \n",
    "    length_dict = {key: len(value) for key, value in d.items()}\n",
    "    \n",
    "def pad_many_seq(sequences, pad_idx, max_length):\n",
    "    '''\n",
    "    Inputs:\n",
    "        sequences: list of list of tokens\n",
    "    '''    \n",
    "    return [ seq + [pad_idx]*(max_length - len(seq)) for seq in sequences]\n",
    "\n",
    "\n",
    "def pad_single_seq(sequence, pad_idx, max_length):\n",
    "    '''\n",
    "    Inputs:\n",
    "        sequence: list of tokens\n",
    "    '''    \n",
    "    return sequence + [pad_idx]*(max_length - len(sequence))\n",
    "\n",
    "def pad_body(sequences, pad_idx, max_length, max_seq_length):\n",
    "    '''\n",
    "    Inputs:\n",
    "        sequences: list of list of tokens\n",
    "    '''    \n",
    "    return sequences + [torch.zeros(max_seq_length)] * (max_length - len(sequences))\n",
    "    \n",
    "    \n",
    "def words_tokenize(line):\n",
    "    return  nltk.word_tokenize(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, path, val):\n",
    "        \n",
    "        \n",
    "        shuffle  = True\n",
    "        self.val = val\n",
    "        self.data = []\n",
    "        primal_data = []\n",
    "        context = []\n",
    "\n",
    "        dict_cell = {}\n",
    "      \n",
    "        with open(path) as f:\n",
    "            file_content = f.readlines()\n",
    "            for x in file_content:\n",
    "\n",
    "                line = nltk.word_tokenize(x)\n",
    "               \n",
    "                if line[0] == '1':\n",
    "                    del context\n",
    "                    context = []\n",
    "\n",
    "                if '?' in line:\n",
    "                    dict_cell['question'] = line[1:line.index('?') + 1]\n",
    "                    dict_cell['context'] =  copy.deepcopy(context)\n",
    "                    dict_cell['answer'] = line[line.index('?') + 1]\n",
    "                    primal_data.append(dict_cell)\n",
    "                    del dict_cell\n",
    "                    dict_cell = {}\n",
    "                else:\n",
    "                    context.append(line[1:])\n",
    "         \n",
    "        words_counter = Counter()\n",
    "        \n",
    "        for cell in primal_data:\n",
    "            \n",
    "            for context in cell['context']:\n",
    "                for token in context:\n",
    "                    words_counter[token] += 1\n",
    "            for token in cell['question']:\n",
    "                words_counter[token] += 1\n",
    "            \n",
    "            words_counter[cell['answer']] += 1\n",
    "                \n",
    "        sos = \"<sos>\"\n",
    "        eos = \"<eos>\"\n",
    "        pad = \"<pad>\"\n",
    "        unk = \"<unk>\"\n",
    "\n",
    "        self.words_vocab = Vocab(words_counter, \n",
    "                            sos, eos, pad, unk)\n",
    "\n",
    " \n",
    "        if not val:\n",
    "            random.shuffle(primal_data)\n",
    "        \n",
    "        for cell in primal_data:\n",
    "\n",
    "            cell_context = [[self.words_vocab.token2idx(item) for item in cntx] for cntx in cell['context']]         \n",
    "            cell_question = [self.words_vocab.token2idx(item) for item in cell['question']]\n",
    "            cell_answer = [self.words_vocab.token2idx(cell['answer'])]# for item in cell['answer']]\n",
    "            self.data.append((cell_context, cell_question, cell_answer))\n",
    "                \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def get_batch(self, batch_size, sort = False):\n",
    "        \n",
    "        random_ids = np.random.randint(0, len(self.data), batch_size)\n",
    "        if not self.val:\n",
    "            batch_data = [self.data[idx] for idx in random_ids]\n",
    "        else:\n",
    "            batch_data = self.data\n",
    "        \n",
    "        max_context_length = max([max(map(len, a)) for (a, _, _) in batch_data])\n",
    "        max_question_length = max([len(b) for (_, b, _) in batch_data])\n",
    "        max_contexts_length = max([len(a) for (a, _, _) in batch_data])\n",
    "\n",
    "        contexts = []\n",
    "        questions = []\n",
    "        answers = []\n",
    "        for a, b, c in batch_data:\n",
    "\n",
    "            cell_context = pad_many_seq(a, self.words_vocab.pad_idx, max_context_length)\n",
    "            cell_context = pad_body(cell_context, self.words_vocab.pad_idx, max_contexts_length, max_context_length)\n",
    "            cell_question = pad_single_seq(b, self.words_vocab.pad_idx, max_question_length)  \n",
    "            \n",
    "            cell_context = torch.LongTensor(cell_context).to(device)\n",
    "            cell_question = torch.LongTensor(cell_question).to(device)\n",
    "            cell_answer = torch.LongTensor(c).to(device)\n",
    "            \n",
    "            contexts.append(cell_context)\n",
    "            questions.append(cell_question)\n",
    "            answers.append(cell_answer)\n",
    "            \n",
    "\n",
    "        contexts = torch.stack(contexts, 0)\n",
    "        questions = torch.stack(questions, 0)      \n",
    "        answers = torch.stack(answers, 0).squeeze(1)\n",
    "\n",
    "        return contexts, questions, answers\n",
    "\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.data[idx]['context'], self.data[idx]['question'], self.data[idx]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "code_folding": [
     10,
     22,
     25,
     57,
     62
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_task(data_dir, task_id, only_supporting=False):\n",
    "    assert task_id > 0 and task_id <= 20\n",
    "    files = os.listdir(data_dir)\n",
    "    files = [os.path.join(data_dir, f) for f in files]\n",
    "    s = \"qa{}_\".format(task_id)\n",
    "    train_file = [f for f in files if s in f and 'train' in f][0]\n",
    "    test_file  = [f for f in files if s in f and 'test'  in f][0]\n",
    "    train_data = get_stories(train_file, only_supporting)\n",
    "    test_data  = get_stories(test_file,  only_supporting)\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    return [x.strip() for x in re.split(\"(\\W+)?\", sent) if x.strip()]\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    data  = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        nid, line = line.lower().split(\" \", 1)\n",
    "        nid  = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line: #question\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            a = [a]\n",
    "            substory = None\n",
    "            if q[-1] == \"?\":\n",
    "                q = q[:-1]\n",
    "            \n",
    "            if only_supporting:\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory   = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                substory   = [x for x in story if x]\n",
    "            \n",
    "            data.append((substory, q, a))\n",
    "            story.append(\"\")\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            if sent[-1] == '.':\n",
    "                sent = sent[:-1]\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False):\n",
    "    with open(f) as f:\n",
    "        return parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    \n",
    "    \n",
    "def vectorize_data(data, word_idx, sentence_size, memory_size):\n",
    "    S, Q, A = [], [], []\n",
    "    for story, query, answer in data:\n",
    "        ss = []\n",
    "        for i, sentence in enumerate(story, 1):\n",
    "            ls = max(0, sentence_size - len(sentence))\n",
    "            ss.append([word_idx[w] for w in sentence] + [0] * ls)\n",
    "        \n",
    "        ss = ss[::-1][:memory_size][::-1]\n",
    "        \n",
    "        for i in range(len(ss)):\n",
    "            ss[i][-1] = len(word_idx) - memory_size - i + len(ss)\n",
    "        \n",
    "        lm = max(0, memory_size - len(ss))\n",
    "        for _ in range(lm):\n",
    "            ss.append([0] * sentence_size)\n",
    "        \n",
    "        lq = max(0, sentence_size - len(query))\n",
    "        q = [word_idx[w] for w in query] + [0] * lq\n",
    "        \n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        for a in answer:\n",
    "            y[word_idx[a]] = 1\n",
    "        \n",
    "        S.append(ss)\n",
    "        Q.append(q)\n",
    "        A.append(y)\n",
    "    return np.array(S), np.array(Q), np.array(A)\n",
    "\n",
    "class bAbIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_dir, task_id=1, memory_size=50, train=True):\n",
    "        self.train       = train\n",
    "        self.task_id     = task_id\n",
    "        self.dataset_dir = dataset_dir\n",
    "        \n",
    "        train_data, test_data = load_task(self.dataset_dir, task_id)\n",
    "        data = train_data + test_data\n",
    "        \n",
    "        self.vocab = set([])\n",
    "        for story, query, answer in data:\n",
    "            self.vocab = self.vocab | set(list(chain.from_iterable(story)) + query + answer)\n",
    "        self.vocab = sorted(self.vocab)\n",
    "        word_idx = {word:i+1 for i, word in enumerate(self.vocab)}\n",
    "        \n",
    "        self.max_story_size = max([len(story) for story, _, _ in data])\n",
    "        self.query_size     = max([len(query) for _, query, _ in data])\n",
    "        self.sentence_size  = max([len(row) for row in chain.from_iterable([story for story, _, _ in data])])\n",
    "        self.memory_size    = max(memory_size, self.max_story_size)\n",
    "        \n",
    "        for i in range(self.memory_size):\n",
    "            word_idx[\"time{}\".format(i+1)] = \"time{}\".format(i + 1)\n",
    "        \n",
    "        self.num_vocab = len(word_idx)\n",
    "        self.sentence_size = max(self.sentence_size, self.query_size)\n",
    "        self.sentence_size += 1\n",
    "        self.word_idx      = word_idx\n",
    "        \n",
    "        self.mean_story_size = int(np.mean([len(s) for s, _, _ in data]))\n",
    "        \n",
    "        if train:\n",
    "            story, query, answer = vectorize_data(train_data, self.word_idx, self.sentence_size, self.memory_size)\n",
    "        else:\n",
    "            story, query, answer = vectorize_data(test_data, self.word_idx, self.sentence_size, self.memory_size)\n",
    "         \n",
    "        self.data_story  = torch.LongTensor(story)\n",
    "        self.data_query  = torch.LongTensor(query)\n",
    "        self.data_answer = torch.LongTensor(np.argmax(answer, axis=1))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_story[idx], self.data_query[idx], self.data_answer[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset('tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_train.txt', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class AtentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, pad_idx):\n",
    "        super(AtentionDecoder, self).__init__()\n",
    "       \n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=pad_idx)\n",
    "\n",
    "        self.GRU_context = nn.GRU(emb_size, hidden_size, batch_first = True)\n",
    "        \n",
    "        self.GRU_query = nn.GRU(emb_size, hidden_size, batch_first = True)\n",
    "        \n",
    "        self.linear_1 = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.linear_2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, context, query, mask):\n",
    "        '''\n",
    "        context: batch_size, N_contexts, seq_len\n",
    "        query: batch_size, seq_len\n",
    "        '''\n",
    "        \n",
    "        emb_context = self.embedding(context)\n",
    "        emb_context = emb_context.view(-1, context.shape[-1], self.emb_size)\n",
    "        _, hidden_context = self.GRU_context(emb_context)\n",
    "        hidden_context = hidden_context.squeeze(0)\n",
    "        hidden_context = hidden_context.view(context.shape[0], context.shape[1], self.hidden_size)    \n",
    "        print(hidden_context.shape, 'hidden')\n",
    "            \n",
    "        embedded_query = self.embedding(query)    \n",
    "        query_outputs, query_hidden = self.GRU_query(embedded_query)     \n",
    "        query_hidden = query_hidden.squeeze(0)\n",
    "        \n",
    "        \n",
    "        _, att_outputs = self.attention(query_hidden, hidden_context, mask)\n",
    "        output = torch.cat([query_hidden, att_outputs], dim = 1)       \n",
    "        linear_1 = self.linear_1(output)        \n",
    "        linear_1 = torch.tanh(linear_1)        \n",
    "        linear_2 = self.linear_2(linear_1)        \n",
    "        out = linear_2.view(-1, self.vocab_size)\n",
    "           \n",
    "        return out\n",
    "    \n",
    "    def attention(self, query, context, mask = None):\n",
    "        \n",
    "        '''\n",
    "        Inputs:\n",
    "            query:   (batch_size, hidden) - outputs of decoder\n",
    "            context: (batch_size, N, hidden) - outputs of encoder\n",
    "            mask:    (batch_size, enc_seq_len)\n",
    "        Outputs:\n",
    "            weights: (batch_size, dec_seq_len, enc_seq_len)\n",
    "            outputs: (batch, dec_seq_len, hidden)\n",
    "        '''\n",
    "        query = query.unsqueeze(-1)\n",
    "        logits = torch.matmul(context, query)\n",
    "\n",
    "        logits = logits.squeeze(-1)\n",
    "        \n",
    "        weights = F.softmax(logits, dim = -1)             \n",
    "        weights = weights.unsqueeze(1)\n",
    "\n",
    "        outputs = torch.matmul(weights, context)\n",
    "        outputs = outputs.squeeze(1)\n",
    "\n",
    "        return weights, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class HierarchyRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, pad_idx):\n",
    "        super(HierarchyRNN, self).__init__()\n",
    "       \n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=pad_idx)\n",
    "\n",
    "        self.GRU_context = nn.GRU(emb_size, hidden_size, batch_first = True)\n",
    "        \n",
    "        self.GRU_hierarchy = nn.GRU(hidden_size, hidden_size, batch_first = True)\n",
    "        \n",
    "        self.GRU_query = nn.GRU(emb_size, hidden_size, batch_first = True)\n",
    "        \n",
    "        self.linear_1 = nn.Linear(1 * hidden_size, int(hidden_size/2))\n",
    "        self.linear_2 = nn.Linear(int(hidden_size/2), vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, context, query, mask):\n",
    "        '''\n",
    "        context: batch_size, N_contexts, seq_len\n",
    "        query: batch_size, seq_len\n",
    "        '''\n",
    "        \n",
    "        emb_context = self.embedding(context)\n",
    "        emb_context = emb_context.view(-1, context.shape[-1], self.emb_size)\n",
    "        _, hidden_context = self.GRU_context(emb_context)\n",
    "        hidden_context = hidden_context.squeeze(0)\n",
    "        hidden_context = hidden_context.view(context.shape[0], context.shape[1], self.hidden_size)    \n",
    "            \n",
    "        embedded_query = self.embedding(query)    \n",
    "        query_outputs, query_hidden = self.GRU_query(embedded_query)     \n",
    "        query_hidden = query_hidden.squeeze(0)\n",
    "        \n",
    "        \n",
    "\n",
    "        query_hidden = query_hidden.unsqueeze(1)\n",
    "        xxx = []\n",
    "        xxx.append(query_hidden)\n",
    "        xxx.append(hidden_context)\n",
    "        \n",
    "        res = torch.cat(xxx, 1)\n",
    "        _, hidden_hierqarchy = self.GRU_hierarchy(res)\n",
    "        hidden_hierqarchy = hidden_hierqarchy.squeeze(0)\n",
    "        \n",
    "        #output = torch.cat([hidden_hierqarchy, query_hidden], dim = 1)       \n",
    "        #linear_1 = self.linear_1(output) \n",
    "        \n",
    "        linear_1 = self.linear_1(hidden_hierqarchy) \n",
    "        linear_1 = torch.relu(linear_1)        \n",
    "        linear_2 = self.linear_2(linear_1)        \n",
    "        out = linear_2.view(-1, self.vocab_size)\n",
    "           \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_decoder = HierarchyRNN(len(train_dataset.words_vocab), 64, 512, pad_idx=train_dataset.words_vocab.pad_idx).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "decoder_optimizer = optim.Adam(att_decoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dataset, encoder, decoder,vocab_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        \n",
    "    def forward(self, context, query, mask):\n",
    "\n",
    "        out = self.decoder(context, query, mask)\n",
    "           \n",
    "        return out   \n",
    "    \n",
    "    def generate(self, bos_idx, eos_idx, batch_words):\n",
    "        inp = [bos_idx]\n",
    "        outputs, hidden = self.encoder(batch_words)\n",
    "        mask_words  = batch_words != 0\n",
    "        \n",
    "\n",
    "        for _ in range(100):\n",
    "            inp_tensor = torch.LongTensor([[inp[-1]]]).to(batch_words.device)\n",
    "            logits, hidden   = self.decoder(hidden, outputs, inp_tensor, mask_words)\n",
    "            next_token = F.softmax(logits, dim=-1)[-1].topk(1)[1].item()\n",
    "            inp.append(next_token)\n",
    "            if next_token == eos_idx:\n",
    "                break\n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, dataset, test_dataset, model,  decoder_optimizer, criterion, batch_size):\n",
    "        \n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.model = model\n",
    "\n",
    "        self.decoder_optimizer = decoder_optimizer\n",
    "        self.criterion = criterion\n",
    "        \n",
    "               \n",
    "    def train(self, n_epochs):\n",
    "        \n",
    "        mask_words = None\n",
    "        for epoch in range(n_epochs):\n",
    "           #scheduler.step()\n",
    "\n",
    "#             for batch_idx in range(len(self.dataset)//self.batch_size):\n",
    "\n",
    "#                 contexts, questions, answers = self.dataset.get_batch(self.batch_size)\n",
    "            for contexts, questions, answers in train_loader:\n",
    "\n",
    "                logits = self.model(contexts, questions, mask_words)\n",
    "                \n",
    "                loss = self.criterion(logits, answers)\n",
    "                \n",
    "                self.decoder_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.decoder_optimizer.step()\n",
    "\n",
    "                self.train_losses.append(loss.item())\n",
    "                \n",
    "                if batch_idx % 200 == 0:\n",
    "                    val_acc = self.eval_()\n",
    "                    self.val_losses.append(val_acc)\n",
    "                    self.plot(epoch, batch_idx, self.train_losses, self.val_losses)\n",
    "                  \n",
    "        \n",
    "    def eval_(self):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            contexts, questions, answers  = self.test_dataset.get_batch(len(self.test_dataset))\n",
    "\n",
    "            logits = self.model(contexts, questions, None)\n",
    "\n",
    "            acc = self.accuracy(logits, answers)\n",
    "\n",
    "        return acc\n",
    "        \n",
    "    def accuracy(self, logits, answers):\n",
    "        \n",
    "        correct = 0\n",
    "        for en, lg in enumerate(logits):\n",
    "\n",
    "            ans = lg.argmax()#.item()     \n",
    "            if ans == answers[en].item():\n",
    "                correct += 1\n",
    "                \n",
    "   \n",
    "        #correct = torch.argmax(logits, 1) == answers\n",
    "        #correct.sum().item()/len(logits)\n",
    "        return correct/len(logits)\n",
    "\n",
    "        \n",
    "    def plot(self, epoch, batch_idx, train_losses, val_losses):\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.subplot(131)\n",
    "        plt.title('epoch %s. | batch: %s | loss: %s' % (epoch, batch_idx, train_losses[-1]))\n",
    "        plt.plot(train_losses)\n",
    "        plt.subplot(132)\n",
    "        plt.title('epoch %s. | acc: %s' % (epoch, val_losses[-1]))\n",
    "        plt.plot(val_losses)\n",
    "        plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset('tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_train.txt',val = False)\n",
    "test_dataset = Dataset('tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_test.txt', val = True)\n",
    "\n",
    "# model = Model(train_dataset, None, decoder, len(train_dataset.words_vocab)).to(device)\n",
    "trainer = Trainer(train_dataset, test_dataset, att_decoder,  decoder_optimizer, criterion, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-369bce2f7b28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-47-aa39de64dd4b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-aa39de64dd4b>\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, epoch, batch_idx, train_losses, val_losses)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch %s. | acc: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \"\"\"\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfigure_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mshow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(*objs, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-9>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
      "\u001b[0;32m/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, **kwargs)\u001b[0m\n\u001b[1;32m   2210\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m                     \u001b[0mdryrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2212\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2213\u001b[0m                 \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cachedRenderer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m                 \u001b[0mbbox_inches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_file_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                 _png.write_png(renderer._renderer, fh,\n\u001b[0;32m--> 532\u001b[0;31m                                self.figure.dpi, metadata=metadata)\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_dpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(trainer.val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print(val):\n",
    "    contexts, questions, answers = data.get_batch(1)\n",
    "\n",
    "    inp = model.generate(data.words_vocab.sos_idx, data.words_vocab.eos_idx, batch_words)\n",
    "            \n",
    "    tokens = [data.trans_vocab.idx2token(idx) for idx in inp if idx not in [data.trans_vocab.sos_idx,\n",
    "                                                                       data.trans_vocab.eos_idx,\n",
    "                                                                       data.trans_vocab.pad_idx]]\n",
    "    print('Src: ', ''.join([data.words_vocab.idx2token(idx) for idx in batch_words[0].tolist()]))\n",
    "    print('Pred:', ''.join(tokens))\n",
    "    print('Real:', ''.join([data.trans_vocab.idx2token(idx) for idx in batch_trans_out[0].tolist() if idx not in [data.trans_vocab.sos_idx,\n",
    "                                                                            data.trans_vocab.eos_idx,\n",
    "                                                                            data.trans_vocab.pad_idx]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
