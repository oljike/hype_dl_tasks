{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "import dlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "from torchvision import models\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class DatasetFromFolder(Dataset):\n",
    "    def __init__(self, img_path, landmarks_path, transform = None):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        \n",
    "#         self.img_path = glob.glob(os.path.join(img_path), '*.jpg')\n",
    "        self.landmarks_path = glob.glob(os.path.join(landmarks_path, '*.jpg'))\n",
    "        self.img_path       = [os.path.join(img_path, os.path.basename(fname)) for fname in self.landmarks_path \n",
    "                                 if os.path.isfile(os.path.join(img_path, os.path.basename(fname)))]\n",
    "        \n",
    "        \n",
    "        transform_list = [transforms.ToTensor()\n",
    "                          ]\n",
    "\n",
    "        self.transform = transforms.Compose(transform_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Load Image}  \n",
    "   \n",
    "        img = Image.open(self.img_path[index])#.convert('RGB')\n",
    "        img = img.resize((256, 128), Image.BICUBIC)\n",
    "        img = self.transform(img)\n",
    "\n",
    "        landmark = Image.open(self.landmarks_path[index]).convert('L')\n",
    "        landmark = landmark.resize((256, 128), Image.BICUBIC)\n",
    "        landmark = self.transform(landmark)\n",
    "\n",
    "\n",
    "        return img, landmark\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     37,
     68,
     143,
     169
    ]
   },
   "outputs": [],
   "source": [
    "class LocalEnhancer(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, generator):        \n",
    "        super(LocalEnhancer, self).__init__()\n",
    "\n",
    "             \n",
    "        model_global       = generator\n",
    "#         model_global.conv7 = Identity()\n",
    "#         model_global.norm7 = Identity()\n",
    "#         model_global.act7  = Identity()\n",
    "        model_global.pad8  = Identity()\n",
    "        model_global.conv8 = Identity()\n",
    "        model_global.act8  = Identity()\n",
    "             \n",
    "        self.model_global  = model_global\n",
    "\n",
    "        ###downsample\n",
    "        self.pad1  = nn.ReflectionPad2d(3)\n",
    "        self.conv1 = nn.Conv2d(input_nc, 32, kernel_size=7, padding=0)\n",
    "        self.norm1 = nn.InstanceNorm2d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.norm2 = nn.InstanceNorm2d(64)\n",
    "         \n",
    "        ### upsample\n",
    "        num_bottlenecks = 3\n",
    "        self.Bottleneck = nn.Sequential(*[\n",
    "                            ResnetBlock(64, nn.InstanceNorm2d(64)) for _ in range(num_bottlenecks)\n",
    "        ])\n",
    "        \n",
    "        self.conv3 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.norm3 = nn.InstanceNorm2d(32)\n",
    "\n",
    "        self.pad4  = nn.ReflectionPad2d(3)\n",
    "        self.conv4 = nn.Conv2d(32, output_nc, kernel_size=7, padding=0)\n",
    "      \n",
    "        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n",
    "\n",
    "    def forward(self, input_): \n",
    "        \n",
    "        ### create input pyramid\n",
    "\n",
    "#         input_downsampled = [input]\n",
    "        input_downsampled = input_.clone()\n",
    "        input_downsampled = self.downsample(input_downsampled)\n",
    "      \n",
    "        output_global = self.model_global(input_downsampled)        \n",
    "      \n",
    "        x_res = self.pad1(input_)\n",
    "        x_res = self.conv1(x_res)\n",
    "        x_res = self.norm1(x_res)\n",
    "        x_res = F.relu(x_res)\n",
    "        \n",
    "        x_res = self.conv2(x_res)\n",
    "        x_res = self.norm2(x_res)\n",
    "        x_res = F.relu(x_res)\n",
    "                \n",
    "        x_out = self.Bottleneck(x_res + output_global)\n",
    "        \n",
    "        x_out = self.conv3(x_out)\n",
    "        x_out = self.norm3(x_out)\n",
    "        x_out = F.relu(x_out)\n",
    "        \n",
    "        x_out = self.pad4(x_out)\n",
    "        x_out = self.conv4(x_out)\n",
    "        x_out = torch.tanh(x_out)\n",
    "        \n",
    "        return x_out\n",
    "\n",
    "class GlobalGenerator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc):\n",
    "        super(GlobalGenerator, self).__init__()        \n",
    "\n",
    "        self.pad1  = nn.ReflectionPad2d(3)\n",
    "        self.conv1 = nn.Conv2d(input_nc, 64, kernel_size=7, padding=0)\n",
    "        self.norm1 = nn.InstanceNorm2d(64)\n",
    "              \n",
    "        self.conv2 = nn.Conv2d(64 * 1, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.norm2 = nn.InstanceNorm2d(128)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.norm3 = nn.InstanceNorm2d(256)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "        self.norm4 = nn.InstanceNorm2d(512)\n",
    "\n",
    "        num_bottlenecks = 9\n",
    "        self.Bottleneck = nn.Sequential(*[\n",
    "                            ResnetBlock(512, nn.InstanceNorm2d(512)) for _ in range(num_bottlenecks)\n",
    "        ])\n",
    "        \n",
    "        self.conv5 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.norm5 = nn.InstanceNorm2d(256)\n",
    "        \n",
    "        self.conv6 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.norm6 = nn.InstanceNorm2d(128)\n",
    "\n",
    "        self.conv7 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.norm7 = nn.InstanceNorm2d(64)\n",
    "        self.act7  = nn.ReLU(True)\n",
    "\n",
    "        self.pad8  = nn.ReflectionPad2d(3)\n",
    "        self.conv8 = nn.Conv2d(64, output_nc, kernel_size=7, padding=0)\n",
    "        self.act8  = nn.Tanh()\n",
    "            \n",
    "    def forward(self, input_):\n",
    "        \n",
    "        x = self.pad1(input_)\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.norm4(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.Bottleneck(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = self.norm5(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv6(x)\n",
    "        x = self.norm6(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv7(x)\n",
    "        x = self.norm7(x)\n",
    "        x = self.act7(x)\n",
    "        \n",
    "        x = self.pad8(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.act8(x)\n",
    "        \n",
    "        return x\n",
    "               \n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, norm_layer):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        \n",
    "        self.pad1  = nn.ReflectionPad2d(1)\n",
    "        self.conv1 = nn.Conv2d(dim, dim, kernel_size=3, padding=0)\n",
    "        self.norm1 = norm_layer\n",
    "        \n",
    "        self.pad2  = nn.ReflectionPad2d(1)\n",
    "        self.conv2 = nn.Conv2d(dim, dim, kernel_size=3, padding=0)\n",
    "        self.norm2 = norm_layer\n",
    " \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x_res = self.pad1(x)\n",
    "        x_res = self.conv1(x_res)\n",
    "        x_res = self.norm1(x_res)\n",
    "        x_res = F.relu(x_res)\n",
    "        x_res = self.pad2(x_res)\n",
    "        x_res = self.conv2(x_res)\n",
    "        x_res = self.norm2(x_res)\n",
    "        \n",
    "        out = x + x_res\n",
    "        return out\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     41
    ]
   },
   "outputs": [],
   "source": [
    "class MultiscaleDiscriminator(nn.Module):\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, \n",
    "                 use_sigmoid=True, num_D=3, getIntermFeat=True):\n",
    "        super(MultiscaleDiscriminator, self).__init__()\n",
    "        self.num_D = num_D\n",
    "        self.n_layers = n_layers\n",
    "        self.getIntermFeat = getIntermFeat\n",
    "     \n",
    "        for i in range(num_D):\n",
    "            netD = NLayerDiscriminator(input_nc, ndf, n_layers, norm_layer, use_sigmoid, getIntermFeat)\n",
    "            if getIntermFeat:                                \n",
    "                for j in range(n_layers+2):\n",
    "                    setattr(self, 'scale'+str(i)+'_layer'+str(j), getattr(netD, 'model'+str(j)))                                   \n",
    "            else:\n",
    "                setattr(self, 'layer'+str(i), netD.model)\n",
    "\n",
    "        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n",
    "\n",
    "    def singleD_forward(self, model, input):\n",
    "        if self.getIntermFeat:\n",
    "            result = [input]\n",
    "            for i in range(len(model)):\n",
    "                result.append(model[i](result[-1]))\n",
    "            return result[1:]\n",
    "        else:\n",
    "            return [model(input)]\n",
    "\n",
    "    def forward(self, input):        \n",
    "        num_D = self.num_D\n",
    "        result = []\n",
    "        input_downsampled = input\n",
    "        for i in range(num_D):\n",
    "            if self.getIntermFeat:\n",
    "                model = [getattr(self, 'scale'+str(num_D-1-i)+'_layer'+str(j)) for j in range(self.n_layers+2)]\n",
    "            else:\n",
    "                model = getattr(self, 'layer'+str(num_D-1-i))\n",
    "            result.append(self.singleD_forward(model, input_downsampled))\n",
    "            if i != (num_D-1):\n",
    "                input_downsampled = self.downsample(input_downsampled)\n",
    "        return result        \n",
    "# Defines the PatchGAN discriminator with the specified arguments.\n",
    "class NLayerDiscriminator(nn.Module):\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, getIntermFeat=False):\n",
    "        super(NLayerDiscriminator, self).__init__()\n",
    "        self.getIntermFeat = getIntermFeat\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        kw = 4\n",
    "        padw = int(np.ceil((kw-1.0)/2))\n",
    "        sequence = [[nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]]\n",
    "\n",
    "        nf = ndf\n",
    "        for n in range(1, n_layers):\n",
    "            nf_prev = nf\n",
    "            nf = min(nf * 2, 512)\n",
    "            sequence += [[\n",
    "                nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=2, padding=padw),\n",
    "                norm_layer(nf), nn.LeakyReLU(0.2, True)\n",
    "            ]]\n",
    "\n",
    "        nf_prev = nf\n",
    "        nf = min(nf * 2, 512)\n",
    "        sequence += [[\n",
    "            nn.Conv2d(nf_prev, nf, kernel_size=kw, stride=1, padding=padw),\n",
    "            norm_layer(nf),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]]\n",
    "\n",
    "        sequence += [[nn.Conv2d(nf, 1, kernel_size=kw, stride=1, padding=padw)]]\n",
    "\n",
    "        if use_sigmoid:\n",
    "            sequence += [[nn.Sigmoid()]]\n",
    "\n",
    "        if getIntermFeat:\n",
    "            for n in range(len(sequence)):\n",
    "                setattr(self, 'model'+str(n), nn.Sequential(*sequence[n]))\n",
    "        else:\n",
    "            sequence_stream = []\n",
    "            for n in range(len(sequence)):\n",
    "                sequence_stream += sequence[n]\n",
    "            self.model = nn.Sequential(*sequence_stream)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.getIntermFeat:\n",
    "            res = [input]\n",
    "            for n in range(self.n_layers+2):\n",
    "                model = getattr(self, 'model'+str(n))\n",
    "                res.append(model(res[-1]))\n",
    "            return res[1:]\n",
    "        else:\n",
    "            return self.model(input) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0,
     45
    ]
   },
   "outputs": [],
   "source": [
    "class OneLayerDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OneLayerDiscriminator, self).__init__()\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=True)\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=True)\n",
    "        self.norm_2 = nn.InstanceNorm2d(128, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.conv_3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=True)\n",
    "        self.norm_3 = nn.InstanceNorm2d(256, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.conv_4 = nn.Conv2d(256, 512, kernel_size=4, padding=1, bias=True)\n",
    "        self.norm_4 = nn.InstanceNorm2d(512, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.conv_output = nn.Conv2d(512, 1, kernel_size=4, padding=1, bias = False)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Inputs:\n",
    "            x: (batch, 3, 128, 128)\n",
    "        Output:\n",
    "            out: (batch, 1)\n",
    "        '''\n",
    "        \n",
    "        x = self.conv_1(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        \n",
    "        x = self.conv_2(x)\n",
    "        x = self.norm_2(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        \n",
    "        x = self.conv_3(x)\n",
    "        x = self.norm_3(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        \n",
    "        x = self.conv_4(x)\n",
    "        x = self.norm_4(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        \n",
    "        x = self.conv_output(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MultiLayerDiscriminator(nn.Module):\n",
    "    def __init__(self, num_D):\n",
    "        super(MultiLayerDiscriminator, self).__init__()\n",
    "        \n",
    "        self.num_D = num_D\n",
    "     \n",
    "        for i in range(3):\n",
    "            netD = OneLayerDiscriminator(input_nc, ndf, n_layers, norm_layer, use_sigmoid, getIntermFeat)           \n",
    "            setattr(self, 'layer'+str(i), netD.model)\n",
    "\n",
    "        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Inputs:\n",
    "            x: (batch, 3, 128, 128)\n",
    "        Output:\n",
    "            out: (batch, 1)\n",
    "        '''\n",
    "            \n",
    "        num_D = self.num_D\n",
    "        result = []\n",
    "        input_downsampled = input\n",
    "        for i in range(num_D):\n",
    "           \n",
    "            model = getattr(self, 'layer'+str(num_D-1-i))\n",
    "            result.append([model(input_downsampled)])\n",
    "            \n",
    "            if i != (num_D-1):\n",
    "                input_downsampled = self.downsample(input_downsampled)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0,
     32,
     46
    ]
   },
   "outputs": [],
   "source": [
    "class Vgg19(torch.nn.Module):\n",
    "    def __init__(self, requires_grad=False):\n",
    "        super(Vgg19, self).__init__()\n",
    "        vgg_pretrained_features = models.vgg19(pretrained=True).features\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        self.slice5 = torch.nn.Sequential()\n",
    "        for x in range(2):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(2, 7):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(7, 12):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(12, 21):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(21, 30):\n",
    "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        h_relu1 = self.slice1(X)\n",
    "        h_relu2 = self.slice2(h_relu1)        \n",
    "        h_relu3 = self.slice3(h_relu2)        \n",
    "        h_relu4 = self.slice4(h_relu3)        \n",
    "        h_relu5 = self.slice5(h_relu4)                \n",
    "        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n",
    "        return out\n",
    "    \n",
    "class VGGLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGLoss, self).__init__()        \n",
    "        self.vgg = Vgg19().to(device)\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]        \n",
    "\n",
    "    def forward(self, x, y):              \n",
    "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
    "        loss = 0\n",
    "        for i in range(len(x_vgg)):\n",
    "            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())        \n",
    "        return loss\n",
    "  \n",
    "class Pix2PixHD(nn.Module):\n",
    "    def __init__(self, num_D, generator, discriminator):\n",
    "        super(Pix2PixHD, self).__init__()\n",
    "        \n",
    "        self.num_D         = num_D\n",
    "        self.generator     = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.feat_loss     = torch.nn.L1Loss()\n",
    "        self.vgg_loss      = VGGLoss()\n",
    "        self.criterionGAN  = torch.nn.MSELoss()\n",
    "    \n",
    "    def GANloss(self, input_, is_real):\n",
    "        \n",
    "        if is_real:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "  \n",
    "        if isinstance(input_[0], list):\n",
    "            loss = 0.0\n",
    "            for i in input_:\n",
    "                pred   = i[-1]\n",
    "                target =  torch.Tensor(pred.size()).fill_(label).to(pred.device)\n",
    "                loss  += self.criterionGAN(pred, target)\n",
    "            return loss\n",
    "        else:\n",
    "            target =  torch.Tensor(input_[-1].size()).fill_(label).to(input_[-1].device)\n",
    "            return self.criterionGAN(input_[-1], target)\n",
    "            \n",
    "    def forward(self, label_map, image):     \n",
    "        self.generator.train()\n",
    "        \n",
    "        fake_image  = self.generator(label_map.to(device))\n",
    "        \n",
    "        ### Fake Loss\n",
    "        pred_fake   = self.discriminator(torch.cat((label_map, fake_image.detach()), dim=1))   \n",
    "        loss_D_fake = self.GANloss(pred_fake, is_real=False)\n",
    "        \n",
    "        ### Real Loss\n",
    "        pred_real   = self.discriminator(torch.cat((label_map, image.detach()), dim=1))\n",
    "        loss_D_real = self.GANloss(pred_real, is_real=True)\n",
    "        \n",
    "        ### GAN loss\n",
    "        pred_fake_GAN = self.discriminator(torch.cat((label_map, fake_image), dim=1))\n",
    "        loss_GAN      = self.GANloss(pred_fake_GAN, is_real=True)\n",
    "        \n",
    "        ### Feature Matching loss\n",
    "        loss_FM = 0\n",
    "        D_weights = 1.0 / self.num_D\n",
    "        for i in range(self.num_D):\n",
    "            for j in range(len(pred_fake[i])-1):\n",
    "                loss_FM += D_weights * self.feat_loss(pred_fake[i][j], pred_real[i][j].detach()) * 10.0 \n",
    "\n",
    "        loss_VGG = self.vgg_loss(fake_image, image) * 10.0\n",
    "        \n",
    "        return loss_D_fake, loss_D_real, loss_GAN, loss_FM, loss_VGG, fake_image\n",
    "    \n",
    "    def eval(self, label_map):\n",
    "        self.generator.eval()\n",
    "        \n",
    "        fake_image  = self.generator(label_map.to(device))\n",
    "        \n",
    "        return fake_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(), 'pix2pixHD_model/edge_gen_epoch_58.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(), 'pix2pixHD_model/edge_whole_model_epoch_119.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_global     = GlobalGenerator(1, 3)\n",
    "generator_global.load_state_dict(torch.load('pix2pixHD_model/edge_gen_epoch_58.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator     = LocalEnhancer(1,3, generator_global).to(device)\n",
    "\n",
    "discriminator = MultiscaleDiscriminator(4).to(device)\n",
    "model         = Pix2PixHD(3, generator, discriminator).to(device)\n",
    "\n",
    "optimizer_gen = optim.Adam(generator.parameters(), lr=0.00002, betas=(0.5, 0.999))\n",
    "optimizer_disc = optim.Adam(discriminator.parameters(), lr=0.00002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetFromFolder('face_dataset/celeba/img_align_celeba', 'face_dataset/celeba/edges_celeba')\n",
    "dataloader = DataLoader(dataset, batch_size=8,\n",
    "                        shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.8765394687652588\n",
      "Generator loss 18.628347396850586\n",
      "Discriminator loss 0.918090283870697\n",
      "Generator loss 13.107489585876465\n",
      "Discriminator loss 0.8691658973693848\n",
      "Generator loss 13.473724365234375\n",
      "Discriminator loss 0.8523504734039307\n",
      "Generator loss 13.335622787475586\n",
      "Discriminator loss 0.8501777648925781\n",
      "Generator loss 12.88961410522461\n",
      "Discriminator loss 0.8391964435577393\n",
      "Generator loss 12.982568740844727\n",
      "Discriminator loss 0.8496569395065308\n",
      "Generator loss 11.973925590515137\n",
      "Discriminator loss 0.8304481506347656\n",
      "Generator loss 12.687139511108398\n",
      "Discriminator loss 0.8325420022010803\n",
      "Generator loss 11.593464851379395\n",
      "Discriminator loss 0.8282899856567383\n",
      "Generator loss 11.615694046020508\n",
      "Discriminator loss 0.8233161568641663\n",
      "Generator loss 11.229833602905273\n",
      "Discriminator loss 0.8522179126739502\n",
      "Generator loss 12.035330772399902\n",
      "Discriminator loss 0.8491238355636597\n",
      "Generator loss 11.539929389953613\n",
      "Discriminator loss 0.8424220681190491\n",
      "Generator loss 10.728367805480957\n",
      "Discriminator loss 0.841148316860199\n",
      "Generator loss 10.602319717407227\n",
      "Discriminator loss 0.8563638925552368\n",
      "Generator loss 10.980690002441406\n",
      "Discriminator loss 0.8423073291778564\n",
      "Generator loss 10.991316795349121\n",
      "Discriminator loss 0.8203108906745911\n",
      "Generator loss 11.159957885742188\n",
      "Discriminator loss 0.8596248030662537\n",
      "Generator loss 10.853704452514648\n",
      "Discriminator loss 0.8636370301246643\n",
      "Generator loss 10.156620025634766\n",
      "Discriminator loss 0.8134084939956665\n",
      "Generator loss 9.273042678833008\n",
      "Discriminator loss 0.8583977818489075\n",
      "Generator loss 9.891162872314453\n",
      "Discriminator loss 0.8243942856788635\n",
      "Generator loss 9.980138778686523\n",
      "Discriminator loss 0.8366992473602295\n",
      "Generator loss 10.456521987915039\n",
      "Discriminator loss 0.8344946503639221\n",
      "Generator loss 10.465933799743652\n",
      "Epoch loss\n",
      "Discriminator loss 0.8454185724258423\n",
      "Generator loss 9.161837577819824\n",
      "Discriminator loss 0.8425622582435608\n",
      "Generator loss 9.278572082519531\n",
      "Discriminator loss 0.8063842058181763\n",
      "Generator loss 9.828359603881836\n",
      "Discriminator loss 0.8347038626670837\n",
      "Generator loss 9.487892150878906\n",
      "Discriminator loss 0.8161739110946655\n",
      "Generator loss 9.163116455078125\n",
      "Discriminator loss 0.8152735233306885\n",
      "Generator loss 9.933286666870117\n",
      "Discriminator loss 0.7940208315849304\n",
      "Generator loss 9.072379112243652\n",
      "Discriminator loss 0.8008583784103394\n",
      "Generator loss 9.525541305541992\n",
      "Discriminator loss 0.8317124247550964\n",
      "Generator loss 9.312198638916016\n",
      "Discriminator loss 0.8214100003242493\n",
      "Generator loss 8.785667419433594\n",
      "Discriminator loss 0.8700503706932068\n",
      "Generator loss 8.527420997619629\n",
      "Discriminator loss 0.8400965929031372\n",
      "Generator loss 10.210586547851562\n",
      "Discriminator loss 0.7972058057785034\n",
      "Generator loss 9.579066276550293\n",
      "Discriminator loss 0.8152738213539124\n",
      "Generator loss 9.141349792480469\n",
      "Discriminator loss 0.8104403018951416\n",
      "Generator loss 8.681771278381348\n",
      "Discriminator loss 0.8318227529525757\n",
      "Generator loss 8.544394493103027\n",
      "Discriminator loss 0.8026151061058044\n",
      "Generator loss 8.717117309570312\n",
      "Discriminator loss 0.8426812887191772\n",
      "Generator loss 8.810735702514648\n",
      "Discriminator loss 0.834077000617981\n",
      "Generator loss 7.672414779663086\n",
      "Discriminator loss 0.8281477093696594\n",
      "Generator loss 8.413373947143555\n",
      "Discriminator loss 0.7815008163452148\n",
      "Generator loss 8.650304794311523\n",
      "Discriminator loss 0.828514814376831\n",
      "Generator loss 7.747359275817871\n",
      "Discriminator loss 0.843289315700531\n",
      "Generator loss 7.828590393066406\n",
      "Discriminator loss 0.8322442173957825\n",
      "Generator loss 8.584779739379883\n",
      "Discriminator loss 0.8238154649734497\n",
      "Generator loss 7.828024864196777\n",
      "Epoch loss\n",
      "Discriminator loss 0.8014160990715027\n",
      "Generator loss 8.078332901000977\n",
      "Discriminator loss 0.8534190058708191\n",
      "Generator loss 7.39812707901001\n",
      "Discriminator loss 0.8071820139884949\n",
      "Generator loss 7.1836700439453125\n",
      "Discriminator loss 0.8091943860054016\n",
      "Generator loss 7.99879789352417\n",
      "Discriminator loss 0.8075920343399048\n",
      "Generator loss 7.485803604125977\n",
      "Discriminator loss 0.8155518770217896\n",
      "Generator loss 7.0798659324646\n",
      "Discriminator loss 0.8042684197425842\n",
      "Generator loss 7.3123321533203125\n",
      "Discriminator loss 0.7926151752471924\n",
      "Generator loss 8.019883155822754\n",
      "Discriminator loss 0.8208007216453552\n",
      "Generator loss 6.940001487731934\n",
      "Discriminator loss 0.7693893909454346\n",
      "Generator loss 7.213504791259766\n",
      "Discriminator loss 0.7936050295829773\n",
      "Generator loss 7.087646007537842\n",
      "Discriminator loss 0.8568739891052246\n",
      "Generator loss 7.302657127380371\n",
      "Discriminator loss 0.8063414096832275\n",
      "Generator loss 7.603165626525879\n",
      "Discriminator loss 0.8539257645606995\n",
      "Generator loss 6.3054399490356445\n",
      "Discriminator loss 0.8018121123313904\n",
      "Generator loss 7.024967193603516\n",
      "Discriminator loss 0.8406527638435364\n",
      "Generator loss 6.7951226234436035\n",
      "Discriminator loss 0.8054563403129578\n",
      "Generator loss 7.318869590759277\n",
      "Discriminator loss 0.7892869710922241\n",
      "Generator loss 7.483889579772949\n",
      "Discriminator loss 0.82193922996521\n",
      "Generator loss 7.213909149169922\n",
      "Discriminator loss 0.846813440322876\n",
      "Generator loss 6.655824184417725\n",
      "Discriminator loss 0.8071388602256775\n",
      "Generator loss 7.301501274108887\n",
      "Discriminator loss 0.7810523509979248\n",
      "Generator loss 8.101723670959473\n",
      "Discriminator loss 0.8134405612945557\n",
      "Generator loss 7.104015827178955\n",
      "Discriminator loss 0.7654008865356445\n",
      "Generator loss 7.169647693634033\n",
      "Discriminator loss 0.8144879937171936\n",
      "Generator loss 6.389294624328613\n",
      "Epoch loss\n",
      "Discriminator loss 0.7683826088905334\n",
      "Generator loss 6.708881855010986\n",
      "Discriminator loss 0.8446285724639893\n",
      "Generator loss 6.774156093597412\n",
      "Discriminator loss 0.8740600347518921\n",
      "Generator loss 7.15654182434082\n",
      "Discriminator loss 0.8144057989120483\n",
      "Generator loss 6.079784870147705\n",
      "Discriminator loss 0.8097190260887146\n",
      "Generator loss 6.19646692276001\n",
      "Discriminator loss 0.8160132765769958\n",
      "Generator loss 6.769149303436279\n",
      "Discriminator loss 0.7870855927467346\n",
      "Generator loss 7.632607460021973\n",
      "Discriminator loss 0.8049380779266357\n",
      "Generator loss 6.6792192459106445\n",
      "Discriminator loss 0.8032177090644836\n",
      "Generator loss 6.674806118011475\n",
      "Discriminator loss 0.771899938583374\n",
      "Generator loss 6.928532123565674\n",
      "Discriminator loss 0.8479841947555542\n",
      "Generator loss 7.037407398223877\n",
      "Discriminator loss 0.7409414649009705\n",
      "Generator loss 6.701374530792236\n",
      "Discriminator loss 0.8179039359092712\n",
      "Generator loss 6.378175735473633\n",
      "Discriminator loss 0.749886691570282\n",
      "Generator loss 6.651780128479004\n",
      "Discriminator loss 0.7826802134513855\n",
      "Generator loss 6.3832688331604\n",
      "Discriminator loss 0.8704163432121277\n",
      "Generator loss 6.6339569091796875\n",
      "Discriminator loss 0.8174026608467102\n",
      "Generator loss 6.510924339294434\n",
      "Discriminator loss 0.7494579553604126\n",
      "Generator loss 6.70564079284668\n",
      "Discriminator loss 0.8726564049720764\n",
      "Generator loss 6.174682140350342\n",
      "Discriminator loss 0.753938615322113\n",
      "Generator loss 6.272575855255127\n",
      "Discriminator loss 0.7475626468658447\n",
      "Generator loss 6.747465133666992\n",
      "Discriminator loss 0.8107437491416931\n",
      "Generator loss 6.826401233673096\n",
      "Discriminator loss 0.740069568157196\n",
      "Generator loss 6.206279754638672\n",
      "Discriminator loss 0.8709266185760498\n",
      "Generator loss 6.55389928817749\n",
      "Discriminator loss 0.9386342763900757\n",
      "Generator loss 6.6630859375\n",
      "Epoch loss\n",
      "Discriminator loss 0.704253077507019\n",
      "Generator loss 6.796679973602295\n",
      "Discriminator loss 0.7173635363578796\n",
      "Generator loss 6.421773433685303\n",
      "Discriminator loss 0.8860471844673157\n",
      "Generator loss 6.156142234802246\n",
      "Discriminator loss 0.8410882949829102\n",
      "Generator loss 6.741591453552246\n",
      "Discriminator loss 0.7493162751197815\n",
      "Generator loss 6.499711036682129\n",
      "Discriminator loss 0.906708300113678\n",
      "Generator loss 6.4499359130859375\n",
      "Discriminator loss 0.7859878540039062\n",
      "Generator loss 6.056805610656738\n",
      "Discriminator loss 0.7957473993301392\n",
      "Generator loss 6.489134311676025\n",
      "Discriminator loss 0.7491016387939453\n",
      "Generator loss 6.793454647064209\n",
      "Discriminator loss 0.9270857572555542\n",
      "Generator loss 6.325113773345947\n",
      "Discriminator loss 0.7847081422805786\n",
      "Generator loss 6.15208625793457\n",
      "Discriminator loss 0.7271585464477539\n",
      "Generator loss 6.441637992858887\n",
      "Discriminator loss 0.7153396606445312\n",
      "Generator loss 6.394794940948486\n",
      "Discriminator loss 0.805081307888031\n",
      "Generator loss 6.667418003082275\n",
      "Discriminator loss 0.8773563504219055\n",
      "Generator loss 6.30035400390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.7510214447975159\n",
      "Generator loss 6.25280237197876\n",
      "Discriminator loss 0.782821536064148\n",
      "Generator loss 5.949898719787598\n",
      "Discriminator loss 0.7238113880157471\n",
      "Generator loss 6.298405170440674\n",
      "Discriminator loss 0.8851546049118042\n",
      "Generator loss 6.283283233642578\n",
      "Discriminator loss 0.7905271649360657\n",
      "Generator loss 6.832790374755859\n",
      "Discriminator loss 0.7425957322120667\n",
      "Generator loss 6.030763626098633\n",
      "Discriminator loss 0.7651146054267883\n",
      "Generator loss 6.724346160888672\n",
      "Discriminator loss 0.7800043821334839\n",
      "Generator loss 6.669084548950195\n",
      "Discriminator loss 0.7035791277885437\n",
      "Generator loss 6.200652122497559\n",
      "Discriminator loss 0.6712820529937744\n",
      "Generator loss 7.093760013580322\n",
      "Epoch loss\n",
      "Discriminator loss 0.7564097046852112\n",
      "Generator loss 6.152312755584717\n",
      "Discriminator loss 0.785067081451416\n",
      "Generator loss 6.188170909881592\n",
      "Discriminator loss 0.8289056420326233\n",
      "Generator loss 6.115095138549805\n",
      "Discriminator loss 0.7342848777770996\n",
      "Generator loss 6.670702934265137\n",
      "Discriminator loss 0.7539063692092896\n",
      "Generator loss 6.324814796447754\n",
      "Discriminator loss 0.8579121828079224\n",
      "Generator loss 5.860057830810547\n",
      "Discriminator loss 0.7047706842422485\n",
      "Generator loss 6.594841003417969\n",
      "Discriminator loss 0.8307555317878723\n",
      "Generator loss 6.165225028991699\n",
      "Discriminator loss 0.8209920525550842\n",
      "Generator loss 6.440304756164551\n",
      "Discriminator loss 0.6646641492843628\n",
      "Generator loss 6.690586090087891\n",
      "Discriminator loss 0.7943458557128906\n",
      "Generator loss 6.450837135314941\n",
      "Discriminator loss 0.7602112293243408\n",
      "Generator loss 6.255479335784912\n",
      "Discriminator loss 0.8653525114059448\n",
      "Generator loss 6.087154388427734\n",
      "Discriminator loss 0.7450255155563354\n",
      "Generator loss 6.6506242752075195\n",
      "Discriminator loss 0.7329844832420349\n",
      "Generator loss 5.884070873260498\n",
      "Discriminator loss 0.7555099725723267\n",
      "Generator loss 5.997303485870361\n",
      "Discriminator loss 0.7840269207954407\n",
      "Generator loss 6.29520320892334\n",
      "Discriminator loss 0.8509105443954468\n",
      "Generator loss 5.871368408203125\n",
      "Discriminator loss 0.719437837600708\n",
      "Generator loss 6.2341132164001465\n",
      "Discriminator loss 0.7566685080528259\n",
      "Generator loss 6.161558151245117\n",
      "Discriminator loss 0.7722166180610657\n",
      "Generator loss 6.678437232971191\n",
      "Discriminator loss 0.8553903698921204\n",
      "Generator loss 6.336299896240234\n",
      "Discriminator loss 0.9105709195137024\n",
      "Generator loss 6.428966045379639\n",
      "Discriminator loss 0.8060144782066345\n",
      "Generator loss 5.913872241973877\n",
      "Discriminator loss 0.7206982374191284\n",
      "Generator loss 6.665522575378418\n",
      "Epoch loss\n",
      "Discriminator loss 0.8069925308227539\n",
      "Generator loss 6.652708053588867\n",
      "Discriminator loss 0.7282916903495789\n",
      "Generator loss 5.600235939025879\n",
      "Discriminator loss 0.7998455762863159\n",
      "Generator loss 5.998476982116699\n",
      "Discriminator loss 0.7169361114501953\n",
      "Generator loss 5.86197566986084\n",
      "Discriminator loss 0.7276163101196289\n",
      "Generator loss 5.809320449829102\n",
      "Discriminator loss 0.7761204242706299\n",
      "Generator loss 7.202178955078125\n",
      "Discriminator loss 0.7233600616455078\n",
      "Generator loss 7.033204078674316\n",
      "Discriminator loss 0.8170716762542725\n",
      "Generator loss 6.061145782470703\n",
      "Discriminator loss 0.8761737942695618\n",
      "Generator loss 6.028890132904053\n",
      "Discriminator loss 0.724701464176178\n",
      "Generator loss 5.963050842285156\n",
      "Discriminator loss 0.7007768154144287\n",
      "Generator loss 6.296813011169434\n",
      "Discriminator loss 0.7447881698608398\n",
      "Generator loss 6.761575222015381\n",
      "Discriminator loss 0.7939019203186035\n",
      "Generator loss 6.081757545471191\n",
      "Discriminator loss 0.7338225245475769\n",
      "Generator loss 5.786877632141113\n",
      "Discriminator loss 0.6865594983100891\n",
      "Generator loss 6.488579750061035\n",
      "Discriminator loss 0.6693295240402222\n",
      "Generator loss 6.048764228820801\n",
      "Discriminator loss 0.6434100270271301\n",
      "Generator loss 6.6295037269592285\n",
      "Discriminator loss 0.7324234843254089\n",
      "Generator loss 6.67118501663208\n",
      "Discriminator loss 0.8967724442481995\n",
      "Generator loss 6.307223796844482\n",
      "Discriminator loss 0.7456344366073608\n",
      "Generator loss 6.958572864532471\n",
      "Discriminator loss 0.8044394850730896\n",
      "Generator loss 6.333197593688965\n",
      "Discriminator loss 0.7072842717170715\n",
      "Generator loss 5.9513678550720215\n",
      "Discriminator loss 0.7343456745147705\n",
      "Generator loss 6.226253986358643\n",
      "Discriminator loss 0.6795752644538879\n",
      "Generator loss 5.866812229156494\n",
      "Discriminator loss 0.7464534640312195\n",
      "Generator loss 5.833288669586182\n",
      "Epoch loss\n",
      "Discriminator loss 0.8670255541801453\n",
      "Generator loss 6.198675632476807\n",
      "Discriminator loss 0.736048698425293\n",
      "Generator loss 5.73248815536499\n",
      "Discriminator loss 0.7156186699867249\n",
      "Generator loss 6.189904689788818\n",
      "Discriminator loss 0.7529765367507935\n",
      "Generator loss 5.654678821563721\n",
      "Discriminator loss 0.98736172914505\n",
      "Generator loss 6.498677730560303\n",
      "Discriminator loss 0.7441421151161194\n",
      "Generator loss 6.248891353607178\n",
      "Discriminator loss 0.7704665660858154\n",
      "Generator loss 6.285075664520264\n",
      "Discriminator loss 0.7746626138687134\n",
      "Generator loss 6.217459678649902\n",
      "Discriminator loss 0.6880980730056763\n",
      "Generator loss 6.723613739013672\n",
      "Discriminator loss 0.6815286874771118\n",
      "Generator loss 6.132209300994873\n",
      "Discriminator loss 0.8137463927268982\n",
      "Generator loss 6.046239852905273\n",
      "Discriminator loss 0.8010445237159729\n",
      "Generator loss 6.3826904296875\n",
      "Discriminator loss 1.014696717262268\n",
      "Generator loss 5.618221282958984\n",
      "Discriminator loss 0.9025344252586365\n",
      "Generator loss 6.463473320007324\n",
      "Discriminator loss 0.7530230283737183\n",
      "Generator loss 5.776759624481201\n",
      "Discriminator loss 0.7152705192565918\n",
      "Generator loss 6.843140125274658\n",
      "Discriminator loss 0.7983695864677429\n",
      "Generator loss 5.6938862800598145\n",
      "Discriminator loss 0.6486563682556152\n",
      "Generator loss 5.828045845031738\n",
      "Discriminator loss 0.7525027990341187\n",
      "Generator loss 6.507635116577148\n",
      "Discriminator loss 0.776082456111908\n",
      "Generator loss 6.613007068634033\n",
      "Discriminator loss 0.7815198302268982\n",
      "Generator loss 6.164833068847656\n",
      "Discriminator loss 0.777736246585846\n",
      "Generator loss 5.615385055541992\n",
      "Discriminator loss 0.8462945818901062\n",
      "Generator loss 5.962955951690674\n",
      "Discriminator loss 0.7208656072616577\n",
      "Generator loss 5.8155012130737305\n",
      "Discriminator loss 0.7209282517433167\n",
      "Generator loss 5.887519836425781\n",
      "Epoch loss\n",
      "Discriminator loss 0.8077807426452637\n",
      "Generator loss 5.617276191711426\n",
      "Discriminator loss 0.7896980047225952\n",
      "Generator loss 6.040501594543457\n",
      "Discriminator loss 0.6904864311218262\n",
      "Generator loss 6.4677042961120605\n",
      "Discriminator loss 0.6739955544471741\n",
      "Generator loss 6.331684112548828\n",
      "Discriminator loss 0.7876268029212952\n",
      "Generator loss 6.121657371520996\n",
      "Discriminator loss 0.7336053252220154\n",
      "Generator loss 5.73331356048584\n",
      "Discriminator loss 0.7128424644470215\n",
      "Generator loss 6.314387321472168\n",
      "Discriminator loss 0.7971395254135132\n",
      "Generator loss 6.155061721801758\n",
      "Discriminator loss 0.84356689453125\n",
      "Generator loss 6.372618675231934\n",
      "Discriminator loss 0.7813892960548401\n",
      "Generator loss 5.87606143951416\n",
      "Discriminator loss 0.8342314958572388\n",
      "Generator loss 5.433508396148682\n",
      "Discriminator loss 0.7600979208946228\n",
      "Generator loss 6.013088703155518\n",
      "Discriminator loss 0.7432734370231628\n",
      "Generator loss 5.522568702697754\n",
      "Discriminator loss 0.7800195217132568\n",
      "Generator loss 6.45763635635376\n",
      "Discriminator loss 1.085682988166809\n",
      "Generator loss 6.070826053619385\n",
      "Discriminator loss 0.6330105662345886\n",
      "Generator loss 6.583740711212158\n",
      "Discriminator loss 0.701688289642334\n",
      "Generator loss 5.834611415863037\n",
      "Discriminator loss 0.7403565645217896\n",
      "Generator loss 5.889945983886719\n",
      "Discriminator loss 0.8080528378486633\n",
      "Generator loss 5.858560085296631\n",
      "Discriminator loss 0.7674999237060547\n",
      "Generator loss 5.885253429412842\n",
      "Discriminator loss 0.7128798961639404\n",
      "Generator loss 5.834866523742676\n",
      "Discriminator loss 0.8589643239974976\n",
      "Generator loss 6.140666484832764\n",
      "Discriminator loss 0.6944528222084045\n",
      "Generator loss 6.555149078369141\n",
      "Discriminator loss 0.7226585149765015\n",
      "Generator loss 5.682769298553467\n",
      "Discriminator loss 0.707615852355957\n",
      "Generator loss 6.747540473937988\n",
      "Epoch loss\n",
      "Discriminator loss 0.7667670845985413\n",
      "Generator loss 5.358229637145996\n",
      "Discriminator loss 0.8990110754966736\n",
      "Generator loss 5.3392462730407715\n",
      "Discriminator loss 0.8855803608894348\n",
      "Generator loss 5.574126243591309\n",
      "Discriminator loss 0.789328396320343\n",
      "Generator loss 5.479905128479004\n",
      "Discriminator loss 0.8839288949966431\n",
      "Generator loss 5.654529094696045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.7846916317939758\n",
      "Generator loss 5.891310214996338\n",
      "Discriminator loss 0.706770122051239\n",
      "Generator loss 5.948126792907715\n",
      "Discriminator loss 0.7413986325263977\n",
      "Generator loss 5.274334907531738\n",
      "Discriminator loss 0.7860912084579468\n",
      "Generator loss 5.917453765869141\n",
      "Discriminator loss 0.6951473951339722\n",
      "Generator loss 5.4522223472595215\n",
      "Discriminator loss 0.7200378179550171\n",
      "Generator loss 5.966160297393799\n",
      "Discriminator loss 0.8399273753166199\n",
      "Generator loss 5.748302936553955\n",
      "Discriminator loss 0.7905372381210327\n",
      "Generator loss 5.901003360748291\n",
      "Discriminator loss 0.8724804520606995\n",
      "Generator loss 5.518301010131836\n",
      "Discriminator loss 0.8109613656997681\n",
      "Generator loss 6.041135311126709\n",
      "Discriminator loss 0.7024373412132263\n",
      "Generator loss 6.156224250793457\n",
      "Discriminator loss 0.7344774007797241\n",
      "Generator loss 5.933777332305908\n",
      "Discriminator loss 0.7583903074264526\n",
      "Generator loss 5.714943885803223\n",
      "Discriminator loss 0.8256441950798035\n",
      "Generator loss 6.166074275970459\n",
      "Discriminator loss 0.767819344997406\n",
      "Generator loss 5.349193572998047\n",
      "Discriminator loss 0.7806227803230286\n",
      "Generator loss 5.77425479888916\n",
      "Discriminator loss 1.013332486152649\n",
      "Generator loss 5.619438171386719\n",
      "Discriminator loss 0.7968619465827942\n",
      "Generator loss 5.371946334838867\n",
      "Discriminator loss 0.8062303066253662\n",
      "Generator loss 5.648817539215088\n",
      "Discriminator loss 0.7215675711631775\n",
      "Generator loss 5.821811676025391\n",
      "Epoch loss\n",
      "Discriminator loss 0.790966272354126\n",
      "Generator loss 5.991973876953125\n",
      "Discriminator loss 0.8739029169082642\n",
      "Generator loss 6.313656806945801\n",
      "Discriminator loss 0.8023726940155029\n",
      "Generator loss 6.11689567565918\n",
      "Discriminator loss 0.7923243641853333\n",
      "Generator loss 6.50400447845459\n",
      "Discriminator loss 0.7014299631118774\n",
      "Generator loss 6.619311809539795\n",
      "Discriminator loss 0.7817093133926392\n",
      "Generator loss 6.1209821701049805\n",
      "Discriminator loss 0.6949858665466309\n",
      "Generator loss 6.096534252166748\n",
      "Discriminator loss 0.7765811085700989\n",
      "Generator loss 6.217777252197266\n",
      "Discriminator loss 0.7268727421760559\n",
      "Generator loss 5.929381370544434\n",
      "Discriminator loss 1.0341145992279053\n",
      "Generator loss 5.905272006988525\n",
      "Discriminator loss 0.7748721837997437\n",
      "Generator loss 6.4425153732299805\n",
      "Discriminator loss 0.7793768644332886\n",
      "Generator loss 6.41552734375\n",
      "Discriminator loss 0.6764782667160034\n",
      "Generator loss 6.0116801261901855\n",
      "Discriminator loss 0.7446419596672058\n",
      "Generator loss 5.90643310546875\n",
      "Discriminator loss 0.6774402856826782\n",
      "Generator loss 6.249787330627441\n",
      "Discriminator loss 0.891011118888855\n",
      "Generator loss 5.715777397155762\n",
      "Discriminator loss 0.8892501592636108\n",
      "Generator loss 5.3710126876831055\n",
      "Discriminator loss 0.7546700239181519\n",
      "Generator loss 5.66266393661499\n",
      "Discriminator loss 0.8449345827102661\n",
      "Generator loss 6.279129981994629\n",
      "Discriminator loss 0.712207555770874\n",
      "Generator loss 5.551363945007324\n",
      "Discriminator loss 0.6894559860229492\n",
      "Generator loss 6.045646667480469\n",
      "Discriminator loss 0.7247391939163208\n",
      "Generator loss 6.749375820159912\n",
      "Discriminator loss 0.7436161041259766\n",
      "Generator loss 5.752573490142822\n",
      "Discriminator loss 0.7138642072677612\n",
      "Generator loss 6.022365093231201\n",
      "Discriminator loss 0.8466677069664001\n",
      "Generator loss 6.2166290283203125\n",
      "Epoch loss\n",
      "Discriminator loss 0.7163709998130798\n",
      "Generator loss 5.584592819213867\n",
      "Discriminator loss 0.8405783176422119\n",
      "Generator loss 5.299403190612793\n",
      "Discriminator loss 0.8971824645996094\n",
      "Generator loss 5.461601734161377\n",
      "Discriminator loss 0.6425381302833557\n",
      "Generator loss 6.567138671875\n",
      "Discriminator loss 0.7833958864212036\n",
      "Generator loss 5.466427803039551\n",
      "Discriminator loss 0.6582794189453125\n",
      "Generator loss 6.288893699645996\n",
      "Discriminator loss 0.8958653807640076\n",
      "Generator loss 5.347092151641846\n",
      "Discriminator loss 0.7771169543266296\n",
      "Generator loss 5.6002373695373535\n",
      "Discriminator loss 0.7569766640663147\n",
      "Generator loss 5.394262790679932\n",
      "Discriminator loss 0.8417322635650635\n",
      "Generator loss 5.517394542694092\n",
      "Discriminator loss 0.7129286527633667\n",
      "Generator loss 6.572183132171631\n",
      "Discriminator loss 0.8626713156700134\n",
      "Generator loss 5.862715721130371\n",
      "Discriminator loss 0.6510189771652222\n",
      "Generator loss 6.102376937866211\n",
      "Discriminator loss 0.8025023937225342\n",
      "Generator loss 5.632002353668213\n",
      "Discriminator loss 0.8720837235450745\n",
      "Generator loss 6.241565227508545\n",
      "Discriminator loss 1.1887311935424805\n",
      "Generator loss 5.478107452392578\n",
      "Discriminator loss 0.7152474522590637\n",
      "Generator loss 5.581756591796875\n",
      "Discriminator loss 0.8177762031555176\n",
      "Generator loss 5.2781171798706055\n",
      "Discriminator loss 0.861548125743866\n",
      "Generator loss 5.675057888031006\n",
      "Discriminator loss 0.7755073308944702\n",
      "Generator loss 5.684003829956055\n",
      "Discriminator loss 0.9317930936813354\n",
      "Generator loss 5.431080341339111\n",
      "Discriminator loss 0.8335971236228943\n",
      "Generator loss 5.7367095947265625\n",
      "Discriminator loss 0.8069108724594116\n",
      "Generator loss 5.783059120178223\n",
      "Discriminator loss 1.0081367492675781\n",
      "Generator loss 5.714257717132568\n",
      "Discriminator loss 0.700451672077179\n",
      "Generator loss 5.881166458129883\n",
      "Epoch loss\n",
      "Discriminator loss 0.6932919025421143\n",
      "Generator loss 5.723819732666016\n",
      "Discriminator loss 0.7330708503723145\n",
      "Generator loss 5.63032865524292\n",
      "Discriminator loss 0.933585524559021\n",
      "Generator loss 5.7314019203186035\n",
      "Discriminator loss 0.7659379839897156\n",
      "Generator loss 5.952209949493408\n",
      "Discriminator loss 0.8208559155464172\n",
      "Generator loss 6.335893154144287\n",
      "Discriminator loss 0.886584460735321\n",
      "Generator loss 5.675422668457031\n",
      "Discriminator loss 0.827379047870636\n",
      "Generator loss 5.641932487487793\n",
      "Discriminator loss 0.8916522264480591\n",
      "Generator loss 6.031012058258057\n",
      "Discriminator loss 0.9016338586807251\n",
      "Generator loss 6.040687084197998\n",
      "Discriminator loss 0.9271437525749207\n",
      "Generator loss 5.2954277992248535\n",
      "Discriminator loss 0.9015913605690002\n",
      "Generator loss 6.085257053375244\n",
      "Discriminator loss 0.6104623079299927\n",
      "Generator loss 6.142460823059082\n",
      "Discriminator loss 0.8938831090927124\n",
      "Generator loss 6.170817852020264\n",
      "Discriminator loss 0.962278425693512\n",
      "Generator loss 5.398046970367432\n",
      "Discriminator loss 0.8821021914482117\n",
      "Generator loss 5.9722185134887695\n",
      "Discriminator loss 0.8372780680656433\n",
      "Generator loss 5.370563507080078\n",
      "Discriminator loss 0.686909019947052\n",
      "Generator loss 5.62150764465332\n",
      "Discriminator loss 0.9335857629776001\n",
      "Generator loss 5.4551873207092285\n",
      "Discriminator loss 0.7588776350021362\n",
      "Generator loss 6.201711654663086\n",
      "Discriminator loss 0.7675687670707703\n",
      "Generator loss 6.171383857727051\n",
      "Discriminator loss 0.7456573843955994\n",
      "Generator loss 6.0477728843688965\n",
      "Discriminator loss 0.8471043109893799\n",
      "Generator loss 5.523158073425293\n",
      "Discriminator loss 0.9007642865180969\n",
      "Generator loss 6.010587215423584\n",
      "Discriminator loss 0.723206102848053\n",
      "Generator loss 6.126516342163086\n",
      "Discriminator loss 0.924217700958252\n",
      "Generator loss 5.4789228439331055\n",
      "Epoch loss\n",
      "Discriminator loss 0.6879062652587891\n",
      "Generator loss 5.778450965881348\n",
      "Discriminator loss 0.7699470520019531\n",
      "Generator loss 5.79732608795166\n",
      "Discriminator loss 0.6867189407348633\n",
      "Generator loss 5.183621406555176\n",
      "Discriminator loss 0.8533107042312622\n",
      "Generator loss 5.295748710632324\n",
      "Discriminator loss 0.9133555889129639\n",
      "Generator loss 5.949550151824951\n",
      "Discriminator loss 0.840945303440094\n",
      "Generator loss 5.8523268699646\n",
      "Discriminator loss 0.854274332523346\n",
      "Generator loss 6.025162696838379\n",
      "Discriminator loss 0.8562728762626648\n",
      "Generator loss 5.366770267486572\n",
      "Discriminator loss 0.7301312685012817\n",
      "Generator loss 5.5013933181762695\n",
      "Discriminator loss 0.8657810688018799\n",
      "Generator loss 5.2856597900390625\n",
      "Discriminator loss 1.0103050470352173\n",
      "Generator loss 5.345219135284424\n",
      "Discriminator loss 0.752082109451294\n",
      "Generator loss 6.215706825256348\n",
      "Discriminator loss 0.8699328303337097\n",
      "Generator loss 5.2359185218811035\n",
      "Discriminator loss 0.8025920391082764\n",
      "Generator loss 5.531752109527588\n",
      "Discriminator loss 0.7871810793876648\n",
      "Generator loss 5.801769733428955\n",
      "Discriminator loss 0.8205584287643433\n",
      "Generator loss 5.527228832244873\n",
      "Discriminator loss 0.7519066333770752\n",
      "Generator loss 5.538451194763184\n",
      "Discriminator loss 0.8578999042510986\n",
      "Generator loss 6.118020057678223\n",
      "Discriminator loss 0.7884175181388855\n",
      "Generator loss 5.740371227264404\n",
      "Discriminator loss 0.8160821199417114\n",
      "Generator loss 5.454133033752441\n",
      "Discriminator loss 0.7467311024665833\n",
      "Generator loss 5.94545841217041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.7965023517608643\n",
      "Generator loss 6.007385730743408\n",
      "Discriminator loss 0.9350168108940125\n",
      "Generator loss 5.291852951049805\n",
      "Discriminator loss 0.8607690930366516\n",
      "Generator loss 5.8872199058532715\n",
      "Discriminator loss 0.7008488774299622\n",
      "Generator loss 5.847125053405762\n",
      "Epoch loss\n",
      "Discriminator loss 0.796623945236206\n",
      "Generator loss 5.230981826782227\n",
      "Discriminator loss 0.8733199834823608\n",
      "Generator loss 6.552877426147461\n",
      "Discriminator loss 0.7675158977508545\n",
      "Generator loss 5.708122253417969\n",
      "Discriminator loss 0.9657703638076782\n",
      "Generator loss 5.4435553550720215\n",
      "Discriminator loss 0.8605028390884399\n",
      "Generator loss 5.302698135375977\n",
      "Discriminator loss 0.731253445148468\n",
      "Generator loss 5.938672065734863\n",
      "Discriminator loss 0.8961424231529236\n",
      "Generator loss 6.277124404907227\n",
      "Discriminator loss 0.7844753861427307\n",
      "Generator loss 5.313469409942627\n",
      "Discriminator loss 0.7980425953865051\n",
      "Generator loss 5.535571098327637\n",
      "Discriminator loss 0.7818599343299866\n",
      "Generator loss 6.073249816894531\n",
      "Discriminator loss 0.8740230798721313\n",
      "Generator loss 5.2145915031433105\n",
      "Discriminator loss 0.8725407719612122\n",
      "Generator loss 5.122715473175049\n",
      "Discriminator loss 0.8481956124305725\n",
      "Generator loss 5.590737342834473\n",
      "Discriminator loss 0.7475190162658691\n",
      "Generator loss 5.4956278800964355\n",
      "Discriminator loss 0.8726065754890442\n",
      "Generator loss 5.849457740783691\n",
      "Discriminator loss 0.8641627430915833\n",
      "Generator loss 5.790472030639648\n",
      "Discriminator loss 0.778390645980835\n",
      "Generator loss 5.538572788238525\n",
      "Discriminator loss 0.8390036821365356\n",
      "Generator loss 5.678430557250977\n",
      "Discriminator loss 0.7709781527519226\n",
      "Generator loss 5.810036659240723\n",
      "Discriminator loss 0.9347230195999146\n",
      "Generator loss 5.483339786529541\n",
      "Discriminator loss 0.8285809755325317\n",
      "Generator loss 5.314746379852295\n",
      "Discriminator loss 0.8336464166641235\n",
      "Generator loss 5.563924312591553\n",
      "Discriminator loss 0.7678941488265991\n",
      "Generator loss 5.916683197021484\n",
      "Discriminator loss 0.8394723534584045\n",
      "Generator loss 5.76810359954834\n",
      "Discriminator loss 1.0220608711242676\n",
      "Generator loss 5.294861316680908\n",
      "Epoch loss\n",
      "Discriminator loss 0.7665530443191528\n",
      "Generator loss 5.209603786468506\n",
      "Discriminator loss 0.918254554271698\n",
      "Generator loss 5.364547252655029\n",
      "Discriminator loss 0.7352451086044312\n",
      "Generator loss 5.362978458404541\n",
      "Discriminator loss 0.8481538891792297\n",
      "Generator loss 5.358062267303467\n",
      "Discriminator loss 0.7490062713623047\n",
      "Generator loss 4.912585258483887\n",
      "Discriminator loss 0.8540493249893188\n",
      "Generator loss 5.630769729614258\n",
      "Discriminator loss 0.7864466905593872\n",
      "Generator loss 5.605408191680908\n",
      "Discriminator loss 0.7597357034683228\n",
      "Generator loss 5.3163676261901855\n",
      "Discriminator loss 0.8021690845489502\n",
      "Generator loss 5.004714488983154\n",
      "Discriminator loss 0.7866788506507874\n",
      "Generator loss 6.036913871765137\n",
      "Discriminator loss 0.7485768795013428\n",
      "Generator loss 5.536738395690918\n",
      "Discriminator loss 0.9377199411392212\n",
      "Generator loss 5.584839820861816\n",
      "Discriminator loss 0.9168329834938049\n",
      "Generator loss 5.364152431488037\n",
      "Discriminator loss 0.7958778738975525\n",
      "Generator loss 5.6530070304870605\n",
      "Discriminator loss 0.7786558866500854\n",
      "Generator loss 6.168800354003906\n",
      "Discriminator loss 0.9072430729866028\n",
      "Generator loss 5.18794584274292\n",
      "Discriminator loss 0.8265208601951599\n",
      "Generator loss 5.993251323699951\n",
      "Discriminator loss 0.8715368509292603\n",
      "Generator loss 5.1135125160217285\n",
      "Discriminator loss 0.8207524418830872\n",
      "Generator loss 5.442898750305176\n",
      "Discriminator loss 0.7604209184646606\n",
      "Generator loss 5.711089611053467\n",
      "Discriminator loss 0.8095152378082275\n",
      "Generator loss 5.058242321014404\n",
      "Discriminator loss 0.8999999165534973\n",
      "Generator loss 5.204091548919678\n",
      "Discriminator loss 0.8318676352500916\n",
      "Generator loss 5.840498924255371\n",
      "Discriminator loss 0.768428385257721\n",
      "Generator loss 6.005250453948975\n",
      "Discriminator loss 0.760405421257019\n",
      "Generator loss 6.03373908996582\n",
      "Epoch loss\n",
      "Discriminator loss 0.9327852725982666\n",
      "Generator loss 5.703577041625977\n",
      "Discriminator loss 0.7261426448822021\n",
      "Generator loss 5.98878288269043\n",
      "Discriminator loss 0.7795976996421814\n",
      "Generator loss 5.844419956207275\n",
      "Discriminator loss 0.760944664478302\n",
      "Generator loss 5.615118026733398\n",
      "Discriminator loss 0.9194884896278381\n",
      "Generator loss 5.495652675628662\n",
      "Discriminator loss 0.77796471118927\n",
      "Generator loss 5.492082595825195\n",
      "Discriminator loss 0.830912709236145\n",
      "Generator loss 5.720159530639648\n",
      "Discriminator loss 0.8111717700958252\n",
      "Generator loss 5.572795391082764\n",
      "Discriminator loss 0.7607633471488953\n",
      "Generator loss 5.379192352294922\n",
      "Discriminator loss 0.810052216053009\n",
      "Generator loss 5.505871295928955\n",
      "Discriminator loss 0.9180731177330017\n",
      "Generator loss 5.585619926452637\n",
      "Discriminator loss 0.7352398633956909\n",
      "Generator loss 5.9452667236328125\n",
      "Discriminator loss 0.9009009003639221\n",
      "Generator loss 5.201951026916504\n",
      "Discriminator loss 0.7001276016235352\n",
      "Generator loss 5.896686553955078\n",
      "Discriminator loss 0.7370238900184631\n",
      "Generator loss 5.875922203063965\n",
      "Discriminator loss 1.004555583000183\n",
      "Generator loss 5.511508941650391\n",
      "Discriminator loss 0.7883031368255615\n",
      "Generator loss 5.766887187957764\n",
      "Discriminator loss 0.8340995907783508\n",
      "Generator loss 5.561748504638672\n",
      "Discriminator loss 0.7239686250686646\n",
      "Generator loss 5.228837013244629\n",
      "Discriminator loss 0.8257988095283508\n",
      "Generator loss 5.043932914733887\n",
      "Discriminator loss 0.8471180200576782\n",
      "Generator loss 5.367275238037109\n",
      "Discriminator loss 0.7968721985816956\n",
      "Generator loss 5.157011985778809\n",
      "Discriminator loss 0.7412824630737305\n",
      "Generator loss 5.7822723388671875\n",
      "Discriminator loss 0.7757434844970703\n",
      "Generator loss 5.0919694900512695\n",
      "Discriminator loss 0.8589990735054016\n",
      "Generator loss 6.2128777503967285\n",
      "Epoch loss\n",
      "Discriminator loss 0.8458272218704224\n",
      "Generator loss 5.743685245513916\n",
      "Discriminator loss 0.9643975496292114\n",
      "Generator loss 5.9133195877075195\n",
      "Discriminator loss 0.8564019799232483\n",
      "Generator loss 5.679262161254883\n",
      "Discriminator loss 0.855076789855957\n",
      "Generator loss 5.397228717803955\n",
      "Discriminator loss 0.8249732255935669\n",
      "Generator loss 5.571168899536133\n",
      "Discriminator loss 0.837815523147583\n",
      "Generator loss 5.617326736450195\n",
      "Discriminator loss 0.7370696663856506\n",
      "Generator loss 5.922598838806152\n",
      "Discriminator loss 0.8525351881980896\n",
      "Generator loss 5.107629299163818\n",
      "Discriminator loss 0.7186683416366577\n",
      "Generator loss 5.975724220275879\n",
      "Discriminator loss 0.9646512866020203\n",
      "Generator loss 4.962285041809082\n",
      "Discriminator loss 0.7435199618339539\n",
      "Generator loss 5.444723129272461\n",
      "Discriminator loss 0.8096156120300293\n",
      "Generator loss 5.804196357727051\n",
      "Discriminator loss 0.8639628887176514\n",
      "Generator loss 5.088215351104736\n",
      "Discriminator loss 0.8623114824295044\n",
      "Generator loss 5.261664867401123\n",
      "Discriminator loss 0.833582878112793\n",
      "Generator loss 5.582337856292725\n",
      "Discriminator loss 0.800406813621521\n",
      "Generator loss 5.206974506378174\n",
      "Discriminator loss 0.9073406457901001\n",
      "Generator loss 5.095052242279053\n",
      "Discriminator loss 0.7534602880477905\n",
      "Generator loss 5.39662504196167\n",
      "Discriminator loss 0.7603622674942017\n",
      "Generator loss 5.424525260925293\n",
      "Discriminator loss 0.8199598789215088\n",
      "Generator loss 4.979953289031982\n",
      "Discriminator loss 0.6891227960586548\n",
      "Generator loss 5.145021915435791\n",
      "Discriminator loss 0.8013941645622253\n",
      "Generator loss 5.714409351348877\n",
      "Discriminator loss 0.7815775871276855\n",
      "Generator loss 5.110384941101074\n",
      "Discriminator loss 0.8739286661148071\n",
      "Generator loss 5.238466262817383\n",
      "Discriminator loss 0.8600866794586182\n",
      "Generator loss 4.882006645202637\n",
      "Epoch loss\n",
      "Discriminator loss 0.7040076851844788\n",
      "Generator loss 5.091121673583984\n",
      "Discriminator loss 0.7533532381057739\n",
      "Generator loss 5.553772449493408\n",
      "Discriminator loss 0.81687992811203\n",
      "Generator loss 5.940430641174316\n",
      "Discriminator loss 0.8645576238632202\n",
      "Generator loss 4.9356465339660645\n",
      "Discriminator loss 0.8424928784370422\n",
      "Generator loss 5.321990489959717\n",
      "Discriminator loss 0.863562285900116\n",
      "Generator loss 5.411862373352051\n",
      "Discriminator loss 0.8616284728050232\n",
      "Generator loss 5.771849632263184\n",
      "Discriminator loss 0.8222898244857788\n",
      "Generator loss 5.5543341636657715\n",
      "Discriminator loss 0.6682006120681763\n",
      "Generator loss 5.67800760269165\n",
      "Discriminator loss 0.7370355725288391\n",
      "Generator loss 4.956699848175049\n",
      "Discriminator loss 0.7987831234931946\n",
      "Generator loss 5.073692798614502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.8107428550720215\n",
      "Generator loss 6.225451469421387\n",
      "Discriminator loss 1.007028579711914\n",
      "Generator loss 5.267165184020996\n",
      "Discriminator loss 0.9577012062072754\n",
      "Generator loss 5.131073951721191\n",
      "Discriminator loss 0.7937965989112854\n",
      "Generator loss 5.778580665588379\n",
      "Discriminator loss 1.070001244544983\n",
      "Generator loss 4.888952255249023\n",
      "Discriminator loss 0.7719207406044006\n",
      "Generator loss 6.058037281036377\n",
      "Discriminator loss 0.794011116027832\n",
      "Generator loss 5.476858615875244\n",
      "Discriminator loss 0.7715946435928345\n",
      "Generator loss 4.999526023864746\n",
      "Discriminator loss 0.9555970430374146\n",
      "Generator loss 5.594051361083984\n",
      "Discriminator loss 0.793639600276947\n",
      "Generator loss 5.125926971435547\n",
      "Discriminator loss 0.7915700674057007\n",
      "Generator loss 5.1835832595825195\n",
      "Discriminator loss 0.755246639251709\n",
      "Generator loss 5.619555473327637\n",
      "Discriminator loss 0.8838896155357361\n",
      "Generator loss 5.744688987731934\n",
      "Discriminator loss 0.8563362956047058\n",
      "Generator loss 5.307394504547119\n",
      "Epoch loss\n",
      "Discriminator loss 1.0236467123031616\n",
      "Generator loss 5.275905132293701\n",
      "Discriminator loss 0.876408040523529\n",
      "Generator loss 5.691354751586914\n",
      "Discriminator loss 0.8850961327552795\n",
      "Generator loss 4.924881458282471\n",
      "Discriminator loss 0.7668019533157349\n",
      "Generator loss 5.870562553405762\n",
      "Discriminator loss 0.8301370143890381\n",
      "Generator loss 5.195004940032959\n",
      "Discriminator loss 0.7491628527641296\n",
      "Generator loss 5.435004711151123\n",
      "Discriminator loss 0.9378067255020142\n",
      "Generator loss 5.2142415046691895\n",
      "Discriminator loss 0.7607260942459106\n",
      "Generator loss 5.419811725616455\n",
      "Discriminator loss 0.7661659717559814\n",
      "Generator loss 5.324365139007568\n",
      "Discriminator loss 0.8473896980285645\n",
      "Generator loss 4.781100273132324\n",
      "Discriminator loss 0.7214237451553345\n",
      "Generator loss 5.435704231262207\n",
      "Discriminator loss 0.7952399253845215\n",
      "Generator loss 5.243600368499756\n",
      "Discriminator loss 0.9689501523971558\n",
      "Generator loss 5.559313774108887\n",
      "Discriminator loss 0.8060082197189331\n",
      "Generator loss 5.9119486808776855\n",
      "Discriminator loss 0.9764920473098755\n",
      "Generator loss 5.470983982086182\n",
      "Discriminator loss 0.7570980787277222\n",
      "Generator loss 5.569551467895508\n",
      "Discriminator loss 0.8401471972465515\n",
      "Generator loss 5.334286689758301\n",
      "Discriminator loss 0.9986361265182495\n",
      "Generator loss 5.671456813812256\n",
      "Discriminator loss 0.8536823987960815\n",
      "Generator loss 5.103438377380371\n",
      "Discriminator loss 0.83735191822052\n",
      "Generator loss 5.204263210296631\n",
      "Discriminator loss 0.8440396189689636\n",
      "Generator loss 5.385205268859863\n",
      "Discriminator loss 0.8935258984565735\n",
      "Generator loss 5.404374122619629\n",
      "Discriminator loss 0.7787067294120789\n",
      "Generator loss 5.0693678855896\n",
      "Discriminator loss 0.7356501817703247\n",
      "Generator loss 5.751837730407715\n",
      "Discriminator loss 0.7853710055351257\n",
      "Generator loss 5.449950218200684\n",
      "Epoch loss\n",
      "Discriminator loss 0.9080305099487305\n",
      "Generator loss 5.901345729827881\n",
      "Discriminator loss 0.9084844589233398\n",
      "Generator loss 5.770787239074707\n",
      "Discriminator loss 0.6882954835891724\n",
      "Generator loss 5.734526634216309\n",
      "Discriminator loss 0.8365297317504883\n",
      "Generator loss 5.0010552406311035\n",
      "Discriminator loss 0.8359252214431763\n",
      "Generator loss 5.239558696746826\n",
      "Discriminator loss 0.9311974048614502\n",
      "Generator loss 5.218455791473389\n",
      "Discriminator loss 0.9964941143989563\n",
      "Generator loss 5.58211612701416\n",
      "Discriminator loss 0.7520123720169067\n",
      "Generator loss 5.151513576507568\n",
      "Discriminator loss 0.7130939364433289\n",
      "Generator loss 5.185624122619629\n",
      "Discriminator loss 0.824732780456543\n",
      "Generator loss 5.711128234863281\n",
      "Discriminator loss 0.8125239610671997\n",
      "Generator loss 5.195067405700684\n",
      "Discriminator loss 0.9464578032493591\n",
      "Generator loss 5.7041521072387695\n",
      "Discriminator loss 0.7977110147476196\n",
      "Generator loss 5.030690670013428\n",
      "Discriminator loss 0.9818544983863831\n",
      "Generator loss 4.86981725692749\n",
      "Discriminator loss 0.6733120679855347\n",
      "Generator loss 5.608637809753418\n",
      "Discriminator loss 0.7555716037750244\n",
      "Generator loss 5.185591220855713\n",
      "Discriminator loss 0.8241768479347229\n",
      "Generator loss 5.711784362792969\n",
      "Discriminator loss 0.8897669911384583\n",
      "Generator loss 5.507044792175293\n",
      "Discriminator loss 0.7655929327011108\n",
      "Generator loss 5.215031623840332\n",
      "Discriminator loss 0.7791350483894348\n",
      "Generator loss 5.820615291595459\n",
      "Discriminator loss 0.8921959400177002\n",
      "Generator loss 5.606553077697754\n",
      "Discriminator loss 0.667214035987854\n",
      "Generator loss 5.082647323608398\n",
      "Discriminator loss 0.7995308637619019\n",
      "Generator loss 4.877438068389893\n",
      "Discriminator loss 0.8030285239219666\n",
      "Generator loss 5.357780933380127\n",
      "Discriminator loss 0.8977801203727722\n",
      "Generator loss 5.542409896850586\n",
      "Epoch loss\n",
      "Discriminator loss 0.7854624390602112\n",
      "Generator loss 5.244514465332031\n",
      "Discriminator loss 0.8768840432167053\n",
      "Generator loss 5.680148601531982\n",
      "Discriminator loss 0.8570284247398376\n",
      "Generator loss 5.4323506355285645\n",
      "Discriminator loss 0.8137970566749573\n",
      "Generator loss 4.721134185791016\n",
      "Discriminator loss 0.9398771524429321\n",
      "Generator loss 5.309695243835449\n",
      "Discriminator loss 0.7662125825881958\n",
      "Generator loss 5.473358154296875\n",
      "Discriminator loss 0.9142735004425049\n",
      "Generator loss 4.899690628051758\n",
      "Discriminator loss 0.7665988206863403\n",
      "Generator loss 5.120513916015625\n",
      "Discriminator loss 0.8858765363693237\n",
      "Generator loss 5.518035411834717\n",
      "Discriminator loss 0.8064566850662231\n",
      "Generator loss 4.996899604797363\n",
      "Discriminator loss 0.794514000415802\n",
      "Generator loss 4.327782154083252\n",
      "Discriminator loss 0.6994321346282959\n",
      "Generator loss 5.311591625213623\n",
      "Discriminator loss 0.6862099766731262\n",
      "Generator loss 5.3395915031433105\n",
      "Discriminator loss 0.769256591796875\n",
      "Generator loss 5.473200798034668\n",
      "Discriminator loss 0.8122133016586304\n",
      "Generator loss 4.7291259765625\n",
      "Discriminator loss 1.0224519968032837\n",
      "Generator loss 5.406306266784668\n",
      "Discriminator loss 0.7906168103218079\n",
      "Generator loss 5.173203468322754\n",
      "Discriminator loss 0.7646858096122742\n",
      "Generator loss 5.249324798583984\n",
      "Discriminator loss 0.9571795463562012\n",
      "Generator loss 5.3514485359191895\n",
      "Discriminator loss 0.9847991466522217\n",
      "Generator loss 5.314496994018555\n",
      "Discriminator loss 0.8636499047279358\n",
      "Generator loss 5.04234504699707\n",
      "Discriminator loss 0.8961941599845886\n",
      "Generator loss 5.259510040283203\n",
      "Discriminator loss 0.7605701088905334\n",
      "Generator loss 6.079596042633057\n",
      "Discriminator loss 0.8986673355102539\n",
      "Generator loss 5.250933647155762\n",
      "Discriminator loss 0.6946495771408081\n",
      "Generator loss 5.681117534637451\n",
      "Epoch loss\n",
      "Discriminator loss 0.8033857345581055\n",
      "Generator loss 5.172832489013672\n",
      "Discriminator loss 0.9044626951217651\n",
      "Generator loss 5.1250081062316895\n",
      "Discriminator loss 0.8545657992362976\n",
      "Generator loss 5.067939758300781\n",
      "Discriminator loss 0.8371336460113525\n",
      "Generator loss 5.20560359954834\n",
      "Discriminator loss 0.9247908592224121\n",
      "Generator loss 5.857150077819824\n",
      "Discriminator loss 0.801869809627533\n",
      "Generator loss 5.537276268005371\n",
      "Discriminator loss 0.7931532859802246\n",
      "Generator loss 5.190685272216797\n",
      "Discriminator loss 0.9232842326164246\n",
      "Generator loss 4.6657819747924805\n",
      "Discriminator loss 0.7229465246200562\n",
      "Generator loss 4.830709457397461\n",
      "Discriminator loss 0.8300183415412903\n",
      "Generator loss 5.6166090965271\n",
      "Discriminator loss 0.7495884299278259\n",
      "Generator loss 4.9429473876953125\n",
      "Discriminator loss 0.9150702357292175\n",
      "Generator loss 5.539121150970459\n",
      "Discriminator loss 0.8728322386741638\n",
      "Generator loss 4.844128131866455\n",
      "Discriminator loss 0.8413937091827393\n",
      "Generator loss 5.442582607269287\n",
      "Discriminator loss 0.8217781782150269\n",
      "Generator loss 5.434597492218018\n",
      "Discriminator loss 1.1234683990478516\n",
      "Generator loss 5.334325790405273\n",
      "Discriminator loss 0.7693259119987488\n",
      "Generator loss 5.697444915771484\n",
      "Discriminator loss 0.8911645412445068\n",
      "Generator loss 5.1285834312438965\n",
      "Discriminator loss 0.7457228899002075\n",
      "Generator loss 5.548807144165039\n",
      "Discriminator loss 0.8015204668045044\n",
      "Generator loss 5.996212005615234\n",
      "Discriminator loss 0.8532682061195374\n",
      "Generator loss 5.589460849761963\n",
      "Discriminator loss 0.8925086259841919\n",
      "Generator loss 4.889240264892578\n",
      "Discriminator loss 0.8433504700660706\n",
      "Generator loss 5.139895439147949\n",
      "Discriminator loss 0.7743037939071655\n",
      "Generator loss 5.339343547821045\n",
      "Discriminator loss 0.7997385263442993\n",
      "Generator loss 5.244604110717773\n",
      "Epoch loss\n",
      "Discriminator loss 0.8815322518348694\n",
      "Generator loss 4.924612522125244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.8010014891624451\n",
      "Generator loss 5.158206462860107\n",
      "Discriminator loss 0.822791576385498\n",
      "Generator loss 4.455216407775879\n",
      "Discriminator loss 0.8153491020202637\n",
      "Generator loss 5.3305230140686035\n",
      "Discriminator loss 0.9341370463371277\n",
      "Generator loss 5.420832633972168\n",
      "Discriminator loss 0.9440594911575317\n",
      "Generator loss 4.889688491821289\n",
      "Discriminator loss 0.8283994793891907\n",
      "Generator loss 5.251354217529297\n",
      "Discriminator loss 0.8638500571250916\n",
      "Generator loss 5.189924240112305\n",
      "Discriminator loss 0.7777554392814636\n",
      "Generator loss 5.390909194946289\n",
      "Discriminator loss 0.9443069100379944\n",
      "Generator loss 5.620785713195801\n",
      "Discriminator loss 0.7057256102561951\n",
      "Generator loss 5.445486068725586\n",
      "Discriminator loss 0.8449733853340149\n",
      "Generator loss 5.075437545776367\n",
      "Discriminator loss 0.7900505065917969\n",
      "Generator loss 5.076018810272217\n",
      "Discriminator loss 0.7990109920501709\n",
      "Generator loss 4.972801208496094\n",
      "Discriminator loss 0.8075703382492065\n",
      "Generator loss 5.375429630279541\n",
      "Discriminator loss 0.9576852321624756\n",
      "Generator loss 5.234917163848877\n",
      "Discriminator loss 0.7816928029060364\n",
      "Generator loss 5.271450042724609\n",
      "Discriminator loss 0.8253272771835327\n",
      "Generator loss 5.243015766143799\n",
      "Discriminator loss 0.8408491611480713\n",
      "Generator loss 4.846652030944824\n",
      "Discriminator loss 0.7879528999328613\n",
      "Generator loss 4.791813850402832\n",
      "Discriminator loss 0.6994554996490479\n",
      "Generator loss 5.5159912109375\n",
      "Discriminator loss 0.9269823431968689\n",
      "Generator loss 5.40147066116333\n",
      "Discriminator loss 0.9963258504867554\n",
      "Generator loss 4.862821578979492\n",
      "Discriminator loss 0.8825952410697937\n",
      "Generator loss 5.001805305480957\n",
      "Discriminator loss 0.9490079283714294\n",
      "Generator loss 5.173605918884277\n",
      "Epoch loss\n",
      "Discriminator loss 0.8015574812889099\n",
      "Generator loss 4.380093097686768\n",
      "Discriminator loss 0.9230337142944336\n",
      "Generator loss 4.8779826164245605\n",
      "Discriminator loss 0.8214951157569885\n",
      "Generator loss 4.498342514038086\n",
      "Discriminator loss 0.789421796798706\n",
      "Generator loss 4.774299621582031\n",
      "Discriminator loss 0.8535866141319275\n",
      "Generator loss 4.608748912811279\n",
      "Discriminator loss 0.8195687532424927\n",
      "Generator loss 4.539604663848877\n",
      "Discriminator loss 1.0137027502059937\n",
      "Generator loss 4.602439880371094\n",
      "Discriminator loss 0.7180581092834473\n",
      "Generator loss 4.548602104187012\n",
      "Discriminator loss 0.9512340426445007\n",
      "Generator loss 5.347606182098389\n",
      "Discriminator loss 0.7888575792312622\n",
      "Generator loss 5.100650310516357\n",
      "Discriminator loss 0.8655634522438049\n",
      "Generator loss 5.07604455947876\n",
      "Discriminator loss 0.983238697052002\n",
      "Generator loss 5.126446723937988\n",
      "Discriminator loss 0.8196181058883667\n",
      "Generator loss 5.3081488609313965\n",
      "Discriminator loss 0.923169732093811\n",
      "Generator loss 4.843873977661133\n",
      "Discriminator loss 0.9326544404029846\n",
      "Generator loss 5.051483154296875\n",
      "Discriminator loss 0.9103532433509827\n",
      "Generator loss 4.9483232498168945\n",
      "Discriminator loss 0.7276142835617065\n",
      "Generator loss 5.976483345031738\n",
      "Discriminator loss 0.777596652507782\n",
      "Generator loss 5.15209436416626\n",
      "Discriminator loss 0.901622474193573\n",
      "Generator loss 5.088037014007568\n",
      "Discriminator loss 0.79067462682724\n",
      "Generator loss 5.2452497482299805\n",
      "Discriminator loss 0.8007046580314636\n",
      "Generator loss 4.8426032066345215\n",
      "Discriminator loss 0.9349222779273987\n",
      "Generator loss 4.526034355163574\n",
      "Discriminator loss 0.9601492881774902\n",
      "Generator loss 5.403284549713135\n",
      "Discriminator loss 0.669540286064148\n",
      "Generator loss 4.654555320739746\n",
      "Discriminator loss 0.9408019781112671\n",
      "Generator loss 5.3228535652160645\n",
      "Epoch loss\n",
      "Discriminator loss 0.8495290279388428\n",
      "Generator loss 5.050639629364014\n",
      "Discriminator loss 0.8680804967880249\n",
      "Generator loss 5.011307716369629\n",
      "Discriminator loss 0.8380935788154602\n",
      "Generator loss 4.928877353668213\n",
      "Discriminator loss 0.8430114388465881\n",
      "Generator loss 4.514275550842285\n",
      "Discriminator loss 0.923247218132019\n",
      "Generator loss 5.297219753265381\n",
      "Discriminator loss 0.6815471649169922\n",
      "Generator loss 5.101768493652344\n",
      "Discriminator loss 0.7118383049964905\n",
      "Generator loss 5.615633964538574\n",
      "Discriminator loss 0.8405114412307739\n",
      "Generator loss 5.2745208740234375\n",
      "Discriminator loss 0.7949714064598083\n",
      "Generator loss 5.069729804992676\n",
      "Discriminator loss 0.9606400728225708\n",
      "Generator loss 5.345554828643799\n",
      "Discriminator loss 0.8572738170623779\n",
      "Generator loss 4.808596611022949\n",
      "Discriminator loss 1.0488051176071167\n",
      "Generator loss 4.886524677276611\n",
      "Discriminator loss 0.7955688238143921\n",
      "Generator loss 4.781452178955078\n",
      "Discriminator loss 0.8724809885025024\n",
      "Generator loss 4.94321346282959\n",
      "Discriminator loss 0.8896156549453735\n",
      "Generator loss 4.985193729400635\n",
      "Discriminator loss 1.0094724893569946\n",
      "Generator loss 5.669682502746582\n",
      "Discriminator loss 0.7764402627944946\n",
      "Generator loss 4.866353988647461\n",
      "Discriminator loss 0.9402682185173035\n",
      "Generator loss 4.812099456787109\n",
      "Discriminator loss 0.8118417859077454\n",
      "Generator loss 5.625484943389893\n",
      "Discriminator loss 0.794562578201294\n",
      "Generator loss 5.567310333251953\n",
      "Discriminator loss 0.8205915689468384\n",
      "Generator loss 5.020832061767578\n",
      "Discriminator loss 0.72179114818573\n",
      "Generator loss 5.201478481292725\n",
      "Discriminator loss 0.9831965565681458\n",
      "Generator loss 5.477008819580078\n",
      "Discriminator loss 0.7848958969116211\n",
      "Generator loss 4.549618721008301\n",
      "Discriminator loss 0.9432054758071899\n",
      "Generator loss 5.484350204467773\n",
      "Epoch loss\n",
      "Discriminator loss 0.8719468712806702\n",
      "Generator loss 4.563478946685791\n",
      "Discriminator loss 0.8879424929618835\n",
      "Generator loss 4.781704902648926\n",
      "Discriminator loss 0.7685091495513916\n",
      "Generator loss 5.104340553283691\n",
      "Discriminator loss 0.9565301537513733\n",
      "Generator loss 4.660590171813965\n",
      "Discriminator loss 0.7630990147590637\n",
      "Generator loss 5.009553909301758\n",
      "Discriminator loss 0.8362387418746948\n",
      "Generator loss 5.328824520111084\n",
      "Discriminator loss 0.9201779365539551\n",
      "Generator loss 6.235267639160156\n",
      "Discriminator loss 0.9176892042160034\n",
      "Generator loss 5.012845993041992\n",
      "Discriminator loss 0.7778851985931396\n",
      "Generator loss 5.050114631652832\n",
      "Discriminator loss 0.9316392540931702\n",
      "Generator loss 5.033067226409912\n",
      "Discriminator loss 0.8625321984291077\n",
      "Generator loss 5.06193208694458\n",
      "Discriminator loss 0.9778669476509094\n",
      "Generator loss 5.107790946960449\n",
      "Discriminator loss 0.9209406971931458\n",
      "Generator loss 5.361937999725342\n",
      "Discriminator loss 0.7181195020675659\n",
      "Generator loss 5.2220964431762695\n",
      "Discriminator loss 0.8779228925704956\n",
      "Generator loss 5.28371000289917\n",
      "Discriminator loss 0.7601982355117798\n",
      "Generator loss 5.419212341308594\n",
      "Discriminator loss 0.794867992401123\n",
      "Generator loss 5.169147968292236\n",
      "Discriminator loss 0.827957808971405\n",
      "Generator loss 5.271909713745117\n",
      "Discriminator loss 0.954245924949646\n",
      "Generator loss 5.023317337036133\n",
      "Discriminator loss 0.7557961344718933\n",
      "Generator loss 4.9462738037109375\n",
      "Discriminator loss 0.9382007122039795\n",
      "Generator loss 5.771952152252197\n",
      "Discriminator loss 0.9313381314277649\n",
      "Generator loss 5.1225810050964355\n",
      "Discriminator loss 0.830970823764801\n",
      "Generator loss 5.621528625488281\n",
      "Discriminator loss 0.9685913324356079\n",
      "Generator loss 4.483616828918457\n",
      "Discriminator loss 0.8900768756866455\n",
      "Generator loss 5.274238109588623\n",
      "Epoch loss\n",
      "Discriminator loss 0.8385935425758362\n",
      "Generator loss 4.784283638000488\n",
      "Discriminator loss 0.7386901378631592\n",
      "Generator loss 4.948792934417725\n",
      "Discriminator loss 0.8011847138404846\n",
      "Generator loss 5.185746192932129\n",
      "Discriminator loss 0.8182762861251831\n",
      "Generator loss 4.864511489868164\n",
      "Discriminator loss 0.9138613343238831\n",
      "Generator loss 5.04602575302124\n",
      "Discriminator loss 0.7989691495895386\n",
      "Generator loss 4.975249290466309\n",
      "Discriminator loss 0.8272430896759033\n",
      "Generator loss 4.771049499511719\n",
      "Discriminator loss 0.7776396870613098\n",
      "Generator loss 4.839315414428711\n",
      "Discriminator loss 0.894925057888031\n",
      "Generator loss 5.038275241851807\n",
      "Discriminator loss 0.8941414952278137\n",
      "Generator loss 5.099005699157715\n",
      "Discriminator loss 0.8690047860145569\n",
      "Generator loss 4.369136810302734\n",
      "Discriminator loss 0.8897223472595215\n",
      "Generator loss 5.018551349639893\n",
      "Discriminator loss 0.808961033821106\n",
      "Generator loss 5.103203773498535\n",
      "Discriminator loss 0.8046199679374695\n",
      "Generator loss 5.712049961090088\n",
      "Discriminator loss 0.8778608441352844\n",
      "Generator loss 4.749571323394775\n",
      "Discriminator loss 0.7987530827522278\n",
      "Generator loss 4.6534552574157715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.8545229434967041\n",
      "Generator loss 4.740628719329834\n",
      "Discriminator loss 0.9148563146591187\n",
      "Generator loss 4.815298080444336\n",
      "Discriminator loss 0.7438629865646362\n",
      "Generator loss 4.722710132598877\n",
      "Discriminator loss 0.8884343504905701\n",
      "Generator loss 4.930844783782959\n",
      "Discriminator loss 0.8763045072555542\n",
      "Generator loss 5.269493579864502\n",
      "Discriminator loss 0.8748648166656494\n",
      "Generator loss 4.905655860900879\n",
      "Discriminator loss 0.9527518153190613\n",
      "Generator loss 5.081620693206787\n",
      "Discriminator loss 0.7875241041183472\n",
      "Generator loss 4.866120338439941\n",
      "Discriminator loss 0.8135489225387573\n",
      "Generator loss 4.913956642150879\n",
      "Epoch loss\n",
      "Discriminator loss 1.0311461687088013\n",
      "Generator loss 5.022593975067139\n",
      "Discriminator loss 0.7287243008613586\n",
      "Generator loss 4.741430282592773\n",
      "Discriminator loss 0.8046703338623047\n",
      "Generator loss 4.925112724304199\n",
      "Discriminator loss 0.8021742105484009\n",
      "Generator loss 4.939183712005615\n",
      "Discriminator loss 0.8051961064338684\n",
      "Generator loss 4.944525241851807\n",
      "Discriminator loss 0.9477861523628235\n",
      "Generator loss 4.600882053375244\n",
      "Discriminator loss 0.9987770915031433\n",
      "Generator loss 4.870824813842773\n",
      "Discriminator loss 0.8336002826690674\n",
      "Generator loss 4.353232383728027\n",
      "Discriminator loss 0.8409711718559265\n",
      "Generator loss 4.838313579559326\n",
      "Discriminator loss 0.7538840770721436\n",
      "Generator loss 4.9250993728637695\n",
      "Discriminator loss 0.7551469206809998\n",
      "Generator loss 4.835620403289795\n",
      "Discriminator loss 0.8072659373283386\n",
      "Generator loss 4.9788055419921875\n",
      "Discriminator loss 0.8934139609336853\n",
      "Generator loss 5.226006507873535\n",
      "Discriminator loss 0.7273784875869751\n",
      "Generator loss 5.064888954162598\n",
      "Discriminator loss 0.8872835636138916\n",
      "Generator loss 5.072352886199951\n",
      "Discriminator loss 0.8762712478637695\n",
      "Generator loss 5.169207572937012\n",
      "Discriminator loss 0.8004825711250305\n",
      "Generator loss 4.931186199188232\n",
      "Discriminator loss 0.7838002443313599\n",
      "Generator loss 5.026488304138184\n",
      "Discriminator loss 0.9186398386955261\n",
      "Generator loss 4.839208126068115\n",
      "Discriminator loss 0.8156691789627075\n",
      "Generator loss 5.2482500076293945\n",
      "Discriminator loss 0.8298202157020569\n",
      "Generator loss 4.844953536987305\n",
      "Discriminator loss 0.7860427498817444\n",
      "Generator loss 5.275869369506836\n",
      "Discriminator loss 0.7292508482933044\n",
      "Generator loss 5.543337821960449\n",
      "Discriminator loss 0.8720426559448242\n",
      "Generator loss 4.852386951446533\n",
      "Discriminator loss 0.7429095506668091\n",
      "Generator loss 4.769029140472412\n",
      "Epoch loss\n",
      "Discriminator loss 0.7311877012252808\n",
      "Generator loss 4.929623603820801\n",
      "Discriminator loss 0.902053713798523\n",
      "Generator loss 4.580359935760498\n",
      "Discriminator loss 0.885856032371521\n",
      "Generator loss 4.488681316375732\n",
      "Discriminator loss 0.9224797487258911\n",
      "Generator loss 4.793118476867676\n",
      "Discriminator loss 0.8689776659011841\n",
      "Generator loss 5.0745158195495605\n",
      "Discriminator loss 0.8220009803771973\n",
      "Generator loss 4.917850971221924\n",
      "Discriminator loss 0.7868003845214844\n",
      "Generator loss 5.052449703216553\n",
      "Discriminator loss 0.7695154547691345\n",
      "Generator loss 5.100687026977539\n",
      "Discriminator loss 0.8057359457015991\n",
      "Generator loss 4.463956356048584\n",
      "Discriminator loss 0.8412883281707764\n",
      "Generator loss 4.881356239318848\n",
      "Discriminator loss 0.8272621035575867\n",
      "Generator loss 4.987672805786133\n",
      "Discriminator loss 0.7840518951416016\n",
      "Generator loss 5.175674915313721\n",
      "Discriminator loss 0.7704869508743286\n",
      "Generator loss 4.941742897033691\n",
      "Discriminator loss 0.9258546829223633\n",
      "Generator loss 4.779418468475342\n",
      "Discriminator loss 0.8984413743019104\n",
      "Generator loss 5.72435188293457\n",
      "Discriminator loss 0.8348414301872253\n",
      "Generator loss 4.904308795928955\n",
      "Discriminator loss 0.8914507627487183\n",
      "Generator loss 5.2000651359558105\n",
      "Discriminator loss 0.8034687042236328\n",
      "Generator loss 5.003344535827637\n",
      "Discriminator loss 1.0169968605041504\n",
      "Generator loss 4.902322292327881\n",
      "Discriminator loss 0.8516905307769775\n",
      "Generator loss 4.9691267013549805\n",
      "Discriminator loss 0.8413704633712769\n",
      "Generator loss 4.788947582244873\n",
      "Discriminator loss 0.8767093420028687\n",
      "Generator loss 4.913212776184082\n",
      "Discriminator loss 0.839577317237854\n",
      "Generator loss 4.524816036224365\n",
      "Discriminator loss 0.8217566609382629\n",
      "Generator loss 4.773704528808594\n",
      "Discriminator loss 0.7260920405387878\n",
      "Generator loss 5.284211158752441\n",
      "Epoch loss\n",
      "Discriminator loss 0.7839141488075256\n",
      "Generator loss 5.2934675216674805\n",
      "Discriminator loss 0.8385379314422607\n",
      "Generator loss 4.586014270782471\n",
      "Discriminator loss 0.8393743634223938\n",
      "Generator loss 4.8286519050598145\n",
      "Discriminator loss 0.8328282237052917\n",
      "Generator loss 5.08583402633667\n",
      "Discriminator loss 0.951975405216217\n",
      "Generator loss 4.461452007293701\n",
      "Discriminator loss 0.9154433012008667\n",
      "Generator loss 4.713003158569336\n",
      "Discriminator loss 0.8561643362045288\n",
      "Generator loss 4.795206546783447\n",
      "Discriminator loss 1.0701473951339722\n",
      "Generator loss 5.278941631317139\n",
      "Discriminator loss 0.8402745723724365\n",
      "Generator loss 4.859462261199951\n",
      "Discriminator loss 0.8385715484619141\n",
      "Generator loss 4.776453971862793\n",
      "Discriminator loss 0.9928301572799683\n",
      "Generator loss 5.244372367858887\n",
      "Discriminator loss 0.8543857336044312\n",
      "Generator loss 4.861024379730225\n",
      "Discriminator loss 0.8596518635749817\n",
      "Generator loss 4.602382659912109\n",
      "Discriminator loss 0.903882622718811\n",
      "Generator loss 4.221086502075195\n",
      "Discriminator loss 1.0248231887817383\n",
      "Generator loss 5.145116329193115\n",
      "Discriminator loss 0.8343028426170349\n",
      "Generator loss 4.863513946533203\n",
      "Discriminator loss 0.7858758568763733\n",
      "Generator loss 4.918243408203125\n",
      "Discriminator loss 0.8288483619689941\n",
      "Generator loss 5.156414985656738\n",
      "Discriminator loss 0.7559003829956055\n",
      "Generator loss 5.19821834564209\n",
      "Discriminator loss 0.8087356090545654\n",
      "Generator loss 5.163691520690918\n",
      "Discriminator loss 0.9743166565895081\n",
      "Generator loss 4.811991214752197\n",
      "Discriminator loss 0.9169672727584839\n",
      "Generator loss 4.602758407592773\n",
      "Discriminator loss 0.7851434946060181\n",
      "Generator loss 4.385045051574707\n",
      "Discriminator loss 0.8389122486114502\n",
      "Generator loss 5.007859230041504\n",
      "Discriminator loss 0.7633066177368164\n",
      "Generator loss 5.127159595489502\n",
      "Epoch loss\n",
      "Discriminator loss 0.9473391175270081\n",
      "Generator loss 4.925872325897217\n",
      "Discriminator loss 0.8541632890701294\n",
      "Generator loss 4.905911922454834\n",
      "Discriminator loss 0.9027533531188965\n",
      "Generator loss 4.29878044128418\n",
      "Discriminator loss 0.832720160484314\n",
      "Generator loss 4.778416156768799\n",
      "Discriminator loss 1.0886839628219604\n",
      "Generator loss 5.100899696350098\n",
      "Discriminator loss 0.8754898905754089\n",
      "Generator loss 4.369848251342773\n",
      "Discriminator loss 0.877099871635437\n",
      "Generator loss 4.896215915679932\n",
      "Discriminator loss 0.8318580985069275\n",
      "Generator loss 5.330861568450928\n",
      "Discriminator loss 0.8473068475723267\n",
      "Generator loss 4.67238187789917\n",
      "Discriminator loss 0.9220454096794128\n",
      "Generator loss 4.946863174438477\n",
      "Discriminator loss 0.7452043890953064\n",
      "Generator loss 5.259859085083008\n",
      "Discriminator loss 0.8821280598640442\n",
      "Generator loss 4.879093170166016\n",
      "Discriminator loss 0.9405517578125\n",
      "Generator loss 4.623441219329834\n",
      "Discriminator loss 0.7935881018638611\n",
      "Generator loss 5.037026882171631\n",
      "Discriminator loss 0.869809627532959\n",
      "Generator loss 5.001622200012207\n",
      "Discriminator loss 0.8198333382606506\n",
      "Generator loss 5.114416122436523\n",
      "Discriminator loss 0.7495486736297607\n",
      "Generator loss 5.230363368988037\n",
      "Discriminator loss 0.9204090237617493\n",
      "Generator loss 5.5693230628967285\n",
      "Discriminator loss 0.8237996101379395\n",
      "Generator loss 4.600790500640869\n",
      "Discriminator loss 0.9048037528991699\n",
      "Generator loss 4.484317302703857\n",
      "Discriminator loss 0.936551034450531\n",
      "Generator loss 5.252357482910156\n",
      "Discriminator loss 0.8158305287361145\n",
      "Generator loss 4.845630168914795\n",
      "Discriminator loss 0.8124065399169922\n",
      "Generator loss 4.6138715744018555\n",
      "Discriminator loss 0.9017308354377747\n",
      "Generator loss 5.349151134490967\n",
      "Discriminator loss 0.7751621007919312\n",
      "Generator loss 4.945128440856934\n",
      "Epoch loss\n",
      "Discriminator loss 0.860033392906189\n",
      "Generator loss 5.14082145690918\n",
      "Discriminator loss 0.8910531401634216\n",
      "Generator loss 4.525601387023926\n",
      "Discriminator loss 0.8424064517021179\n",
      "Generator loss 4.314849376678467\n",
      "Discriminator loss 0.7969483137130737\n",
      "Generator loss 4.090850830078125\n",
      "Discriminator loss 1.0353671312332153\n",
      "Generator loss 5.355457305908203\n",
      "Discriminator loss 0.8644773364067078\n",
      "Generator loss 4.943458557128906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.8906303644180298\n",
      "Generator loss 4.639670372009277\n",
      "Discriminator loss 0.8144105672836304\n",
      "Generator loss 5.006467819213867\n",
      "Discriminator loss 0.7249502539634705\n",
      "Generator loss 4.858238697052002\n",
      "Discriminator loss 0.7683765888214111\n",
      "Generator loss 5.00036096572876\n",
      "Discriminator loss 0.8491978049278259\n",
      "Generator loss 5.752389430999756\n",
      "Discriminator loss 0.7836432456970215\n",
      "Generator loss 4.492183208465576\n",
      "Discriminator loss 0.9381465911865234\n",
      "Generator loss 4.703773498535156\n",
      "Discriminator loss 0.7862087488174438\n",
      "Generator loss 4.851586818695068\n",
      "Discriminator loss 0.9705391526222229\n",
      "Generator loss 4.864732265472412\n",
      "Discriminator loss 0.9391013383865356\n",
      "Generator loss 4.733675479888916\n",
      "Discriminator loss 0.8659742474555969\n",
      "Generator loss 4.850804805755615\n",
      "Discriminator loss 0.772734522819519\n",
      "Generator loss 5.129702091217041\n",
      "Discriminator loss 0.8378052711486816\n",
      "Generator loss 4.587889194488525\n",
      "Discriminator loss 0.8715907335281372\n",
      "Generator loss 5.3703413009643555\n",
      "Discriminator loss 0.9791074395179749\n",
      "Generator loss 5.121960639953613\n",
      "Discriminator loss 0.6886124610900879\n",
      "Generator loss 5.051017761230469\n",
      "Discriminator loss 0.9495911598205566\n",
      "Generator loss 5.049002647399902\n",
      "Discriminator loss 0.7126924395561218\n",
      "Generator loss 5.082925319671631\n",
      "Discriminator loss 0.9738453030586243\n",
      "Generator loss 4.7791032791137695\n",
      "Epoch loss\n",
      "Discriminator loss 0.8555441498756409\n",
      "Generator loss 4.895482063293457\n",
      "Discriminator loss 0.9215124249458313\n",
      "Generator loss 4.583554744720459\n",
      "Discriminator loss 0.7686634659767151\n",
      "Generator loss 4.671990871429443\n",
      "Discriminator loss 0.9674866199493408\n",
      "Generator loss 5.43768310546875\n",
      "Discriminator loss 0.9877737760543823\n",
      "Generator loss 5.021567344665527\n",
      "Discriminator loss 0.8840530514717102\n",
      "Generator loss 4.843843460083008\n",
      "Discriminator loss 0.8301785588264465\n",
      "Generator loss 4.469245910644531\n",
      "Discriminator loss 0.9078622460365295\n",
      "Generator loss 5.002756118774414\n",
      "Discriminator loss 0.8899955749511719\n",
      "Generator loss 5.112111568450928\n",
      "Discriminator loss 0.8804072737693787\n",
      "Generator loss 4.718379974365234\n",
      "Discriminator loss 0.8108435273170471\n",
      "Generator loss 4.952314376831055\n",
      "Discriminator loss 0.9133468866348267\n",
      "Generator loss 4.650359630584717\n",
      "Discriminator loss 0.8110700845718384\n",
      "Generator loss 4.474391460418701\n",
      "Discriminator loss 0.9406630396842957\n",
      "Generator loss 4.68323278427124\n",
      "Discriminator loss 0.7835102677345276\n",
      "Generator loss 4.8130106925964355\n",
      "Discriminator loss 0.822109580039978\n",
      "Generator loss 4.920910835266113\n",
      "Discriminator loss 1.0401952266693115\n",
      "Generator loss 5.334042549133301\n",
      "Discriminator loss 0.900478184223175\n",
      "Generator loss 4.893923759460449\n",
      "Discriminator loss 0.8894511461257935\n",
      "Generator loss 4.923924922943115\n",
      "Discriminator loss 0.9180814623832703\n",
      "Generator loss 4.846714019775391\n",
      "Discriminator loss 0.8001848459243774\n",
      "Generator loss 4.742990970611572\n",
      "Discriminator loss 0.8211468458175659\n",
      "Generator loss 4.233882904052734\n",
      "Discriminator loss 0.7638517618179321\n",
      "Generator loss 4.8802924156188965\n",
      "Discriminator loss 0.7738825082778931\n",
      "Generator loss 4.877218246459961\n",
      "Discriminator loss 0.7616510391235352\n",
      "Generator loss 4.950173854827881\n",
      "Epoch loss\n",
      "Discriminator loss 0.7358540892601013\n",
      "Generator loss 4.253223419189453\n",
      "Discriminator loss 0.8002890944480896\n",
      "Generator loss 5.617208957672119\n",
      "Discriminator loss 0.912533164024353\n",
      "Generator loss 4.526819229125977\n",
      "Discriminator loss 0.9556260704994202\n",
      "Generator loss 4.5263166427612305\n",
      "Discriminator loss 0.990237832069397\n",
      "Generator loss 4.980268478393555\n",
      "Discriminator loss 0.8074826002120972\n",
      "Generator loss 4.710219383239746\n",
      "Discriminator loss 0.9028133153915405\n",
      "Generator loss 5.2545623779296875\n",
      "Discriminator loss 0.7677412033081055\n",
      "Generator loss 4.75147819519043\n",
      "Discriminator loss 0.7621116042137146\n",
      "Generator loss 5.1963019371032715\n",
      "Discriminator loss 0.8637709617614746\n",
      "Generator loss 4.652869701385498\n",
      "Discriminator loss 0.8925293684005737\n",
      "Generator loss 5.337440013885498\n",
      "Discriminator loss 0.8489741683006287\n",
      "Generator loss 4.5628662109375\n",
      "Discriminator loss 0.9364586472511292\n",
      "Generator loss 4.648064613342285\n",
      "Discriminator loss 0.9816217422485352\n",
      "Generator loss 4.390220642089844\n",
      "Discriminator loss 0.7866848707199097\n",
      "Generator loss 4.407316207885742\n",
      "Discriminator loss 0.9273627400398254\n",
      "Generator loss 5.028546333312988\n",
      "Discriminator loss 0.9156062006950378\n",
      "Generator loss 3.923510789871216\n",
      "Discriminator loss 0.8886935710906982\n",
      "Generator loss 4.267796039581299\n",
      "Discriminator loss 0.7529273629188538\n",
      "Generator loss 4.716810703277588\n",
      "Discriminator loss 0.8137385845184326\n",
      "Generator loss 4.560975551605225\n",
      "Discriminator loss 1.0166364908218384\n",
      "Generator loss 4.800445556640625\n",
      "Discriminator loss 0.8140987753868103\n",
      "Generator loss 5.03394079208374\n",
      "Discriminator loss 0.8633538484573364\n",
      "Generator loss 5.041001796722412\n",
      "Discriminator loss 0.8070253729820251\n",
      "Generator loss 4.52655553817749\n",
      "Discriminator loss 1.0034946203231812\n",
      "Generator loss 5.1618428230285645\n",
      "Epoch loss\n",
      "Discriminator loss 0.8847856521606445\n",
      "Generator loss 4.590330600738525\n",
      "Discriminator loss 0.7350292205810547\n",
      "Generator loss 5.056222438812256\n",
      "Discriminator loss 0.8828377723693848\n",
      "Generator loss 6.036932945251465\n",
      "Discriminator loss 0.8156696557998657\n",
      "Generator loss 4.827263355255127\n",
      "Discriminator loss 0.8315512537956238\n",
      "Generator loss 4.824737071990967\n",
      "Discriminator loss 0.8510549664497375\n",
      "Generator loss 4.780481338500977\n",
      "Discriminator loss 0.8390650749206543\n",
      "Generator loss 4.616508960723877\n",
      "Discriminator loss 0.8570306897163391\n",
      "Generator loss 4.705819606781006\n",
      "Discriminator loss 0.7835422158241272\n",
      "Generator loss 4.435805320739746\n",
      "Discriminator loss 0.7888156175613403\n",
      "Generator loss 4.419925689697266\n",
      "Discriminator loss 0.85707688331604\n",
      "Generator loss 4.597720146179199\n",
      "Discriminator loss 0.7132704854011536\n",
      "Generator loss 4.65139627456665\n",
      "Discriminator loss 0.801246166229248\n",
      "Generator loss 4.641557216644287\n",
      "Discriminator loss 0.8233677744865417\n",
      "Generator loss 4.600098133087158\n",
      "Discriminator loss 0.8962999582290649\n",
      "Generator loss 4.812747955322266\n",
      "Discriminator loss 0.8727630972862244\n",
      "Generator loss 5.491806983947754\n",
      "Discriminator loss 0.8336491584777832\n",
      "Generator loss 4.393759250640869\n",
      "Discriminator loss 0.8149741291999817\n",
      "Generator loss 4.686363220214844\n",
      "Discriminator loss 1.0255281925201416\n",
      "Generator loss 4.575776100158691\n",
      "Discriminator loss 0.7422914505004883\n",
      "Generator loss 4.765398025512695\n",
      "Discriminator loss 0.7231683731079102\n",
      "Generator loss 4.874507904052734\n",
      "Discriminator loss 0.8095899820327759\n",
      "Generator loss 4.4768218994140625\n",
      "Discriminator loss 0.7981187105178833\n",
      "Generator loss 5.334282875061035\n",
      "Discriminator loss 0.9661245346069336\n",
      "Generator loss 5.005772113800049\n",
      "Discriminator loss 0.9192768335342407\n",
      "Generator loss 5.135656833648682\n",
      "Epoch loss\n",
      "Discriminator loss 0.7680040597915649\n",
      "Generator loss 4.157886028289795\n",
      "Discriminator loss 0.7517513036727905\n",
      "Generator loss 4.7908220291137695\n",
      "Discriminator loss 0.8822396397590637\n",
      "Generator loss 4.343457221984863\n",
      "Discriminator loss 0.748400092124939\n",
      "Generator loss 5.445831775665283\n",
      "Discriminator loss 0.7892475724220276\n",
      "Generator loss 5.335978031158447\n",
      "Discriminator loss 0.8542978763580322\n",
      "Generator loss 4.68663215637207\n",
      "Discriminator loss 0.7952491641044617\n",
      "Generator loss 4.827171325683594\n",
      "Discriminator loss 1.0818692445755005\n",
      "Generator loss 4.084422588348389\n",
      "Discriminator loss 0.7555592656135559\n",
      "Generator loss 4.8420257568359375\n",
      "Discriminator loss 0.8475751280784607\n",
      "Generator loss 4.621896266937256\n",
      "Discriminator loss 0.8449140787124634\n",
      "Generator loss 4.738770961761475\n",
      "Discriminator loss 0.9267188906669617\n",
      "Generator loss 4.91046667098999\n",
      "Discriminator loss 0.7751997709274292\n",
      "Generator loss 4.556363105773926\n",
      "Discriminator loss 0.8160144686698914\n",
      "Generator loss 5.019608020782471\n",
      "Discriminator loss 0.7762958407402039\n",
      "Generator loss 5.045021057128906\n",
      "Discriminator loss 0.7960366606712341\n",
      "Generator loss 4.704498767852783\n",
      "Discriminator loss 0.8658990263938904\n",
      "Generator loss 4.529553413391113\n",
      "Discriminator loss 0.7836208939552307\n",
      "Generator loss 4.370527267456055\n",
      "Discriminator loss 0.9074444770812988\n",
      "Generator loss 4.785428047180176\n",
      "Discriminator loss 0.7429016828536987\n",
      "Generator loss 4.7571258544921875\n",
      "Discriminator loss 0.7783584594726562\n",
      "Generator loss 5.125422477722168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.8800134062767029\n",
      "Generator loss 4.47133731842041\n",
      "Discriminator loss 0.732567310333252\n",
      "Generator loss 4.704921722412109\n",
      "Discriminator loss 0.9934558868408203\n",
      "Generator loss 4.212327003479004\n",
      "Discriminator loss 0.870566725730896\n",
      "Generator loss 4.24415922164917\n",
      "Epoch loss\n",
      "Discriminator loss 0.7417029142379761\n",
      "Generator loss 4.576262474060059\n",
      "Discriminator loss 0.8008600473403931\n",
      "Generator loss 4.742727756500244\n",
      "Discriminator loss 0.8060302734375\n",
      "Generator loss 4.57005500793457\n",
      "Discriminator loss 0.9823094606399536\n",
      "Generator loss 4.936155796051025\n",
      "Discriminator loss 0.8260598182678223\n",
      "Generator loss 4.939733505249023\n",
      "Discriminator loss 0.8500334620475769\n",
      "Generator loss 5.377714157104492\n",
      "Discriminator loss 0.7599782347679138\n",
      "Generator loss 5.170180797576904\n",
      "Discriminator loss 0.7213099002838135\n",
      "Generator loss 4.6989359855651855\n",
      "Discriminator loss 0.7645156979560852\n",
      "Generator loss 4.9888811111450195\n",
      "Discriminator loss 0.943010151386261\n",
      "Generator loss 4.858019828796387\n",
      "Discriminator loss 0.7957881689071655\n",
      "Generator loss 4.474289417266846\n",
      "Discriminator loss 0.8696459531784058\n",
      "Generator loss 4.695727348327637\n",
      "Discriminator loss 0.7292689681053162\n",
      "Generator loss 4.520365238189697\n",
      "Discriminator loss 0.8966179490089417\n",
      "Generator loss 4.683247089385986\n",
      "Discriminator loss 0.8738267421722412\n",
      "Generator loss 4.205149173736572\n",
      "Discriminator loss 0.9409769177436829\n",
      "Generator loss 4.232851982116699\n",
      "Discriminator loss 0.8854988217353821\n",
      "Generator loss 5.204128742218018\n",
      "Discriminator loss 0.9587064981460571\n",
      "Generator loss 4.958288192749023\n",
      "Discriminator loss 0.702547550201416\n",
      "Generator loss 4.959331035614014\n",
      "Discriminator loss 0.837152898311615\n",
      "Generator loss 4.560266971588135\n",
      "Discriminator loss 0.8683120012283325\n",
      "Generator loss 4.638594150543213\n",
      "Discriminator loss 0.7528248429298401\n",
      "Generator loss 4.681169033050537\n",
      "Discriminator loss 0.848175048828125\n",
      "Generator loss 4.972007751464844\n",
      "Discriminator loss 0.971419095993042\n",
      "Generator loss 4.784734725952148\n",
      "Discriminator loss 0.8064664006233215\n",
      "Generator loss 4.877200126647949\n",
      "Epoch loss\n",
      "Discriminator loss 1.0609989166259766\n",
      "Generator loss 4.218921184539795\n",
      "Discriminator loss 0.8823556900024414\n",
      "Generator loss 4.89443826675415\n",
      "Discriminator loss 0.833767294883728\n",
      "Generator loss 4.424518585205078\n",
      "Discriminator loss 0.9394460916519165\n",
      "Generator loss 4.419537544250488\n",
      "Discriminator loss 0.8979246616363525\n",
      "Generator loss 4.3345441818237305\n",
      "Discriminator loss 0.8424752950668335\n",
      "Generator loss 5.161359786987305\n",
      "Discriminator loss 0.9269113540649414\n",
      "Generator loss 5.231644153594971\n",
      "Discriminator loss 0.8317891955375671\n",
      "Generator loss 4.384942531585693\n",
      "Discriminator loss 0.9280998110771179\n",
      "Generator loss 4.8262619972229\n",
      "Discriminator loss 0.8665809631347656\n",
      "Generator loss 4.69190788269043\n",
      "Discriminator loss 0.9744701385498047\n",
      "Generator loss 5.5128278732299805\n",
      "Discriminator loss 0.9493440389633179\n",
      "Generator loss 4.7198309898376465\n",
      "Discriminator loss 0.8602532744407654\n",
      "Generator loss 5.067768573760986\n",
      "Discriminator loss 1.0504045486450195\n",
      "Generator loss 4.1060309410095215\n",
      "Discriminator loss 0.9840653538703918\n",
      "Generator loss 4.670835971832275\n",
      "Discriminator loss 0.8640819787979126\n",
      "Generator loss 4.4370293617248535\n",
      "Discriminator loss 0.9114074110984802\n",
      "Generator loss 4.7823991775512695\n",
      "Discriminator loss 0.8468747138977051\n",
      "Generator loss 4.328016757965088\n",
      "Discriminator loss 0.8378252983093262\n",
      "Generator loss 4.501291751861572\n",
      "Discriminator loss 0.8680480718612671\n",
      "Generator loss 4.420539855957031\n",
      "Discriminator loss 0.8604668974876404\n",
      "Generator loss 4.458244800567627\n",
      "Discriminator loss 0.966624915599823\n",
      "Generator loss 4.640407085418701\n",
      "Discriminator loss 0.8674876093864441\n",
      "Generator loss 4.21607780456543\n",
      "Discriminator loss 0.8640949130058289\n",
      "Generator loss 5.050535202026367\n",
      "Discriminator loss 0.8968801498413086\n",
      "Generator loss 4.891884803771973\n",
      "Epoch loss\n",
      "Discriminator loss 0.9428046941757202\n",
      "Generator loss 4.483612537384033\n",
      "Discriminator loss 0.982286810874939\n",
      "Generator loss 4.115261077880859\n",
      "Discriminator loss 0.8199917674064636\n",
      "Generator loss 4.754371643066406\n",
      "Discriminator loss 0.8888339996337891\n",
      "Generator loss 4.569561958312988\n",
      "Discriminator loss 0.900734543800354\n",
      "Generator loss 4.661189556121826\n",
      "Discriminator loss 0.7632372379302979\n",
      "Generator loss 4.40767765045166\n",
      "Discriminator loss 0.743192732334137\n",
      "Generator loss 4.424136161804199\n",
      "Discriminator loss 0.865247368812561\n",
      "Generator loss 4.339476585388184\n",
      "Discriminator loss 0.7909208536148071\n",
      "Generator loss 4.827664852142334\n",
      "Discriminator loss 0.945607602596283\n",
      "Generator loss 4.068462371826172\n",
      "Discriminator loss 0.8575972318649292\n",
      "Generator loss 4.524102210998535\n",
      "Discriminator loss 0.7728720307350159\n",
      "Generator loss 4.6162590980529785\n",
      "Discriminator loss 0.7858768105506897\n",
      "Generator loss 4.608031272888184\n",
      "Discriminator loss 1.0176249742507935\n",
      "Generator loss 4.411700248718262\n",
      "Discriminator loss 0.9540785551071167\n",
      "Generator loss 5.188407897949219\n",
      "Discriminator loss 0.8084403872489929\n",
      "Generator loss 4.407641887664795\n",
      "Discriminator loss 0.9191325306892395\n",
      "Generator loss 4.952771186828613\n",
      "Discriminator loss 0.810438334941864\n",
      "Generator loss 4.576006889343262\n",
      "Discriminator loss 0.9794655442237854\n",
      "Generator loss 4.768150806427002\n",
      "Discriminator loss 0.8877647519111633\n",
      "Generator loss 5.088686466217041\n",
      "Discriminator loss 0.7624243497848511\n",
      "Generator loss 4.842401027679443\n",
      "Discriminator loss 0.8147668838500977\n",
      "Generator loss 4.918217658996582\n",
      "Discriminator loss 0.7948440313339233\n",
      "Generator loss 5.20218563079834\n",
      "Discriminator loss 0.9418400526046753\n",
      "Generator loss 4.680675506591797\n",
      "Discriminator loss 0.8592937588691711\n",
      "Generator loss 4.69832706451416\n",
      "Epoch loss\n",
      "Discriminator loss 0.9327051639556885\n",
      "Generator loss 5.153675556182861\n",
      "Discriminator loss 0.9317371249198914\n",
      "Generator loss 4.615067958831787\n",
      "Discriminator loss 0.9850687384605408\n",
      "Generator loss 4.492258071899414\n",
      "Discriminator loss 0.8572852611541748\n",
      "Generator loss 5.395941257476807\n",
      "Discriminator loss 0.837023913860321\n",
      "Generator loss 4.742671966552734\n",
      "Discriminator loss 0.946865439414978\n",
      "Generator loss 4.419437408447266\n",
      "Discriminator loss 0.9010210633277893\n",
      "Generator loss 4.2290191650390625\n",
      "Discriminator loss 0.7701963782310486\n",
      "Generator loss 4.903275489807129\n",
      "Discriminator loss 0.871555745601654\n",
      "Generator loss 4.992105484008789\n",
      "Discriminator loss 0.7633146643638611\n",
      "Generator loss 4.347845077514648\n",
      "Discriminator loss 0.7876141667366028\n",
      "Generator loss 5.107058048248291\n",
      "Discriminator loss 0.8766984939575195\n",
      "Generator loss 4.668206691741943\n",
      "Discriminator loss 0.8499887585639954\n",
      "Generator loss 4.309931755065918\n",
      "Discriminator loss 0.8105090856552124\n",
      "Generator loss 4.7131757736206055\n",
      "Discriminator loss 0.8636276721954346\n",
      "Generator loss 4.938201904296875\n",
      "Discriminator loss 0.8435083627700806\n",
      "Generator loss 4.996110439300537\n",
      "Discriminator loss 0.896061897277832\n",
      "Generator loss 4.553608417510986\n",
      "Discriminator loss 0.8944017291069031\n",
      "Generator loss 4.78544282913208\n",
      "Discriminator loss 0.8311172723770142\n",
      "Generator loss 4.9129109382629395\n",
      "Discriminator loss 0.9793373942375183\n",
      "Generator loss 4.663267135620117\n",
      "Discriminator loss 0.8314959406852722\n",
      "Generator loss 4.441092014312744\n",
      "Discriminator loss 0.8160545229911804\n",
      "Generator loss 4.814484596252441\n",
      "Discriminator loss 0.7431110739707947\n",
      "Generator loss 4.739987850189209\n",
      "Discriminator loss 0.8348312377929688\n",
      "Generator loss 5.021798133850098\n",
      "Discriminator loss 0.8123207688331604\n",
      "Generator loss 4.788408279418945\n",
      "Epoch loss\n",
      "Discriminator loss 0.860017716884613\n",
      "Generator loss 4.2931742668151855\n",
      "Discriminator loss 0.838963508605957\n",
      "Generator loss 4.723464488983154\n",
      "Discriminator loss 0.7997242212295532\n",
      "Generator loss 4.524285793304443\n",
      "Discriminator loss 0.7934548258781433\n",
      "Generator loss 4.503444671630859\n",
      "Discriminator loss 0.7706036567687988\n",
      "Generator loss 4.641018867492676\n",
      "Discriminator loss 0.8500279188156128\n",
      "Generator loss 4.46287202835083\n",
      "Discriminator loss 0.8728548884391785\n",
      "Generator loss 4.254002571105957\n",
      "Discriminator loss 0.8457080125808716\n",
      "Generator loss 4.582291603088379\n",
      "Discriminator loss 0.7002559304237366\n",
      "Generator loss 4.678863525390625\n",
      "Discriminator loss 0.9738794565200806\n",
      "Generator loss 4.580250263214111\n",
      "Discriminator loss 0.964976966381073\n",
      "Generator loss 4.554502964019775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.8204589486122131\n",
      "Generator loss 4.710984706878662\n",
      "Discriminator loss 0.8047559261322021\n",
      "Generator loss 5.043751239776611\n",
      "Discriminator loss 0.9267740845680237\n",
      "Generator loss 4.756914138793945\n",
      "Discriminator loss 0.820781946182251\n",
      "Generator loss 4.734769821166992\n",
      "Discriminator loss 1.0198484659194946\n",
      "Generator loss 4.222135543823242\n",
      "Discriminator loss 0.8989124298095703\n",
      "Generator loss 4.542082786560059\n",
      "Discriminator loss 0.8125655055046082\n",
      "Generator loss 4.587017059326172\n",
      "Discriminator loss 0.8710024356842041\n",
      "Generator loss 4.694131374359131\n",
      "Discriminator loss 0.801662027835846\n",
      "Generator loss 4.498655796051025\n",
      "Discriminator loss 0.8264545798301697\n",
      "Generator loss 4.666018962860107\n",
      "Discriminator loss 0.9310503602027893\n",
      "Generator loss 3.886254072189331\n",
      "Discriminator loss 0.7792652249336243\n",
      "Generator loss 4.465196132659912\n",
      "Discriminator loss 0.8888128995895386\n",
      "Generator loss 4.581752300262451\n",
      "Discriminator loss 0.8334595561027527\n",
      "Generator loss 4.879152774810791\n",
      "Epoch loss\n",
      "Discriminator loss 0.9046207070350647\n",
      "Generator loss 4.227734088897705\n",
      "Discriminator loss 0.8805676698684692\n",
      "Generator loss 3.9606363773345947\n",
      "Discriminator loss 0.8346673846244812\n",
      "Generator loss 4.317956447601318\n",
      "Discriminator loss 0.7832084894180298\n",
      "Generator loss 4.714953422546387\n",
      "Discriminator loss 0.8032890558242798\n",
      "Generator loss 4.6646647453308105\n",
      "Discriminator loss 0.9353019595146179\n",
      "Generator loss 4.034038066864014\n",
      "Discriminator loss 0.8068314790725708\n",
      "Generator loss 4.545164585113525\n",
      "Discriminator loss 0.8658681511878967\n",
      "Generator loss 3.9739673137664795\n",
      "Discriminator loss 1.0517525672912598\n",
      "Generator loss 4.509236812591553\n",
      "Discriminator loss 0.9262802004814148\n",
      "Generator loss 4.353369235992432\n",
      "Discriminator loss 0.9516801834106445\n",
      "Generator loss 4.495821952819824\n",
      "Discriminator loss 0.7135790586471558\n",
      "Generator loss 4.3156328201293945\n",
      "Discriminator loss 0.7471277713775635\n",
      "Generator loss 4.778485298156738\n",
      "Discriminator loss 0.9092803001403809\n",
      "Generator loss 4.493331432342529\n",
      "Discriminator loss 0.8684878349304199\n",
      "Generator loss 4.540362358093262\n",
      "Discriminator loss 0.8851531744003296\n",
      "Generator loss 4.835151672363281\n",
      "Discriminator loss 0.8086770176887512\n",
      "Generator loss 4.846974849700928\n",
      "Discriminator loss 0.8307746648788452\n",
      "Generator loss 4.91165018081665\n",
      "Discriminator loss 0.840018093585968\n",
      "Generator loss 4.6319403648376465\n",
      "Discriminator loss 0.791104257106781\n",
      "Generator loss 4.719066143035889\n",
      "Discriminator loss 0.9415423274040222\n",
      "Generator loss 4.745684623718262\n",
      "Discriminator loss 0.8210801482200623\n",
      "Generator loss 4.726390838623047\n",
      "Discriminator loss 0.8210303783416748\n",
      "Generator loss 4.283871650695801\n",
      "Discriminator loss 0.8146677613258362\n",
      "Generator loss 4.411565780639648\n",
      "Discriminator loss 0.828787624835968\n",
      "Generator loss 4.960484504699707\n",
      "Epoch loss\n",
      "Discriminator loss 0.7416057586669922\n",
      "Generator loss 4.938767433166504\n",
      "Discriminator loss 0.8084752559661865\n",
      "Generator loss 4.521059036254883\n",
      "Discriminator loss 0.7685299515724182\n",
      "Generator loss 4.487144470214844\n",
      "Discriminator loss 0.8101181983947754\n",
      "Generator loss 4.171988487243652\n",
      "Discriminator loss 0.846321702003479\n",
      "Generator loss 5.1066999435424805\n",
      "Discriminator loss 0.9048126339912415\n",
      "Generator loss 4.5901923179626465\n",
      "Discriminator loss 0.8666138052940369\n",
      "Generator loss 4.712014675140381\n",
      "Discriminator loss 0.9399068355560303\n",
      "Generator loss 4.7272796630859375\n",
      "Discriminator loss 0.9523832201957703\n",
      "Generator loss 4.807323932647705\n",
      "Discriminator loss 0.819073498249054\n",
      "Generator loss 4.418368339538574\n",
      "Discriminator loss 0.8457907438278198\n",
      "Generator loss 4.2838029861450195\n",
      "Discriminator loss 0.7767489552497864\n",
      "Generator loss 4.575592041015625\n",
      "Discriminator loss 0.8265827298164368\n",
      "Generator loss 4.878753662109375\n",
      "Discriminator loss 0.7915354371070862\n",
      "Generator loss 4.664826393127441\n",
      "Discriminator loss 0.8365011215209961\n",
      "Generator loss 4.8185834884643555\n",
      "Discriminator loss 1.0423024892807007\n",
      "Generator loss 4.876510143280029\n",
      "Discriminator loss 0.9370644092559814\n",
      "Generator loss 4.773099899291992\n",
      "Discriminator loss 0.9404853582382202\n",
      "Generator loss 4.746893882751465\n",
      "Discriminator loss 0.880936324596405\n",
      "Generator loss 4.396063804626465\n",
      "Discriminator loss 0.8321536183357239\n",
      "Generator loss 4.8026299476623535\n",
      "Discriminator loss 1.062217354774475\n",
      "Generator loss 4.933152675628662\n",
      "Discriminator loss 0.8819183707237244\n",
      "Generator loss 4.755970478057861\n",
      "Discriminator loss 1.012146234512329\n",
      "Generator loss 4.756648540496826\n",
      "Discriminator loss 0.7682440876960754\n",
      "Generator loss 4.805793762207031\n",
      "Discriminator loss 0.7556437253952026\n",
      "Generator loss 4.784144401550293\n",
      "Epoch loss\n",
      "Discriminator loss 0.8180731534957886\n",
      "Generator loss 4.540834426879883\n",
      "Discriminator loss 0.7628437280654907\n",
      "Generator loss 4.955806732177734\n",
      "Discriminator loss 0.9742560982704163\n",
      "Generator loss 4.359935760498047\n",
      "Discriminator loss 0.9992761611938477\n",
      "Generator loss 4.408897399902344\n",
      "Discriminator loss 0.886081337928772\n",
      "Generator loss 4.069070816040039\n",
      "Discriminator loss 0.9048095941543579\n",
      "Generator loss 4.185786247253418\n",
      "Discriminator loss 0.9377967119216919\n",
      "Generator loss 4.2030181884765625\n",
      "Discriminator loss 0.7447230815887451\n",
      "Generator loss 4.6755571365356445\n",
      "Discriminator loss 0.7904024720191956\n",
      "Generator loss 4.705448627471924\n",
      "Discriminator loss 0.8205267190933228\n",
      "Generator loss 4.417420864105225\n",
      "Discriminator loss 0.8097811341285706\n",
      "Generator loss 4.974806308746338\n",
      "Discriminator loss 0.856406569480896\n",
      "Generator loss 4.201918125152588\n",
      "Discriminator loss 0.7907180786132812\n",
      "Generator loss 4.993341445922852\n",
      "Discriminator loss 0.8032004237174988\n",
      "Generator loss 4.085508346557617\n",
      "Discriminator loss 0.8473016023635864\n",
      "Generator loss 4.281826496124268\n",
      "Discriminator loss 0.810246467590332\n",
      "Generator loss 4.352728843688965\n",
      "Discriminator loss 0.9319396018981934\n",
      "Generator loss 4.356927394866943\n",
      "Discriminator loss 0.8704293966293335\n",
      "Generator loss 4.736716270446777\n",
      "Discriminator loss 0.9534775018692017\n",
      "Generator loss 4.195621967315674\n",
      "Discriminator loss 0.9384617209434509\n",
      "Generator loss 4.38804292678833\n",
      "Discriminator loss 0.7822232842445374\n",
      "Generator loss 4.76880407333374\n",
      "Discriminator loss 0.8409818410873413\n",
      "Generator loss 4.540116310119629\n",
      "Discriminator loss 0.860651433467865\n",
      "Generator loss 5.173904895782471\n",
      "Discriminator loss 0.9087979197502136\n",
      "Generator loss 4.5771098136901855\n",
      "Discriminator loss 0.7671416401863098\n",
      "Generator loss 4.162487030029297\n",
      "Epoch loss\n",
      "Discriminator loss 1.0220756530761719\n",
      "Generator loss 3.925138473510742\n",
      "Discriminator loss 0.8620886206626892\n",
      "Generator loss 4.8674163818359375\n",
      "Discriminator loss 0.8648775219917297\n",
      "Generator loss 4.846491813659668\n",
      "Discriminator loss 0.9237521886825562\n",
      "Generator loss 4.814636707305908\n",
      "Discriminator loss 0.9028729796409607\n",
      "Generator loss 4.799726963043213\n",
      "Discriminator loss 0.7507941722869873\n",
      "Generator loss 4.47153377532959\n",
      "Discriminator loss 0.8608623147010803\n",
      "Generator loss 4.184045314788818\n",
      "Discriminator loss 0.9050887823104858\n",
      "Generator loss 4.5540337562561035\n",
      "Discriminator loss 0.9425103068351746\n",
      "Generator loss 4.889899730682373\n",
      "Discriminator loss 0.8676959276199341\n",
      "Generator loss 4.635686874389648\n",
      "Discriminator loss 0.8719050884246826\n",
      "Generator loss 4.624375343322754\n",
      "Discriminator loss 0.8621593713760376\n",
      "Generator loss 4.503625392913818\n",
      "Discriminator loss 0.8557878136634827\n",
      "Generator loss 4.698282718658447\n",
      "Discriminator loss 0.870335042476654\n",
      "Generator loss 4.719948768615723\n",
      "Discriminator loss 1.0411796569824219\n",
      "Generator loss 4.4858598709106445\n",
      "Discriminator loss 0.8286985158920288\n",
      "Generator loss 4.140719890594482\n",
      "Discriminator loss 0.7236467599868774\n",
      "Generator loss 4.248940944671631\n",
      "Discriminator loss 0.7622591853141785\n",
      "Generator loss 4.647922992706299\n",
      "Discriminator loss 0.83353590965271\n",
      "Generator loss 4.730045318603516\n",
      "Discriminator loss 0.8202226161956787\n",
      "Generator loss 4.5834197998046875\n",
      "Discriminator loss 0.9254356026649475\n",
      "Generator loss 4.8732452392578125\n",
      "Discriminator loss 0.8487799167633057\n",
      "Generator loss 4.480539798736572\n",
      "Discriminator loss 0.8577935695648193\n",
      "Generator loss 4.690639019012451\n",
      "Discriminator loss 0.8654767870903015\n",
      "Generator loss 5.124908924102783\n",
      "Discriminator loss 0.9071937799453735\n",
      "Generator loss 5.019496917724609\n",
      "Epoch loss\n",
      "Discriminator loss 0.8717322945594788\n",
      "Generator loss 4.79667329788208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.7649492621421814\n",
      "Generator loss 4.503565311431885\n",
      "Discriminator loss 0.8398370742797852\n",
      "Generator loss 4.92773962020874\n",
      "Discriminator loss 0.8739435076713562\n",
      "Generator loss 4.333778381347656\n",
      "Discriminator loss 0.967787504196167\n",
      "Generator loss 4.303646087646484\n",
      "Discriminator loss 0.9091299772262573\n",
      "Generator loss 3.7511487007141113\n",
      "Discriminator loss 0.9119343757629395\n",
      "Generator loss 4.249995708465576\n",
      "Discriminator loss 0.7931916117668152\n",
      "Generator loss 4.252151966094971\n",
      "Discriminator loss 0.981683611869812\n",
      "Generator loss 4.077849864959717\n",
      "Discriminator loss 1.0178149938583374\n",
      "Generator loss 4.492042541503906\n",
      "Discriminator loss 0.7821770906448364\n",
      "Generator loss 4.17903470993042\n",
      "Discriminator loss 0.9153111577033997\n",
      "Generator loss 4.330174922943115\n",
      "Discriminator loss 0.8807815909385681\n",
      "Generator loss 4.673823833465576\n",
      "Discriminator loss 0.7620646357536316\n",
      "Generator loss 5.425937175750732\n",
      "Discriminator loss 0.815233588218689\n",
      "Generator loss 4.916712284088135\n",
      "Discriminator loss 0.8902472853660583\n",
      "Generator loss 4.6154351234436035\n",
      "Discriminator loss 0.8246480226516724\n",
      "Generator loss 4.306616306304932\n",
      "Discriminator loss 0.7969443202018738\n",
      "Generator loss 4.636346817016602\n",
      "Discriminator loss 0.9356784820556641\n",
      "Generator loss 4.254880428314209\n",
      "Discriminator loss 0.7787598967552185\n",
      "Generator loss 4.695672035217285\n",
      "Discriminator loss 0.835447371006012\n",
      "Generator loss 4.607422828674316\n",
      "Discriminator loss 0.9065778851509094\n",
      "Generator loss 4.136276721954346\n",
      "Discriminator loss 0.9316432476043701\n",
      "Generator loss 4.310607433319092\n",
      "Discriminator loss 1.0153470039367676\n",
      "Generator loss 4.9143171310424805\n",
      "Discriminator loss 0.9406589269638062\n",
      "Generator loss 4.7959089279174805\n",
      "Epoch loss\n",
      "Discriminator loss 0.840692937374115\n",
      "Generator loss 4.5608062744140625\n",
      "Discriminator loss 0.8894391655921936\n",
      "Generator loss 4.70780611038208\n",
      "Discriminator loss 0.8289278149604797\n",
      "Generator loss 4.918740749359131\n",
      "Discriminator loss 0.8522530198097229\n",
      "Generator loss 4.211616039276123\n",
      "Discriminator loss 0.7922588586807251\n",
      "Generator loss 4.420748233795166\n",
      "Discriminator loss 0.7587838768959045\n",
      "Generator loss 4.201143741607666\n",
      "Discriminator loss 0.8376703858375549\n",
      "Generator loss 5.0623555183410645\n",
      "Discriminator loss 0.7920291423797607\n",
      "Generator loss 4.278445720672607\n",
      "Discriminator loss 0.7729707360267639\n",
      "Generator loss 4.758201599121094\n",
      "Discriminator loss 0.8395445942878723\n",
      "Generator loss 4.646821975708008\n",
      "Discriminator loss 0.7728157043457031\n",
      "Generator loss 4.650353908538818\n",
      "Discriminator loss 0.944610059261322\n",
      "Generator loss 4.419412136077881\n",
      "Discriminator loss 0.8675350546836853\n",
      "Generator loss 4.0940985679626465\n",
      "Discriminator loss 0.911583662033081\n",
      "Generator loss 4.99094820022583\n",
      "Discriminator loss 0.813324511051178\n",
      "Generator loss 4.54822301864624\n",
      "Discriminator loss 0.8440955877304077\n",
      "Generator loss 4.206892490386963\n",
      "Discriminator loss 0.9497591257095337\n",
      "Generator loss 5.11814022064209\n",
      "Discriminator loss 0.8197997808456421\n",
      "Generator loss 4.555405139923096\n",
      "Discriminator loss 0.8612251877784729\n",
      "Generator loss 4.4066057205200195\n",
      "Discriminator loss 0.8971736431121826\n",
      "Generator loss 4.413099765777588\n",
      "Discriminator loss 1.0023908615112305\n",
      "Generator loss 4.330222129821777\n",
      "Discriminator loss 0.8914471864700317\n",
      "Generator loss 4.312864303588867\n",
      "Discriminator loss 0.9166005253791809\n",
      "Generator loss 4.806375980377197\n",
      "Discriminator loss 0.9618416428565979\n",
      "Generator loss 4.776625633239746\n",
      "Discriminator loss 0.841417133808136\n",
      "Generator loss 4.4023823738098145\n",
      "Epoch loss\n",
      "Discriminator loss 1.0001367330551147\n",
      "Generator loss 5.221429824829102\n",
      "Discriminator loss 0.8010080456733704\n",
      "Generator loss 4.2932024002075195\n",
      "Discriminator loss 0.8384953737258911\n",
      "Generator loss 3.9768295288085938\n",
      "Discriminator loss 0.8502495288848877\n",
      "Generator loss 4.699161052703857\n",
      "Discriminator loss 0.9334295988082886\n",
      "Generator loss 4.390322208404541\n",
      "Discriminator loss 0.8299620747566223\n",
      "Generator loss 4.303694725036621\n",
      "Discriminator loss 0.8576163649559021\n",
      "Generator loss 4.160707473754883\n",
      "Discriminator loss 0.7593767642974854\n",
      "Generator loss 4.60224723815918\n",
      "Discriminator loss 0.8714964389801025\n",
      "Generator loss 4.113779067993164\n",
      "Discriminator loss 0.9156521558761597\n",
      "Generator loss 3.818253993988037\n",
      "Discriminator loss 0.8295638561248779\n",
      "Generator loss 4.475038528442383\n",
      "Discriminator loss 0.8483118414878845\n",
      "Generator loss 4.508251667022705\n",
      "Discriminator loss 0.8078272938728333\n",
      "Generator loss 4.648852348327637\n",
      "Discriminator loss 0.9740616083145142\n",
      "Generator loss 4.427600860595703\n",
      "Discriminator loss 0.9269315004348755\n",
      "Generator loss 4.743824005126953\n",
      "Discriminator loss 0.9052978157997131\n",
      "Generator loss 4.868384838104248\n",
      "Discriminator loss 0.8266335129737854\n",
      "Generator loss 4.2455949783325195\n",
      "Discriminator loss 0.8292994499206543\n",
      "Generator loss 3.85668683052063\n",
      "Discriminator loss 0.8005176186561584\n",
      "Generator loss 4.47428560256958\n",
      "Discriminator loss 1.0324053764343262\n",
      "Generator loss 4.768631458282471\n",
      "Discriminator loss 0.7707016468048096\n",
      "Generator loss 4.687714099884033\n",
      "Discriminator loss 0.864149808883667\n",
      "Generator loss 4.826934814453125\n",
      "Discriminator loss 0.8637639284133911\n",
      "Generator loss 4.489754676818848\n",
      "Discriminator loss 1.056946039199829\n",
      "Generator loss 5.038971900939941\n",
      "Discriminator loss 0.8240235447883606\n",
      "Generator loss 4.147245407104492\n",
      "Epoch loss\n",
      "Discriminator loss 0.85845947265625\n",
      "Generator loss 4.341316223144531\n",
      "Discriminator loss 1.0633189678192139\n",
      "Generator loss 4.253925323486328\n",
      "Discriminator loss 0.8941003680229187\n",
      "Generator loss 4.736774921417236\n",
      "Discriminator loss 0.9396357536315918\n",
      "Generator loss 4.397626876831055\n",
      "Discriminator loss 0.8613979816436768\n",
      "Generator loss 4.425353050231934\n",
      "Discriminator loss 0.7820500731468201\n",
      "Generator loss 3.9257402420043945\n",
      "Discriminator loss 0.8055382966995239\n",
      "Generator loss 4.571042060852051\n",
      "Discriminator loss 0.853967010974884\n",
      "Generator loss 4.030538558959961\n",
      "Discriminator loss 0.9275957345962524\n",
      "Generator loss 4.10831880569458\n",
      "Discriminator loss 0.987272322177887\n",
      "Generator loss 4.681769371032715\n",
      "Discriminator loss 0.7956573963165283\n",
      "Generator loss 4.458340644836426\n",
      "Discriminator loss 0.784626841545105\n",
      "Generator loss 4.408161163330078\n",
      "Discriminator loss 0.781630277633667\n",
      "Generator loss 4.31148624420166\n",
      "Discriminator loss 0.8052035570144653\n",
      "Generator loss 4.5667877197265625\n",
      "Discriminator loss 0.8197336792945862\n",
      "Generator loss 4.705305099487305\n",
      "Discriminator loss 0.9894756078720093\n",
      "Generator loss 4.31725549697876\n",
      "Discriminator loss 0.7844923138618469\n",
      "Generator loss 4.383163928985596\n",
      "Discriminator loss 0.7693849802017212\n",
      "Generator loss 4.530466079711914\n",
      "Discriminator loss 0.8273078203201294\n",
      "Generator loss 4.049159049987793\n",
      "Discriminator loss 0.9016749858856201\n",
      "Generator loss 3.9881961345672607\n",
      "Discriminator loss 0.7493905425071716\n",
      "Generator loss 4.716319561004639\n",
      "Discriminator loss 0.8003412485122681\n",
      "Generator loss 4.719825744628906\n",
      "Discriminator loss 0.863979697227478\n",
      "Generator loss 4.224364280700684\n",
      "Discriminator loss 0.814500629901886\n",
      "Generator loss 4.065989017486572\n",
      "Discriminator loss 0.972144603729248\n",
      "Generator loss 3.91953706741333\n",
      "Epoch loss\n",
      "Discriminator loss 0.8846710324287415\n",
      "Generator loss 4.352831840515137\n",
      "Discriminator loss 0.7199202179908752\n",
      "Generator loss 4.406216621398926\n",
      "Discriminator loss 0.920796811580658\n",
      "Generator loss 4.65944766998291\n",
      "Discriminator loss 0.9003188610076904\n",
      "Generator loss 4.487078666687012\n",
      "Discriminator loss 0.8413215279579163\n",
      "Generator loss 4.750673294067383\n",
      "Discriminator loss 0.8765054941177368\n",
      "Generator loss 4.459333419799805\n",
      "Discriminator loss 0.7500913143157959\n",
      "Generator loss 4.183612823486328\n",
      "Discriminator loss 0.7643309235572815\n",
      "Generator loss 4.510321140289307\n",
      "Discriminator loss 0.8079902529716492\n",
      "Generator loss 3.989924192428589\n",
      "Discriminator loss 0.8505791425704956\n",
      "Generator loss 4.635217189788818\n",
      "Discriminator loss 0.7325376868247986\n",
      "Generator loss 4.811444282531738\n",
      "Discriminator loss 0.9618247747421265\n",
      "Generator loss 4.7066802978515625\n",
      "Discriminator loss 0.8827366828918457\n",
      "Generator loss 4.241239547729492\n",
      "Discriminator loss 0.7687352299690247\n",
      "Generator loss 4.368971347808838\n",
      "Discriminator loss 0.8210287690162659\n",
      "Generator loss 4.2004194259643555\n",
      "Discriminator loss 0.9097022414207458\n",
      "Generator loss 4.514304161071777\n",
      "Discriminator loss 0.7731928825378418\n",
      "Generator loss 4.139930248260498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 1.018534541130066\n",
      "Generator loss 3.9136011600494385\n",
      "Discriminator loss 0.8749475479125977\n",
      "Generator loss 3.9510271549224854\n",
      "Discriminator loss 0.8302792310714722\n",
      "Generator loss 4.445544719696045\n",
      "Discriminator loss 0.7528486847877502\n",
      "Generator loss 3.980027914047241\n",
      "Discriminator loss 0.8670280575752258\n",
      "Generator loss 4.063272476196289\n",
      "Discriminator loss 0.752193033695221\n",
      "Generator loss 4.221699237823486\n",
      "Discriminator loss 0.8624599575996399\n",
      "Generator loss 4.544161796569824\n",
      "Discriminator loss 0.8011631369590759\n",
      "Generator loss 4.35595703125\n",
      "Epoch loss\n",
      "Discriminator loss 0.8833342790603638\n",
      "Generator loss 4.641791343688965\n",
      "Discriminator loss 0.8276493549346924\n",
      "Generator loss 4.523740768432617\n",
      "Discriminator loss 0.8782155513763428\n",
      "Generator loss 5.198191165924072\n",
      "Discriminator loss 0.8580538630485535\n",
      "Generator loss 4.014571189880371\n",
      "Discriminator loss 0.8464764952659607\n",
      "Generator loss 4.174193382263184\n",
      "Discriminator loss 0.9055525660514832\n",
      "Generator loss 4.586592674255371\n",
      "Discriminator loss 0.8069968223571777\n",
      "Generator loss 4.345980167388916\n",
      "Discriminator loss 1.0323939323425293\n",
      "Generator loss 4.614333629608154\n",
      "Discriminator loss 0.7986060380935669\n",
      "Generator loss 3.848348617553711\n",
      "Discriminator loss 0.8036371469497681\n",
      "Generator loss 4.025853157043457\n",
      "Discriminator loss 0.8919929265975952\n",
      "Generator loss 4.107657432556152\n",
      "Discriminator loss 0.9147743582725525\n",
      "Generator loss 4.88934850692749\n",
      "Discriminator loss 0.8535302877426147\n",
      "Generator loss 4.694818019866943\n",
      "Discriminator loss 0.8644644021987915\n",
      "Generator loss 3.9202663898468018\n",
      "Discriminator loss 0.7742363214492798\n",
      "Generator loss 4.194545269012451\n",
      "Discriminator loss 1.008118748664856\n",
      "Generator loss 4.120364665985107\n",
      "Discriminator loss 0.9516211748123169\n",
      "Generator loss 4.451709747314453\n",
      "Discriminator loss 0.8023860454559326\n",
      "Generator loss 4.443893909454346\n",
      "Discriminator loss 0.8679139018058777\n",
      "Generator loss 4.324569225311279\n",
      "Discriminator loss 0.8031525611877441\n",
      "Generator loss 4.283485412597656\n",
      "Discriminator loss 0.7947016954421997\n",
      "Generator loss 4.139518737792969\n",
      "Discriminator loss 0.7734952569007874\n",
      "Generator loss 4.3010663986206055\n",
      "Discriminator loss 0.8338643908500671\n",
      "Generator loss 4.8018293380737305\n",
      "Discriminator loss 0.770448625087738\n",
      "Generator loss 4.344610214233398\n",
      "Discriminator loss 0.9305758476257324\n",
      "Generator loss 4.7946600914001465\n",
      "Epoch loss\n",
      "Discriminator loss 0.9502027034759521\n",
      "Generator loss 4.451755523681641\n",
      "Discriminator loss 0.9596739411354065\n",
      "Generator loss 3.9139256477355957\n",
      "Discriminator loss 0.8564753532409668\n",
      "Generator loss 4.632413864135742\n",
      "Discriminator loss 0.8027214407920837\n",
      "Generator loss 4.151117324829102\n",
      "Discriminator loss 0.8262454271316528\n",
      "Generator loss 4.395389080047607\n",
      "Discriminator loss 0.9758380651473999\n",
      "Generator loss 4.357350826263428\n",
      "Discriminator loss 0.8737621903419495\n",
      "Generator loss 5.198220252990723\n",
      "Discriminator loss 0.892692506313324\n",
      "Generator loss 4.204512119293213\n",
      "Discriminator loss 0.7791050672531128\n",
      "Generator loss 3.9294931888580322\n",
      "Discriminator loss 0.781711995601654\n",
      "Generator loss 4.047079086303711\n",
      "Discriminator loss 0.768711268901825\n",
      "Generator loss 4.544765472412109\n",
      "Discriminator loss 0.7690671682357788\n",
      "Generator loss 4.028841495513916\n",
      "Discriminator loss 0.7928289175033569\n",
      "Generator loss 5.252955913543701\n",
      "Discriminator loss 0.881437361240387\n",
      "Generator loss 4.690491676330566\n",
      "Discriminator loss 0.7333160638809204\n",
      "Generator loss 4.605197429656982\n",
      "Discriminator loss 0.8915197253227234\n",
      "Generator loss 4.129776954650879\n",
      "Discriminator loss 0.8341793417930603\n",
      "Generator loss 4.771657943725586\n",
      "Discriminator loss 0.8610762357711792\n",
      "Generator loss 4.578178405761719\n",
      "Discriminator loss 1.004241943359375\n",
      "Generator loss 4.118563175201416\n",
      "Discriminator loss 0.9180458784103394\n",
      "Generator loss 4.081784725189209\n",
      "Discriminator loss 0.7635132074356079\n",
      "Generator loss 4.595114231109619\n",
      "Discriminator loss 0.8558738231658936\n",
      "Generator loss 4.0210981369018555\n",
      "Discriminator loss 0.9710296392440796\n",
      "Generator loss 4.391587257385254\n",
      "Discriminator loss 0.8196256160736084\n",
      "Generator loss 4.514431476593018\n",
      "Discriminator loss 0.8630357980728149\n",
      "Generator loss 4.288379669189453\n",
      "Epoch loss\n",
      "Discriminator loss 0.7970002889633179\n",
      "Generator loss 4.251155853271484\n",
      "Discriminator loss 0.7879812717437744\n",
      "Generator loss 4.162830829620361\n",
      "Discriminator loss 0.8707850575447083\n",
      "Generator loss 4.308032512664795\n",
      "Discriminator loss 0.854219913482666\n",
      "Generator loss 4.589013576507568\n",
      "Discriminator loss 0.9504200220108032\n",
      "Generator loss 4.85038948059082\n",
      "Discriminator loss 0.8302072286605835\n",
      "Generator loss 4.714657783508301\n",
      "Discriminator loss 0.8797764778137207\n",
      "Generator loss 4.4647345542907715\n",
      "Discriminator loss 0.8512956500053406\n",
      "Generator loss 4.588452339172363\n",
      "Discriminator loss 0.9066380858421326\n",
      "Generator loss 4.40416145324707\n",
      "Discriminator loss 1.0367696285247803\n",
      "Generator loss 3.9612834453582764\n",
      "Discriminator loss 0.7987526655197144\n",
      "Generator loss 4.234329700469971\n",
      "Discriminator loss 0.7986356019973755\n",
      "Generator loss 4.304477691650391\n",
      "Discriminator loss 0.8097469806671143\n",
      "Generator loss 4.401994705200195\n",
      "Discriminator loss 0.7935563325881958\n",
      "Generator loss 4.2180256843566895\n",
      "Discriminator loss 0.821746289730072\n",
      "Generator loss 4.0080952644348145\n",
      "Discriminator loss 0.8741893768310547\n",
      "Generator loss 4.214234352111816\n",
      "Discriminator loss 1.0135085582733154\n",
      "Generator loss 4.333568096160889\n",
      "Discriminator loss 0.7865883708000183\n",
      "Generator loss 4.645636081695557\n",
      "Discriminator loss 0.838900089263916\n",
      "Generator loss 4.0598978996276855\n",
      "Discriminator loss 0.9775170683860779\n",
      "Generator loss 4.8222551345825195\n",
      "Discriminator loss 1.0557315349578857\n",
      "Generator loss 4.2457356452941895\n",
      "Discriminator loss 0.8410817980766296\n",
      "Generator loss 4.447333335876465\n",
      "Discriminator loss 0.91920006275177\n",
      "Generator loss 4.323031425476074\n",
      "Discriminator loss 0.943248987197876\n",
      "Generator loss 4.399083137512207\n",
      "Discriminator loss 0.8155937194824219\n",
      "Generator loss 3.8958826065063477\n",
      "Epoch loss\n",
      "Discriminator loss 0.8089552521705627\n",
      "Generator loss 4.023760795593262\n",
      "Discriminator loss 0.7618684768676758\n",
      "Generator loss 4.026813983917236\n",
      "Discriminator loss 0.922926127910614\n",
      "Generator loss 4.356593608856201\n",
      "Discriminator loss 0.7199603319168091\n",
      "Generator loss 4.550562858581543\n",
      "Discriminator loss 0.9958269000053406\n",
      "Generator loss 4.563683032989502\n",
      "Discriminator loss 0.8980340957641602\n",
      "Generator loss 3.7099204063415527\n",
      "Discriminator loss 0.923191487789154\n",
      "Generator loss 4.464212894439697\n",
      "Discriminator loss 0.8341534733772278\n",
      "Generator loss 4.3764848709106445\n",
      "Discriminator loss 0.9050852060317993\n",
      "Generator loss 4.72739315032959\n",
      "Discriminator loss 0.903836190700531\n",
      "Generator loss 3.8530218601226807\n",
      "Discriminator loss 0.8334486484527588\n",
      "Generator loss 4.487270832061768\n",
      "Discriminator loss 0.9855246543884277\n",
      "Generator loss 4.4355387687683105\n",
      "Discriminator loss 0.8890843987464905\n",
      "Generator loss 4.433959007263184\n",
      "Discriminator loss 0.9915201663970947\n",
      "Generator loss 4.754327774047852\n",
      "Discriminator loss 0.938786506652832\n",
      "Generator loss 4.434886932373047\n",
      "Discriminator loss 0.8899421095848083\n",
      "Generator loss 4.693253040313721\n",
      "Discriminator loss 0.9122540354728699\n",
      "Generator loss 4.342472553253174\n",
      "Discriminator loss 0.8900254964828491\n",
      "Generator loss 4.594848155975342\n",
      "Discriminator loss 0.8321141004562378\n",
      "Generator loss 4.755082130432129\n",
      "Discriminator loss 1.0345239639282227\n",
      "Generator loss 4.439042568206787\n",
      "Discriminator loss 0.7907407283782959\n",
      "Generator loss 4.355896949768066\n",
      "Discriminator loss 0.8286386728286743\n",
      "Generator loss 4.47054386138916\n",
      "Discriminator loss 0.8960576057434082\n",
      "Generator loss 3.9383647441864014\n",
      "Discriminator loss 0.7836230397224426\n",
      "Generator loss 4.463833808898926\n",
      "Discriminator loss 0.8172507882118225\n",
      "Generator loss 4.538247108459473\n",
      "Epoch loss\n",
      "Discriminator loss 0.8789010643959045\n",
      "Generator loss 4.015294075012207\n",
      "Discriminator loss 0.935039758682251\n",
      "Generator loss 3.841620922088623\n",
      "Discriminator loss 0.8706240653991699\n",
      "Generator loss 4.317242622375488\n",
      "Discriminator loss 0.855797290802002\n",
      "Generator loss 4.048160552978516\n",
      "Discriminator loss 0.8892910480499268\n",
      "Generator loss 4.8257246017456055\n",
      "Discriminator loss 0.8097788095474243\n",
      "Generator loss 4.438015460968018\n",
      "Discriminator loss 0.8530797958374023\n",
      "Generator loss 4.349712371826172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.786984384059906\n",
      "Generator loss 4.13228702545166\n",
      "Discriminator loss 0.7731291055679321\n",
      "Generator loss 3.9486608505249023\n",
      "Discriminator loss 0.7917535901069641\n",
      "Generator loss 4.450437545776367\n",
      "Discriminator loss 0.9276891946792603\n",
      "Generator loss 4.2181596755981445\n",
      "Discriminator loss 0.7855433225631714\n",
      "Generator loss 3.8518905639648438\n",
      "Discriminator loss 0.8348317742347717\n",
      "Generator loss 4.6620073318481445\n",
      "Discriminator loss 0.8719627857208252\n",
      "Generator loss 4.212392330169678\n",
      "Discriminator loss 0.9157901406288147\n",
      "Generator loss 4.2780914306640625\n",
      "Discriminator loss 0.8760010004043579\n",
      "Generator loss 4.476953983306885\n",
      "Discriminator loss 0.8390368223190308\n",
      "Generator loss 4.249024391174316\n",
      "Discriminator loss 0.8390963077545166\n",
      "Generator loss 4.397148609161377\n",
      "Discriminator loss 0.8042670488357544\n",
      "Generator loss 4.481566429138184\n",
      "Discriminator loss 0.8788701295852661\n",
      "Generator loss 4.35421085357666\n",
      "Discriminator loss 0.8916977047920227\n",
      "Generator loss 3.817319631576538\n",
      "Discriminator loss 0.9629868865013123\n",
      "Generator loss 4.5305328369140625\n",
      "Discriminator loss 0.9066115617752075\n",
      "Generator loss 4.691716194152832\n",
      "Discriminator loss 0.8265647888183594\n",
      "Generator loss 4.088874816894531\n",
      "Discriminator loss 0.8005359768867493\n",
      "Generator loss 4.52317476272583\n",
      "Epoch loss\n",
      "Discriminator loss 0.7824101448059082\n",
      "Generator loss 4.280189514160156\n",
      "Discriminator loss 0.9455668330192566\n",
      "Generator loss 3.670227527618408\n",
      "Discriminator loss 0.8371423482894897\n",
      "Generator loss 4.003566265106201\n",
      "Discriminator loss 0.8706915378570557\n",
      "Generator loss 3.8912649154663086\n",
      "Discriminator loss 0.8196232318878174\n",
      "Generator loss 4.393949031829834\n",
      "Discriminator loss 0.9558640718460083\n",
      "Generator loss 4.8252668380737305\n",
      "Discriminator loss 0.8011858463287354\n",
      "Generator loss 4.4476237297058105\n",
      "Discriminator loss 0.8162828683853149\n",
      "Generator loss 4.573142051696777\n",
      "Discriminator loss 0.8856569528579712\n",
      "Generator loss 4.398034572601318\n",
      "Discriminator loss 0.7513325214385986\n",
      "Generator loss 4.1940484046936035\n",
      "Discriminator loss 0.8463883399963379\n",
      "Generator loss 4.101587295532227\n",
      "Discriminator loss 0.8235197067260742\n",
      "Generator loss 4.312465667724609\n",
      "Discriminator loss 0.9013867378234863\n",
      "Generator loss 4.181183338165283\n",
      "Discriminator loss 0.9933694005012512\n",
      "Generator loss 4.999647617340088\n",
      "Discriminator loss 0.8863941431045532\n",
      "Generator loss 4.3147687911987305\n",
      "Discriminator loss 0.7797489166259766\n",
      "Generator loss 4.255048751831055\n",
      "Discriminator loss 0.8286426663398743\n",
      "Generator loss 4.507499694824219\n",
      "Discriminator loss 1.0554708242416382\n",
      "Generator loss 4.1670074462890625\n",
      "Discriminator loss 0.8146114349365234\n",
      "Generator loss 4.445347309112549\n",
      "Discriminator loss 0.9478495717048645\n",
      "Generator loss 4.6077561378479\n",
      "Discriminator loss 0.77094566822052\n",
      "Generator loss 4.587573528289795\n",
      "Discriminator loss 0.7623923420906067\n",
      "Generator loss 4.245169162750244\n",
      "Discriminator loss 0.7820003032684326\n",
      "Generator loss 4.36527156829834\n",
      "Discriminator loss 0.8018307685852051\n",
      "Generator loss 4.61434268951416\n",
      "Discriminator loss 0.7895854115486145\n",
      "Generator loss 4.123169422149658\n",
      "Epoch loss\n",
      "Discriminator loss 0.8869606256484985\n",
      "Generator loss 4.316094398498535\n",
      "Discriminator loss 0.8797293305397034\n",
      "Generator loss 4.131089687347412\n",
      "Discriminator loss 0.9889020919799805\n",
      "Generator loss 4.481486797332764\n",
      "Discriminator loss 0.8059914708137512\n",
      "Generator loss 3.909492015838623\n",
      "Discriminator loss 0.8892816305160522\n",
      "Generator loss 4.230772018432617\n",
      "Discriminator loss 0.8369755744934082\n",
      "Generator loss 4.441744804382324\n",
      "Discriminator loss 0.8568984270095825\n",
      "Generator loss 4.151552677154541\n",
      "Discriminator loss 0.8279057741165161\n",
      "Generator loss 4.684261798858643\n",
      "Discriminator loss 0.8443158864974976\n",
      "Generator loss 4.365131855010986\n",
      "Discriminator loss 0.8990539908409119\n",
      "Generator loss 4.0963969230651855\n",
      "Discriminator loss 0.8806089162826538\n",
      "Generator loss 4.319245338439941\n",
      "Discriminator loss 0.9509288668632507\n",
      "Generator loss 3.8901991844177246\n",
      "Discriminator loss 1.013819694519043\n",
      "Generator loss 4.625247478485107\n",
      "Discriminator loss 0.8673316240310669\n",
      "Generator loss 3.945533275604248\n",
      "Discriminator loss 0.7902587652206421\n",
      "Generator loss 5.010572910308838\n",
      "Discriminator loss 0.8438592553138733\n",
      "Generator loss 4.555824279785156\n",
      "Discriminator loss 0.9756801128387451\n",
      "Generator loss 4.274204254150391\n",
      "Discriminator loss 0.8317784070968628\n",
      "Generator loss 3.9707767963409424\n",
      "Discriminator loss 0.8896493315696716\n",
      "Generator loss 4.47570276260376\n",
      "Discriminator loss 0.7790297865867615\n",
      "Generator loss 4.4783244132995605\n",
      "Discriminator loss 0.9060381650924683\n",
      "Generator loss 4.435561180114746\n",
      "Discriminator loss 0.9313116669654846\n",
      "Generator loss 4.532814979553223\n",
      "Discriminator loss 0.9389806389808655\n",
      "Generator loss 4.534707546234131\n",
      "Discriminator loss 0.7708688378334045\n",
      "Generator loss 4.476323127746582\n",
      "Discriminator loss 0.7964785099029541\n",
      "Generator loss 3.802299737930298\n",
      "Epoch loss\n",
      "Discriminator loss 1.0542420148849487\n",
      "Generator loss 4.183093070983887\n",
      "Discriminator loss 0.8153682947158813\n",
      "Generator loss 4.785754680633545\n",
      "Discriminator loss 0.9490216970443726\n",
      "Generator loss 4.164118766784668\n",
      "Discriminator loss 0.7790586352348328\n",
      "Generator loss 4.329200744628906\n",
      "Discriminator loss 0.8988925814628601\n",
      "Generator loss 4.331268787384033\n",
      "Discriminator loss 0.9724960923194885\n",
      "Generator loss 4.421085357666016\n",
      "Discriminator loss 0.7993177771568298\n",
      "Generator loss 4.425626277923584\n",
      "Discriminator loss 0.9183722734451294\n",
      "Generator loss 4.482646942138672\n",
      "Discriminator loss 0.9831733703613281\n",
      "Generator loss 4.519102096557617\n",
      "Discriminator loss 0.8216826319694519\n",
      "Generator loss 4.138440132141113\n",
      "Discriminator loss 0.8198941946029663\n",
      "Generator loss 4.3822407722473145\n",
      "Discriminator loss 0.9973716735839844\n",
      "Generator loss 3.503040313720703\n",
      "Discriminator loss 0.9131288528442383\n",
      "Generator loss 3.8334670066833496\n",
      "Discriminator loss 0.7711294889450073\n",
      "Generator loss 4.219570636749268\n",
      "Discriminator loss 0.8579033613204956\n",
      "Generator loss 4.574633598327637\n",
      "Discriminator loss 0.7892208695411682\n",
      "Generator loss 4.285373210906982\n",
      "Discriminator loss 0.8869516253471375\n",
      "Generator loss 3.859834909439087\n",
      "Discriminator loss 0.7694163918495178\n",
      "Generator loss 3.64803147315979\n",
      "Discriminator loss 0.9793912172317505\n",
      "Generator loss 3.955817222595215\n",
      "Discriminator loss 0.7709002494812012\n",
      "Generator loss 4.099215984344482\n",
      "Discriminator loss 0.7413322925567627\n",
      "Generator loss 3.9678666591644287\n",
      "Discriminator loss 0.84508216381073\n",
      "Generator loss 4.461378574371338\n",
      "Discriminator loss 0.888749897480011\n",
      "Generator loss 4.448633193969727\n",
      "Discriminator loss 0.7960724830627441\n",
      "Generator loss 4.306130409240723\n",
      "Discriminator loss 0.8514734506607056\n",
      "Generator loss 4.306148052215576\n",
      "Epoch loss\n",
      "Discriminator loss 0.9437520503997803\n",
      "Generator loss 4.585184574127197\n",
      "Discriminator loss 0.9149115681648254\n",
      "Generator loss 4.165225982666016\n",
      "Discriminator loss 0.8596987724304199\n",
      "Generator loss 4.267449378967285\n",
      "Discriminator loss 0.9306080937385559\n",
      "Generator loss 3.9904932975769043\n",
      "Discriminator loss 0.9269627928733826\n",
      "Generator loss 4.337406635284424\n",
      "Discriminator loss 0.7876109480857849\n",
      "Generator loss 4.583435535430908\n",
      "Discriminator loss 0.813813328742981\n",
      "Generator loss 4.211639404296875\n",
      "Discriminator loss 0.7880790829658508\n",
      "Generator loss 4.0750837326049805\n",
      "Discriminator loss 0.9442921876907349\n",
      "Generator loss 4.221124172210693\n",
      "Discriminator loss 0.8356859683990479\n",
      "Generator loss 4.594146251678467\n",
      "Discriminator loss 0.824699878692627\n",
      "Generator loss 4.079054355621338\n",
      "Discriminator loss 0.8766827583312988\n",
      "Generator loss 4.195557117462158\n",
      "Discriminator loss 0.9304186105728149\n",
      "Generator loss 4.481197357177734\n",
      "Discriminator loss 0.8413043022155762\n",
      "Generator loss 4.020744800567627\n",
      "Discriminator loss 0.742938220500946\n",
      "Generator loss 4.233931064605713\n",
      "Discriminator loss 0.7472939491271973\n",
      "Generator loss 4.219814777374268\n",
      "Discriminator loss 0.9569817781448364\n",
      "Generator loss 4.182714462280273\n",
      "Discriminator loss 0.7891914248466492\n",
      "Generator loss 3.97426700592041\n",
      "Discriminator loss 0.9083333015441895\n",
      "Generator loss 4.035532474517822\n",
      "Discriminator loss 0.8996188640594482\n",
      "Generator loss 4.162668704986572\n",
      "Discriminator loss 0.912446141242981\n",
      "Generator loss 4.329185485839844\n",
      "Discriminator loss 0.8429980278015137\n",
      "Generator loss 4.348470687866211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.8713403344154358\n",
      "Generator loss 3.652967929840088\n",
      "Discriminator loss 1.0951083898544312\n",
      "Generator loss 4.569049835205078\n",
      "Discriminator loss 0.9509618878364563\n",
      "Generator loss 4.233605861663818\n",
      "Epoch loss\n",
      "Discriminator loss 0.8333959579467773\n",
      "Generator loss 4.530324935913086\n",
      "Discriminator loss 0.8873797655105591\n",
      "Generator loss 3.4855294227600098\n",
      "Discriminator loss 0.8600580096244812\n",
      "Generator loss 4.11953067779541\n",
      "Discriminator loss 0.8463473320007324\n",
      "Generator loss 4.24485969543457\n",
      "Discriminator loss 0.8796025514602661\n",
      "Generator loss 3.793522596359253\n",
      "Discriminator loss 0.8790638446807861\n",
      "Generator loss 3.9367713928222656\n",
      "Discriminator loss 0.8251523971557617\n",
      "Generator loss 3.907442569732666\n",
      "Discriminator loss 0.8514469861984253\n",
      "Generator loss 4.001031875610352\n",
      "Discriminator loss 0.7875217199325562\n",
      "Generator loss 4.532201766967773\n",
      "Discriminator loss 0.8001596927642822\n",
      "Generator loss 4.283600807189941\n",
      "Discriminator loss 0.8123978972434998\n",
      "Generator loss 4.1486496925354\n",
      "Discriminator loss 0.9919042587280273\n",
      "Generator loss 4.4190874099731445\n",
      "Discriminator loss 0.7647131681442261\n",
      "Generator loss 4.036205291748047\n",
      "Discriminator loss 0.7960638403892517\n",
      "Generator loss 4.4748382568359375\n",
      "Discriminator loss 0.7359502911567688\n",
      "Generator loss 4.844236373901367\n",
      "Discriminator loss 0.9008282423019409\n",
      "Generator loss 4.503923416137695\n",
      "Discriminator loss 0.7569465637207031\n",
      "Generator loss 4.498788356781006\n",
      "Discriminator loss 0.8385635614395142\n",
      "Generator loss 4.252771377563477\n",
      "Discriminator loss 0.8323832154273987\n",
      "Generator loss 3.9082250595092773\n",
      "Discriminator loss 0.8056137561798096\n",
      "Generator loss 4.600996971130371\n",
      "Discriminator loss 0.904931366443634\n",
      "Generator loss 4.314196586608887\n",
      "Discriminator loss 0.9052764177322388\n",
      "Generator loss 4.528561115264893\n",
      "Discriminator loss 0.809767484664917\n",
      "Generator loss 4.513548374176025\n",
      "Discriminator loss 0.8242123126983643\n",
      "Generator loss 3.7593414783477783\n",
      "Discriminator loss 0.7911114692687988\n",
      "Generator loss 4.615375995635986\n",
      "Epoch loss\n",
      "Discriminator loss 0.8680263757705688\n",
      "Generator loss 3.8094749450683594\n",
      "Discriminator loss 0.8945179581642151\n",
      "Generator loss 4.257524013519287\n",
      "Discriminator loss 0.8304349780082703\n",
      "Generator loss 4.334909915924072\n",
      "Discriminator loss 0.9547468423843384\n",
      "Generator loss 3.5434107780456543\n",
      "Discriminator loss 0.8780826926231384\n",
      "Generator loss 4.250432014465332\n",
      "Discriminator loss 0.845700204372406\n",
      "Generator loss 4.436559677124023\n",
      "Discriminator loss 0.7834669351577759\n",
      "Generator loss 4.139961242675781\n",
      "Discriminator loss 0.7546249032020569\n",
      "Generator loss 4.085944652557373\n",
      "Discriminator loss 0.8018522262573242\n",
      "Generator loss 3.9189577102661133\n",
      "Discriminator loss 0.8405138850212097\n",
      "Generator loss 3.900434970855713\n",
      "Discriminator loss 0.8424454927444458\n",
      "Generator loss 4.107170104980469\n",
      "Discriminator loss 0.8214664459228516\n",
      "Generator loss 4.381093502044678\n",
      "Discriminator loss 0.8315663933753967\n",
      "Generator loss 4.103113174438477\n",
      "Discriminator loss 0.7662465572357178\n",
      "Generator loss 4.628481864929199\n",
      "Discriminator loss 0.8331342935562134\n",
      "Generator loss 4.184188365936279\n",
      "Discriminator loss 0.8357765078544617\n",
      "Generator loss 4.5495147705078125\n",
      "Discriminator loss 0.9316959381103516\n",
      "Generator loss 3.9206180572509766\n",
      "Discriminator loss 0.8432692885398865\n",
      "Generator loss 3.938983201980591\n",
      "Discriminator loss 0.708273708820343\n",
      "Generator loss 4.147852420806885\n",
      "Discriminator loss 0.8395254611968994\n",
      "Generator loss 5.0063581466674805\n",
      "Discriminator loss 0.8758636116981506\n",
      "Generator loss 4.378245830535889\n",
      "Discriminator loss 0.8256638050079346\n",
      "Generator loss 4.0878825187683105\n",
      "Discriminator loss 0.8241981863975525\n",
      "Generator loss 3.963517189025879\n",
      "Discriminator loss 0.8311904072761536\n",
      "Generator loss 4.004194736480713\n",
      "Discriminator loss 0.8047141432762146\n",
      "Generator loss 3.8914437294006348\n",
      "Epoch loss\n",
      "Discriminator loss 0.9124104976654053\n",
      "Generator loss 4.544527053833008\n",
      "Discriminator loss 0.9548220634460449\n",
      "Generator loss 3.5756561756134033\n",
      "Discriminator loss 0.8146356344223022\n",
      "Generator loss 4.16450834274292\n",
      "Discriminator loss 0.7719714045524597\n",
      "Generator loss 4.111117362976074\n",
      "Discriminator loss 0.8796021938323975\n",
      "Generator loss 4.455069541931152\n",
      "Discriminator loss 0.8139404058456421\n",
      "Generator loss 4.806008338928223\n",
      "Discriminator loss 0.8286393880844116\n",
      "Generator loss 3.8136863708496094\n",
      "Discriminator loss 0.9838978052139282\n",
      "Generator loss 4.016380786895752\n",
      "Discriminator loss 0.9859921932220459\n",
      "Generator loss 4.302973747253418\n",
      "Discriminator loss 0.8202956914901733\n",
      "Generator loss 3.737173557281494\n",
      "Discriminator loss 0.9473208785057068\n",
      "Generator loss 3.5752768516540527\n",
      "Discriminator loss 0.8686391115188599\n",
      "Generator loss 4.229955673217773\n",
      "Discriminator loss 0.8787930011749268\n",
      "Generator loss 4.671959400177002\n",
      "Discriminator loss 0.9647578597068787\n",
      "Generator loss 4.1721510887146\n",
      "Discriminator loss 0.8502242565155029\n",
      "Generator loss 3.981539011001587\n",
      "Discriminator loss 0.807396411895752\n",
      "Generator loss 4.1522650718688965\n",
      "Discriminator loss 0.8688571453094482\n",
      "Generator loss 4.227921009063721\n",
      "Discriminator loss 0.9176428318023682\n",
      "Generator loss 4.369112968444824\n",
      "Discriminator loss 0.8690565824508667\n",
      "Generator loss 4.034862041473389\n",
      "Discriminator loss 0.8594768643379211\n",
      "Generator loss 3.574613094329834\n",
      "Discriminator loss 0.7823432683944702\n",
      "Generator loss 4.3789567947387695\n",
      "Discriminator loss 0.8555335402488708\n",
      "Generator loss 3.949542760848999\n",
      "Discriminator loss 0.8076209425926208\n",
      "Generator loss 4.277932167053223\n",
      "Discriminator loss 1.0534712076187134\n",
      "Generator loss 4.716221332550049\n",
      "Discriminator loss 0.8176993727684021\n",
      "Generator loss 3.986114501953125\n",
      "Epoch loss\n",
      "Discriminator loss 0.840391218662262\n",
      "Generator loss 4.019804954528809\n",
      "Discriminator loss 0.8016456365585327\n",
      "Generator loss 3.6528499126434326\n",
      "Discriminator loss 0.8483482003211975\n",
      "Generator loss 4.4851508140563965\n",
      "Discriminator loss 0.8438700437545776\n",
      "Generator loss 3.914870500564575\n",
      "Discriminator loss 0.9313234686851501\n",
      "Generator loss 4.268345355987549\n",
      "Discriminator loss 0.8466213941574097\n",
      "Generator loss 4.225350856781006\n",
      "Discriminator loss 0.8457591533660889\n",
      "Generator loss 4.508450031280518\n",
      "Discriminator loss 0.8543961048126221\n",
      "Generator loss 4.004831314086914\n",
      "Discriminator loss 0.7386695146560669\n",
      "Generator loss 4.252176284790039\n",
      "Discriminator loss 0.8281729221343994\n",
      "Generator loss 4.219339370727539\n",
      "Discriminator loss 0.8456885814666748\n",
      "Generator loss 3.950497627258301\n",
      "Discriminator loss 0.7948413491249084\n",
      "Generator loss 3.890307664871216\n",
      "Discriminator loss 0.8175160884857178\n",
      "Generator loss 4.0408196449279785\n",
      "Discriminator loss 0.782459020614624\n",
      "Generator loss 4.386646747589111\n",
      "Discriminator loss 0.898526132106781\n",
      "Generator loss 3.6858465671539307\n",
      "Discriminator loss 0.7945258617401123\n",
      "Generator loss 3.84283185005188\n",
      "Discriminator loss 0.8622397184371948\n",
      "Generator loss 4.189239025115967\n",
      "Discriminator loss 1.053247094154358\n",
      "Generator loss 4.365114688873291\n",
      "Discriminator loss 0.7727493643760681\n",
      "Generator loss 3.8088912963867188\n",
      "Discriminator loss 0.8166345953941345\n",
      "Generator loss 4.265180587768555\n",
      "Discriminator loss 0.8654375076293945\n",
      "Generator loss 4.059177398681641\n",
      "Discriminator loss 0.9186731576919556\n",
      "Generator loss 4.0727858543396\n",
      "Discriminator loss 0.7779099941253662\n",
      "Generator loss 4.684513568878174\n",
      "Discriminator loss 0.8571996092796326\n",
      "Generator loss 3.6529576778411865\n",
      "Discriminator loss 0.8960756063461304\n",
      "Generator loss 4.3163886070251465\n",
      "Epoch loss\n",
      "Discriminator loss 0.777244508266449\n",
      "Generator loss 4.169819355010986\n",
      "Discriminator loss 0.8620762825012207\n",
      "Generator loss 4.040875434875488\n",
      "Discriminator loss 0.7831381559371948\n",
      "Generator loss 4.046884536743164\n",
      "Discriminator loss 0.9322414398193359\n",
      "Generator loss 4.031816482543945\n",
      "Discriminator loss 0.8831300139427185\n",
      "Generator loss 4.428200721740723\n",
      "Discriminator loss 0.7882126569747925\n",
      "Generator loss 3.96077036857605\n",
      "Discriminator loss 0.9840671420097351\n",
      "Generator loss 3.84529447555542\n",
      "Discriminator loss 0.8706152439117432\n",
      "Generator loss 4.070401191711426\n",
      "Discriminator loss 0.8720178604125977\n",
      "Generator loss 3.8042759895324707\n",
      "Discriminator loss 0.907067596912384\n",
      "Generator loss 4.152166366577148\n",
      "Discriminator loss 0.9747244119644165\n",
      "Generator loss 3.946037769317627\n",
      "Discriminator loss 0.8507528901100159\n",
      "Generator loss 3.960984945297241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.8583415150642395\n",
      "Generator loss 4.240627288818359\n",
      "Discriminator loss 0.8476859927177429\n",
      "Generator loss 4.320918560028076\n",
      "Discriminator loss 0.8605061769485474\n",
      "Generator loss 3.889143705368042\n",
      "Discriminator loss 0.8064121603965759\n",
      "Generator loss 4.520173072814941\n",
      "Discriminator loss 0.8104203939437866\n",
      "Generator loss 4.113149642944336\n",
      "Discriminator loss 0.9123428463935852\n",
      "Generator loss 4.2436842918396\n",
      "Discriminator loss 0.8905027508735657\n",
      "Generator loss 4.283705711364746\n",
      "Discriminator loss 0.8943967819213867\n",
      "Generator loss 4.093051910400391\n",
      "Discriminator loss 0.9005874991416931\n",
      "Generator loss 4.062463283538818\n",
      "Discriminator loss 0.8203067183494568\n",
      "Generator loss 4.463438510894775\n",
      "Discriminator loss 0.8814199566841125\n",
      "Generator loss 3.5102744102478027\n",
      "Discriminator loss 0.8313875794410706\n",
      "Generator loss 4.166378021240234\n",
      "Discriminator loss 0.8650410771369934\n",
      "Generator loss 4.169442653656006\n",
      "Epoch loss\n",
      "Discriminator loss 0.8943284153938293\n",
      "Generator loss 3.794097423553467\n",
      "Discriminator loss 0.7725903987884521\n",
      "Generator loss 3.8759801387786865\n",
      "Discriminator loss 0.8381258249282837\n",
      "Generator loss 4.08274507522583\n",
      "Discriminator loss 0.9057731628417969\n",
      "Generator loss 4.039538860321045\n",
      "Discriminator loss 0.9027634859085083\n",
      "Generator loss 3.7879841327667236\n",
      "Discriminator loss 0.9441023468971252\n",
      "Generator loss 4.272376537322998\n",
      "Discriminator loss 0.8983384370803833\n",
      "Generator loss 4.498751640319824\n",
      "Discriminator loss 0.9142727255821228\n",
      "Generator loss 3.8966593742370605\n",
      "Discriminator loss 0.8628832101821899\n",
      "Generator loss 4.556118488311768\n",
      "Discriminator loss 1.0268208980560303\n",
      "Generator loss 4.179562568664551\n",
      "Discriminator loss 0.9036675691604614\n",
      "Generator loss 4.512936115264893\n",
      "Discriminator loss 0.9311802387237549\n",
      "Generator loss 3.9289042949676514\n",
      "Discriminator loss 0.8298120498657227\n",
      "Generator loss 4.480642318725586\n",
      "Discriminator loss 0.85597825050354\n",
      "Generator loss 4.372132778167725\n",
      "Discriminator loss 0.7678381204605103\n",
      "Generator loss 4.579770088195801\n",
      "Discriminator loss 0.8521686792373657\n",
      "Generator loss 4.0087409019470215\n",
      "Discriminator loss 0.8765032291412354\n",
      "Generator loss 3.7057859897613525\n",
      "Discriminator loss 1.1273870468139648\n",
      "Generator loss 4.246957778930664\n",
      "Discriminator loss 0.7765440344810486\n",
      "Generator loss 4.174984455108643\n",
      "Discriminator loss 0.8970416784286499\n",
      "Generator loss 4.031184196472168\n",
      "Discriminator loss 0.8598312735557556\n",
      "Generator loss 4.115381240844727\n",
      "Discriminator loss 0.8897153735160828\n",
      "Generator loss 4.412703037261963\n",
      "Discriminator loss 0.8098708391189575\n",
      "Generator loss 3.9087576866149902\n",
      "Discriminator loss 0.8123937249183655\n",
      "Generator loss 4.065612316131592\n",
      "Discriminator loss 0.9526050090789795\n",
      "Generator loss 4.130165100097656\n",
      "Epoch loss\n",
      "Discriminator loss 0.7210107445716858\n",
      "Generator loss 4.879178524017334\n",
      "Discriminator loss 0.7857414484024048\n",
      "Generator loss 4.568244457244873\n",
      "Discriminator loss 0.8801582455635071\n",
      "Generator loss 3.6839990615844727\n",
      "Discriminator loss 0.8662869334220886\n",
      "Generator loss 4.416112422943115\n",
      "Discriminator loss 0.9728958010673523\n",
      "Generator loss 3.8251090049743652\n",
      "Discriminator loss 0.9558621048927307\n",
      "Generator loss 4.285492897033691\n",
      "Discriminator loss 0.8545992970466614\n",
      "Generator loss 4.373289585113525\n",
      "Discriminator loss 0.8877102732658386\n",
      "Generator loss 4.38923454284668\n",
      "Discriminator loss 0.8549899458885193\n",
      "Generator loss 3.669173002243042\n",
      "Discriminator loss 0.9783760905265808\n",
      "Generator loss 4.173389911651611\n",
      "Discriminator loss 0.8023243546485901\n",
      "Generator loss 3.890933036804199\n",
      "Discriminator loss 0.9212855696678162\n",
      "Generator loss 4.287054538726807\n",
      "Discriminator loss 0.8947716355323792\n",
      "Generator loss 4.580150127410889\n",
      "Discriminator loss 0.9110233187675476\n",
      "Generator loss 4.120163917541504\n",
      "Discriminator loss 0.8932065367698669\n",
      "Generator loss 4.301699638366699\n",
      "Discriminator loss 0.9529671669006348\n",
      "Generator loss 4.061650276184082\n",
      "Discriminator loss 0.8981043100357056\n",
      "Generator loss 4.022283554077148\n",
      "Discriminator loss 0.8599490523338318\n",
      "Generator loss 4.095812797546387\n",
      "Discriminator loss 0.9133816957473755\n",
      "Generator loss 4.113344669342041\n",
      "Discriminator loss 0.8050650358200073\n",
      "Generator loss 4.506708145141602\n",
      "Discriminator loss 0.8182950019836426\n",
      "Generator loss 4.289775848388672\n",
      "Discriminator loss 0.9051863551139832\n",
      "Generator loss 4.41848087310791\n",
      "Discriminator loss 0.7730897068977356\n",
      "Generator loss 3.929469347000122\n",
      "Discriminator loss 0.7228907346725464\n",
      "Generator loss 4.500006675720215\n",
      "Discriminator loss 0.8311670422554016\n",
      "Generator loss 4.761252403259277\n",
      "Epoch loss\n",
      "Discriminator loss 0.843039333820343\n",
      "Generator loss 4.021516799926758\n",
      "Discriminator loss 0.7938634753227234\n",
      "Generator loss 4.1798787117004395\n",
      "Discriminator loss 0.834730863571167\n",
      "Generator loss 4.35921049118042\n",
      "Discriminator loss 0.8175326585769653\n",
      "Generator loss 3.5816869735717773\n",
      "Discriminator loss 0.8509830832481384\n",
      "Generator loss 4.453596115112305\n",
      "Discriminator loss 0.799024760723114\n",
      "Generator loss 4.55995512008667\n",
      "Discriminator loss 0.8482588529586792\n",
      "Generator loss 3.939218521118164\n",
      "Discriminator loss 0.8374424576759338\n",
      "Generator loss 4.215921878814697\n",
      "Discriminator loss 0.8393235802650452\n",
      "Generator loss 4.131073951721191\n",
      "Discriminator loss 1.0000656843185425\n",
      "Generator loss 4.263734340667725\n",
      "Discriminator loss 0.8964061737060547\n",
      "Generator loss 4.10557746887207\n",
      "Discriminator loss 0.7844361662864685\n",
      "Generator loss 3.6801114082336426\n",
      "Discriminator loss 0.767397403717041\n",
      "Generator loss 4.559967517852783\n",
      "Discriminator loss 0.9177377820014954\n",
      "Generator loss 3.9484219551086426\n",
      "Discriminator loss 0.7952716946601868\n",
      "Generator loss 4.123599052429199\n",
      "Discriminator loss 0.8736236095428467\n",
      "Generator loss 4.23132848739624\n",
      "Discriminator loss 0.7545811533927917\n",
      "Generator loss 4.492999076843262\n",
      "Discriminator loss 0.7902175188064575\n",
      "Generator loss 4.785396099090576\n",
      "Discriminator loss 0.8953164219856262\n",
      "Generator loss 3.7835934162139893\n",
      "Discriminator loss 0.9889816045761108\n",
      "Generator loss 3.8100743293762207\n",
      "Discriminator loss 0.8635227084159851\n",
      "Generator loss 4.280848503112793\n",
      "Discriminator loss 0.8124284148216248\n",
      "Generator loss 3.9558253288269043\n",
      "Discriminator loss 0.8405787944793701\n",
      "Generator loss 4.422881603240967\n",
      "Discriminator loss 0.9344494342803955\n",
      "Generator loss 3.875009536743164\n",
      "Discriminator loss 0.898554801940918\n",
      "Generator loss 3.7179455757141113\n",
      "Epoch loss\n",
      "Discriminator loss 0.7872858643531799\n",
      "Generator loss 4.215294361114502\n",
      "Discriminator loss 0.813352108001709\n",
      "Generator loss 4.2569732666015625\n",
      "Discriminator loss 0.867156445980072\n",
      "Generator loss 4.276564598083496\n",
      "Discriminator loss 0.8975692987442017\n",
      "Generator loss 3.8140759468078613\n",
      "Discriminator loss 0.8248371481895447\n",
      "Generator loss 3.8998022079467773\n",
      "Discriminator loss 0.790414571762085\n",
      "Generator loss 4.0871262550354\n",
      "Discriminator loss 0.8574479222297668\n",
      "Generator loss 3.7041242122650146\n",
      "Discriminator loss 0.8514511585235596\n",
      "Generator loss 4.196702003479004\n",
      "Discriminator loss 0.8185257315635681\n",
      "Generator loss 4.091335296630859\n",
      "Discriminator loss 0.8920043706893921\n",
      "Generator loss 4.1462297439575195\n",
      "Discriminator loss 0.8077845573425293\n",
      "Generator loss 3.875574827194214\n",
      "Discriminator loss 0.84822016954422\n",
      "Generator loss 4.246697425842285\n",
      "Discriminator loss 0.8488277792930603\n",
      "Generator loss 4.371589183807373\n",
      "Discriminator loss 0.8959969878196716\n",
      "Generator loss 4.305842876434326\n",
      "Discriminator loss 0.8866691589355469\n",
      "Generator loss 4.158641338348389\n",
      "Discriminator loss 0.8244986534118652\n",
      "Generator loss 4.024507522583008\n",
      "Discriminator loss 0.9591628313064575\n",
      "Generator loss 3.9107704162597656\n",
      "Discriminator loss 0.7913536429405212\n",
      "Generator loss 4.4503254890441895\n",
      "Discriminator loss 1.0174425840377808\n",
      "Generator loss 4.292851448059082\n",
      "Discriminator loss 0.836491584777832\n",
      "Generator loss 4.610528945922852\n",
      "Discriminator loss 0.8985304236412048\n",
      "Generator loss 3.910576581954956\n",
      "Discriminator loss 0.816167950630188\n",
      "Generator loss 4.27724552154541\n",
      "Discriminator loss 0.8602962493896484\n",
      "Generator loss 3.9958267211914062\n",
      "Discriminator loss 0.9602419137954712\n",
      "Generator loss 4.078577995300293\n",
      "Discriminator loss 0.8833208084106445\n",
      "Generator loss 3.786816120147705\n",
      "Epoch loss\n",
      "Discriminator loss 0.8707298636436462\n",
      "Generator loss 4.4182281494140625\n",
      "Discriminator loss 0.8995903134346008\n",
      "Generator loss 4.344662666320801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.8575974702835083\n",
      "Generator loss 3.6291894912719727\n",
      "Discriminator loss 0.7302228212356567\n",
      "Generator loss 3.991732597351074\n",
      "Discriminator loss 0.8873907327651978\n",
      "Generator loss 3.9963464736938477\n",
      "Discriminator loss 0.8472645282745361\n",
      "Generator loss 4.136930465698242\n",
      "Discriminator loss 0.8441489934921265\n",
      "Generator loss 3.89363694190979\n",
      "Discriminator loss 0.8365560173988342\n",
      "Generator loss 3.9208059310913086\n",
      "Discriminator loss 0.9338507056236267\n",
      "Generator loss 3.9517416954040527\n",
      "Discriminator loss 0.8740854263305664\n",
      "Generator loss 4.270742416381836\n",
      "Discriminator loss 0.9308071732521057\n",
      "Generator loss 4.398636817932129\n",
      "Discriminator loss 0.7416395545005798\n",
      "Generator loss 4.068292140960693\n",
      "Discriminator loss 0.8969092965126038\n",
      "Generator loss 4.287948131561279\n",
      "Discriminator loss 0.7749593257904053\n",
      "Generator loss 4.246126651763916\n",
      "Discriminator loss 0.8622623682022095\n",
      "Generator loss 3.874382972717285\n",
      "Discriminator loss 0.8284279108047485\n",
      "Generator loss 3.5888073444366455\n",
      "Discriminator loss 0.7852683663368225\n",
      "Generator loss 3.6741068363189697\n",
      "Discriminator loss 0.8443952202796936\n",
      "Generator loss 4.24418306350708\n",
      "Discriminator loss 0.8305986523628235\n",
      "Generator loss 4.050417423248291\n",
      "Discriminator loss 0.848517894744873\n",
      "Generator loss 3.937525749206543\n",
      "Discriminator loss 0.761182427406311\n",
      "Generator loss 4.411273002624512\n",
      "Discriminator loss 0.9014593362808228\n",
      "Generator loss 4.518194198608398\n",
      "Discriminator loss 0.9373849630355835\n",
      "Generator loss 4.197948455810547\n",
      "Discriminator loss 0.8414676785469055\n",
      "Generator loss 4.381048202514648\n",
      "Discriminator loss 0.9896506667137146\n",
      "Generator loss 4.061976909637451\n",
      "Epoch loss\n",
      "Discriminator loss 0.8107157945632935\n",
      "Generator loss 3.592618942260742\n",
      "Discriminator loss 0.9570232033729553\n",
      "Generator loss 3.9658560752868652\n",
      "Discriminator loss 0.7875717878341675\n",
      "Generator loss 4.015983581542969\n",
      "Discriminator loss 0.8484349846839905\n",
      "Generator loss 4.242837905883789\n",
      "Discriminator loss 0.7775683403015137\n",
      "Generator loss 4.006174564361572\n",
      "Discriminator loss 0.7773482799530029\n",
      "Generator loss 4.809220314025879\n",
      "Discriminator loss 0.8788180351257324\n",
      "Generator loss 4.208267688751221\n",
      "Discriminator loss 0.8099288940429688\n",
      "Generator loss 3.628945827484131\n",
      "Discriminator loss 0.8685814738273621\n",
      "Generator loss 4.029824256896973\n",
      "Discriminator loss 0.8728760480880737\n",
      "Generator loss 4.413241386413574\n",
      "Discriminator loss 0.7605348229408264\n",
      "Generator loss 4.260437965393066\n",
      "Discriminator loss 0.7766640782356262\n",
      "Generator loss 3.9937543869018555\n",
      "Discriminator loss 0.948539137840271\n",
      "Generator loss 3.999328851699829\n",
      "Discriminator loss 0.8691422343254089\n",
      "Generator loss 4.4334306716918945\n",
      "Discriminator loss 0.9667901396751404\n",
      "Generator loss 4.413582801818848\n",
      "Discriminator loss 0.9346324801445007\n",
      "Generator loss 4.102353096008301\n",
      "Discriminator loss 0.8668230175971985\n",
      "Generator loss 3.8512086868286133\n",
      "Discriminator loss 0.8309599757194519\n",
      "Generator loss 4.225536823272705\n",
      "Discriminator loss 0.8713642954826355\n",
      "Generator loss 4.414610862731934\n",
      "Discriminator loss 0.8626585602760315\n",
      "Generator loss 3.852510929107666\n",
      "Discriminator loss 0.8682864904403687\n",
      "Generator loss 3.78118896484375\n",
      "Discriminator loss 0.8007203340530396\n",
      "Generator loss 3.8353285789489746\n",
      "Discriminator loss 0.8431434035301208\n",
      "Generator loss 4.232037544250488\n",
      "Discriminator loss 0.8368484973907471\n",
      "Generator loss 4.087153434753418\n",
      "Discriminator loss 0.9372455477714539\n",
      "Generator loss 3.665989398956299\n",
      "Epoch loss\n",
      "Discriminator loss 0.7254523038864136\n",
      "Generator loss 4.07816743850708\n",
      "Discriminator loss 0.8941792845726013\n",
      "Generator loss 3.66890811920166\n",
      "Discriminator loss 0.7971805334091187\n",
      "Generator loss 4.052137851715088\n",
      "Discriminator loss 0.9234800934791565\n",
      "Generator loss 4.057866096496582\n",
      "Discriminator loss 0.7671449780464172\n",
      "Generator loss 3.9788355827331543\n",
      "Discriminator loss 0.9609160423278809\n",
      "Generator loss 3.7616219520568848\n",
      "Discriminator loss 0.9626017212867737\n",
      "Generator loss 4.1673197746276855\n",
      "Discriminator loss 0.9380302429199219\n",
      "Generator loss 3.8979649543762207\n",
      "Discriminator loss 0.7095044851303101\n",
      "Generator loss 3.7154572010040283\n",
      "Discriminator loss 0.7572097182273865\n",
      "Generator loss 4.331015586853027\n",
      "Discriminator loss 0.8213696479797363\n",
      "Generator loss 3.901528835296631\n",
      "Discriminator loss 0.844251275062561\n",
      "Generator loss 3.9608383178710938\n",
      "Discriminator loss 0.8990296721458435\n",
      "Generator loss 3.730778217315674\n",
      "Discriminator loss 0.8731907606124878\n",
      "Generator loss 3.8602442741394043\n",
      "Discriminator loss 0.9818243384361267\n",
      "Generator loss 3.8820013999938965\n",
      "Discriminator loss 0.8174722790718079\n",
      "Generator loss 4.284830570220947\n",
      "Discriminator loss 0.8152066469192505\n",
      "Generator loss 4.140737533569336\n",
      "Discriminator loss 0.8275713324546814\n",
      "Generator loss 4.326586723327637\n",
      "Discriminator loss 0.9392789602279663\n",
      "Generator loss 3.9510741233825684\n",
      "Discriminator loss 0.8811228275299072\n",
      "Generator loss 3.7034873962402344\n",
      "Discriminator loss 0.8043786883354187\n",
      "Generator loss 4.547029495239258\n",
      "Discriminator loss 0.9985812306404114\n",
      "Generator loss 4.1484198570251465\n",
      "Discriminator loss 0.8885279297828674\n",
      "Generator loss 3.7368156909942627\n",
      "Discriminator loss 0.8175905346870422\n",
      "Generator loss 3.8076274394989014\n",
      "Discriminator loss 1.045430064201355\n",
      "Generator loss 3.753530263900757\n",
      "Epoch loss\n",
      "Discriminator loss 0.8889043927192688\n",
      "Generator loss 4.403587341308594\n",
      "Discriminator loss 0.8585742712020874\n",
      "Generator loss 3.730473279953003\n",
      "Discriminator loss 0.9908661246299744\n",
      "Generator loss 3.8886168003082275\n",
      "Discriminator loss 0.9219408631324768\n",
      "Generator loss 4.2850165367126465\n",
      "Discriminator loss 0.851264238357544\n",
      "Generator loss 4.1789045333862305\n",
      "Discriminator loss 0.8998474478721619\n",
      "Generator loss 3.8673744201660156\n",
      "Discriminator loss 0.9330301880836487\n",
      "Generator loss 4.091994762420654\n",
      "Discriminator loss 0.7945616245269775\n",
      "Generator loss 3.8110861778259277\n",
      "Discriminator loss 0.8388634324073792\n",
      "Generator loss 3.7621467113494873\n",
      "Discriminator loss 0.8986234664916992\n",
      "Generator loss 3.5094802379608154\n",
      "Discriminator loss 0.7949607968330383\n",
      "Generator loss 3.7694733142852783\n",
      "Discriminator loss 0.7983921766281128\n",
      "Generator loss 4.040693283081055\n",
      "Discriminator loss 0.957526445388794\n",
      "Generator loss 4.261259078979492\n",
      "Discriminator loss 0.8175672888755798\n",
      "Generator loss 3.689417600631714\n",
      "Discriminator loss 0.8767427206039429\n",
      "Generator loss 3.4670939445495605\n",
      "Discriminator loss 0.8702207207679749\n",
      "Generator loss 4.308794021606445\n",
      "Discriminator loss 0.7875629663467407\n",
      "Generator loss 3.9106602668762207\n",
      "Discriminator loss 0.7756383419036865\n",
      "Generator loss 4.28972053527832\n",
      "Discriminator loss 0.808760941028595\n",
      "Generator loss 3.5625691413879395\n",
      "Discriminator loss 0.8171339631080627\n",
      "Generator loss 3.909665107727051\n",
      "Discriminator loss 0.9898317456245422\n",
      "Generator loss 4.3621602058410645\n",
      "Discriminator loss 0.9333084225654602\n",
      "Generator loss 4.003693103790283\n",
      "Discriminator loss 0.8039407730102539\n",
      "Generator loss 4.398921966552734\n",
      "Discriminator loss 0.8950538635253906\n",
      "Generator loss 4.053022861480713\n",
      "Discriminator loss 0.7968298196792603\n",
      "Generator loss 4.656437873840332\n",
      "Epoch loss\n",
      "Discriminator loss 0.791022539138794\n",
      "Generator loss 4.048519611358643\n",
      "Discriminator loss 1.0303006172180176\n",
      "Generator loss 4.374713897705078\n",
      "Discriminator loss 0.7874586582183838\n",
      "Generator loss 4.075920104980469\n",
      "Discriminator loss 0.8630568385124207\n",
      "Generator loss 3.6320619583129883\n",
      "Discriminator loss 0.861057460308075\n",
      "Generator loss 3.976330041885376\n",
      "Discriminator loss 0.8279999494552612\n",
      "Generator loss 3.746209144592285\n",
      "Discriminator loss 0.9408339262008667\n",
      "Generator loss 4.326201915740967\n",
      "Discriminator loss 0.7762010097503662\n",
      "Generator loss 4.219442844390869\n",
      "Discriminator loss 0.9936128854751587\n",
      "Generator loss 3.824073314666748\n",
      "Discriminator loss 0.7830846309661865\n",
      "Generator loss 4.456580638885498\n",
      "Discriminator loss 0.8396864533424377\n",
      "Generator loss 3.9359753131866455\n",
      "Discriminator loss 0.8937909603118896\n",
      "Generator loss 3.596204996109009\n",
      "Discriminator loss 0.8331353068351746\n",
      "Generator loss 3.8349812030792236\n",
      "Discriminator loss 0.8838557004928589\n",
      "Generator loss 4.392532825469971\n",
      "Discriminator loss 0.8530795574188232\n",
      "Generator loss 4.638535022735596\n",
      "Discriminator loss 0.7794597744941711\n",
      "Generator loss 4.120255470275879\n",
      "Discriminator loss 0.8467640280723572\n",
      "Generator loss 4.596138954162598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.7801833748817444\n",
      "Generator loss 4.057733535766602\n",
      "Discriminator loss 0.8284012079238892\n",
      "Generator loss 4.1410346031188965\n",
      "Discriminator loss 0.8843474388122559\n",
      "Generator loss 4.048421859741211\n",
      "Discriminator loss 0.8387420177459717\n",
      "Generator loss 3.829021453857422\n",
      "Discriminator loss 0.8403458595275879\n",
      "Generator loss 3.7435858249664307\n",
      "Discriminator loss 1.0061964988708496\n",
      "Generator loss 4.374170303344727\n",
      "Discriminator loss 0.9667895436286926\n",
      "Generator loss 4.359292984008789\n",
      "Discriminator loss 0.9709850549697876\n",
      "Generator loss 4.303666591644287\n",
      "Epoch loss\n",
      "Discriminator loss 0.9778202772140503\n",
      "Generator loss 4.120607376098633\n",
      "Discriminator loss 0.8467650413513184\n",
      "Generator loss 4.261774063110352\n",
      "Discriminator loss 0.8740850687026978\n",
      "Generator loss 4.179353713989258\n",
      "Discriminator loss 0.8046573996543884\n",
      "Generator loss 4.191792011260986\n",
      "Discriminator loss 1.037692666053772\n",
      "Generator loss 4.170041084289551\n",
      "Discriminator loss 0.8849781155586243\n",
      "Generator loss 3.9380416870117188\n",
      "Discriminator loss 0.9736689329147339\n",
      "Generator loss 3.8669815063476562\n",
      "Discriminator loss 0.8170501589775085\n",
      "Generator loss 3.708836078643799\n",
      "Discriminator loss 0.9155003428459167\n",
      "Generator loss 3.82096791267395\n",
      "Discriminator loss 0.8725680112838745\n",
      "Generator loss 3.3979692459106445\n",
      "Discriminator loss 0.9810529947280884\n",
      "Generator loss 3.9543092250823975\n",
      "Discriminator loss 0.7828812003135681\n",
      "Generator loss 4.095058917999268\n",
      "Discriminator loss 0.8630352020263672\n",
      "Generator loss 4.1097211837768555\n",
      "Discriminator loss 0.827535092830658\n",
      "Generator loss 3.878931999206543\n",
      "Discriminator loss 0.9827561974525452\n",
      "Generator loss 4.042356967926025\n",
      "Discriminator loss 0.8920538425445557\n",
      "Generator loss 3.9677693843841553\n",
      "Discriminator loss 0.8500666618347168\n",
      "Generator loss 4.462342739105225\n",
      "Discriminator loss 0.8826605081558228\n",
      "Generator loss 3.9638051986694336\n",
      "Discriminator loss 0.8052352666854858\n",
      "Generator loss 4.285232067108154\n",
      "Discriminator loss 0.7653724551200867\n",
      "Generator loss 3.8061203956604004\n",
      "Discriminator loss 0.9456081390380859\n",
      "Generator loss 4.215444564819336\n",
      "Discriminator loss 0.8768599033355713\n",
      "Generator loss 3.9797279834747314\n",
      "Discriminator loss 0.7691540718078613\n",
      "Generator loss 3.8859806060791016\n",
      "Discriminator loss 0.8872144222259521\n",
      "Generator loss 4.023665904998779\n",
      "Discriminator loss 0.8454066514968872\n",
      "Generator loss 3.9914660453796387\n",
      "Epoch loss\n",
      "Discriminator loss 0.8570795059204102\n",
      "Generator loss 4.681482315063477\n",
      "Discriminator loss 0.8948914408683777\n",
      "Generator loss 4.1754608154296875\n",
      "Discriminator loss 0.8561066389083862\n",
      "Generator loss 4.361202716827393\n",
      "Discriminator loss 0.8316929936408997\n",
      "Generator loss 3.9031927585601807\n",
      "Discriminator loss 0.9169142842292786\n",
      "Generator loss 4.219522476196289\n",
      "Discriminator loss 0.8733394742012024\n",
      "Generator loss 4.2323150634765625\n",
      "Discriminator loss 0.8263712525367737\n",
      "Generator loss 3.940239429473877\n",
      "Discriminator loss 0.8530246019363403\n",
      "Generator loss 3.739182710647583\n",
      "Discriminator loss 0.8341891765594482\n",
      "Generator loss 3.8579211235046387\n",
      "Discriminator loss 0.8953241109848022\n",
      "Generator loss 4.230851650238037\n",
      "Discriminator loss 0.852509081363678\n",
      "Generator loss 4.16890811920166\n",
      "Discriminator loss 0.8804548978805542\n",
      "Generator loss 4.0737996101379395\n",
      "Discriminator loss 0.8468485474586487\n",
      "Generator loss 3.5651259422302246\n",
      "Discriminator loss 0.7750058174133301\n",
      "Generator loss 3.865581512451172\n",
      "Discriminator loss 0.9176155924797058\n",
      "Generator loss 3.94425892829895\n",
      "Discriminator loss 0.7944579720497131\n",
      "Generator loss 4.238826751708984\n",
      "Discriminator loss 0.8418715000152588\n",
      "Generator loss 3.5515267848968506\n",
      "Discriminator loss 0.886139988899231\n",
      "Generator loss 3.952157974243164\n",
      "Discriminator loss 0.8058768510818481\n",
      "Generator loss 4.135015964508057\n",
      "Discriminator loss 0.8524175882339478\n",
      "Generator loss 3.962907552719116\n",
      "Discriminator loss 0.9372472167015076\n",
      "Generator loss 4.299354553222656\n",
      "Discriminator loss 0.8105987310409546\n",
      "Generator loss 3.9702954292297363\n",
      "Discriminator loss 0.9205313920974731\n",
      "Generator loss 4.029323577880859\n",
      "Discriminator loss 0.9835984110832214\n",
      "Generator loss 3.6486544609069824\n",
      "Discriminator loss 0.7798136472702026\n",
      "Generator loss 4.204660892486572\n",
      "Epoch loss\n",
      "Discriminator loss 0.8313338756561279\n",
      "Generator loss 3.9097819328308105\n",
      "Discriminator loss 0.8969384431838989\n",
      "Generator loss 4.365217208862305\n",
      "Discriminator loss 0.819451093673706\n",
      "Generator loss 3.7901134490966797\n",
      "Discriminator loss 0.8211767673492432\n",
      "Generator loss 3.621124744415283\n",
      "Discriminator loss 0.8789990544319153\n",
      "Generator loss 3.306856632232666\n",
      "Discriminator loss 0.9241898059844971\n",
      "Generator loss 4.222997665405273\n",
      "Discriminator loss 0.8720546364784241\n",
      "Generator loss 3.625882148742676\n",
      "Discriminator loss 0.8072671890258789\n",
      "Generator loss 3.508950710296631\n",
      "Discriminator loss 0.846651017665863\n",
      "Generator loss 4.162670612335205\n",
      "Discriminator loss 0.8423367142677307\n",
      "Generator loss 3.8786568641662598\n",
      "Discriminator loss 0.8874872922897339\n",
      "Generator loss 4.154510974884033\n",
      "Discriminator loss 0.8580836653709412\n",
      "Generator loss 3.882673978805542\n",
      "Discriminator loss 0.8556309938430786\n",
      "Generator loss 3.679399013519287\n",
      "Discriminator loss 0.8275070190429688\n",
      "Generator loss 4.266201496124268\n",
      "Discriminator loss 0.8667647838592529\n",
      "Generator loss 3.8866825103759766\n",
      "Discriminator loss 0.8024122714996338\n",
      "Generator loss 4.2140092849731445\n",
      "Discriminator loss 0.8244010210037231\n",
      "Generator loss 4.00784158706665\n",
      "Discriminator loss 0.8993057608604431\n",
      "Generator loss 3.7995052337646484\n",
      "Discriminator loss 0.8422362804412842\n",
      "Generator loss 4.010138034820557\n",
      "Discriminator loss 0.8509402275085449\n",
      "Generator loss 4.066878795623779\n",
      "Discriminator loss 0.932890772819519\n",
      "Generator loss 3.649810791015625\n",
      "Discriminator loss 0.8626856803894043\n",
      "Generator loss 3.838113307952881\n",
      "Discriminator loss 0.9391441345214844\n",
      "Generator loss 3.5579051971435547\n",
      "Discriminator loss 0.9329819679260254\n",
      "Generator loss 3.814215898513794\n",
      "Discriminator loss 0.9353079795837402\n",
      "Generator loss 4.126185417175293\n",
      "Epoch loss\n",
      "Discriminator loss 0.8287065029144287\n",
      "Generator loss 3.9299087524414062\n",
      "Discriminator loss 0.8182952404022217\n",
      "Generator loss 3.8277440071105957\n",
      "Discriminator loss 0.8526200652122498\n",
      "Generator loss 3.8034961223602295\n",
      "Discriminator loss 0.788699746131897\n",
      "Generator loss 3.954498767852783\n",
      "Discriminator loss 0.9092509746551514\n",
      "Generator loss 3.7775251865386963\n",
      "Discriminator loss 0.8190047144889832\n",
      "Generator loss 3.8627567291259766\n",
      "Discriminator loss 0.9213682413101196\n",
      "Generator loss 3.7301712036132812\n",
      "Discriminator loss 1.0238255262374878\n",
      "Generator loss 4.112987518310547\n",
      "Discriminator loss 0.8144693970680237\n",
      "Generator loss 4.2881903648376465\n",
      "Discriminator loss 0.8471673727035522\n",
      "Generator loss 3.9730935096740723\n",
      "Discriminator loss 0.9243426322937012\n",
      "Generator loss 4.10117244720459\n",
      "Discriminator loss 0.7918769717216492\n",
      "Generator loss 4.100584030151367\n",
      "Discriminator loss 0.8586421012878418\n",
      "Generator loss 3.8999011516571045\n",
      "Discriminator loss 0.8220193386077881\n",
      "Generator loss 4.191100120544434\n",
      "Discriminator loss 0.8221688866615295\n",
      "Generator loss 3.5016980171203613\n",
      "Discriminator loss 0.7824753522872925\n",
      "Generator loss 4.088092803955078\n",
      "Discriminator loss 0.8777013421058655\n",
      "Generator loss 3.693467378616333\n",
      "Discriminator loss 0.8747711777687073\n",
      "Generator loss 3.66247820854187\n",
      "Discriminator loss 0.8880428075790405\n",
      "Generator loss 3.549710273742676\n",
      "Discriminator loss 0.8491782546043396\n",
      "Generator loss 3.6475250720977783\n",
      "Discriminator loss 0.7756911516189575\n",
      "Generator loss 3.9267916679382324\n",
      "Discriminator loss 0.8421326279640198\n",
      "Generator loss 3.9447059631347656\n",
      "Discriminator loss 0.933600902557373\n",
      "Generator loss 3.619520664215088\n",
      "Discriminator loss 0.8469987511634827\n",
      "Generator loss 4.49881649017334\n",
      "Discriminator loss 0.8338665962219238\n",
      "Generator loss 3.705448627471924\n",
      "Epoch loss\n",
      "Discriminator loss 0.8982516527175903\n",
      "Generator loss 4.171468257904053\n",
      "Discriminator loss 0.9094969034194946\n",
      "Generator loss 3.6729483604431152\n",
      "Discriminator loss 0.8232529759407043\n",
      "Generator loss 3.9789445400238037\n",
      "Discriminator loss 1.017094373703003\n",
      "Generator loss 4.297445297241211\n",
      "Discriminator loss 0.8683536052703857\n",
      "Generator loss 4.077117919921875\n",
      "Discriminator loss 0.8219784498214722\n",
      "Generator loss 3.976712226867676\n",
      "Discriminator loss 0.8274329304695129\n",
      "Generator loss 4.369557857513428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.8939975500106812\n",
      "Generator loss 3.9479856491088867\n",
      "Discriminator loss 0.8997868299484253\n",
      "Generator loss 4.091053485870361\n",
      "Discriminator loss 0.7764737010002136\n",
      "Generator loss 3.8696999549865723\n",
      "Discriminator loss 0.9137790203094482\n",
      "Generator loss 3.8878867626190186\n",
      "Discriminator loss 0.8276262283325195\n",
      "Generator loss 4.1900129318237305\n",
      "Discriminator loss 0.8397303223609924\n",
      "Generator loss 3.932121753692627\n",
      "Discriminator loss 0.891330361366272\n",
      "Generator loss 4.316271781921387\n",
      "Discriminator loss 0.8212823867797852\n",
      "Generator loss 3.7677860260009766\n",
      "Discriminator loss 0.926976203918457\n",
      "Generator loss 3.7143566608428955\n",
      "Discriminator loss 0.8203649520874023\n",
      "Generator loss 3.9988577365875244\n",
      "Discriminator loss 0.906726598739624\n",
      "Generator loss 3.448599338531494\n",
      "Discriminator loss 0.7272314429283142\n",
      "Generator loss 3.5896472930908203\n",
      "Discriminator loss 0.7874866724014282\n",
      "Generator loss 3.8041529655456543\n",
      "Discriminator loss 0.8041930198669434\n",
      "Generator loss 3.8188750743865967\n",
      "Discriminator loss 0.930191159248352\n",
      "Generator loss 3.678947687149048\n",
      "Discriminator loss 0.8573055863380432\n",
      "Generator loss 4.164688587188721\n",
      "Discriminator loss 0.862591028213501\n",
      "Generator loss 3.8842568397521973\n",
      "Discriminator loss 0.8247892260551453\n",
      "Generator loss 3.7222533226013184\n",
      "Epoch loss\n",
      "Discriminator loss 0.8019838333129883\n",
      "Generator loss 4.139425754547119\n",
      "Discriminator loss 0.7871366739273071\n",
      "Generator loss 3.7429447174072266\n",
      "Discriminator loss 0.8052723407745361\n",
      "Generator loss 4.144622325897217\n",
      "Discriminator loss 0.8951289653778076\n",
      "Generator loss 4.232463836669922\n",
      "Discriminator loss 0.8701005578041077\n",
      "Generator loss 3.647265911102295\n",
      "Discriminator loss 0.8614442944526672\n",
      "Generator loss 4.01074743270874\n",
      "Discriminator loss 0.9035896062850952\n",
      "Generator loss 4.105718612670898\n",
      "Discriminator loss 0.9061365127563477\n",
      "Generator loss 3.786776304244995\n",
      "Discriminator loss 0.7577791213989258\n",
      "Generator loss 3.779724597930908\n",
      "Discriminator loss 0.7141700983047485\n",
      "Generator loss 3.828298330307007\n",
      "Discriminator loss 0.9724671840667725\n",
      "Generator loss 3.8119056224823\n",
      "Discriminator loss 0.802315890789032\n",
      "Generator loss 3.9738681316375732\n",
      "Discriminator loss 0.8478881120681763\n",
      "Generator loss 3.786597490310669\n",
      "Discriminator loss 0.8822447657585144\n",
      "Generator loss 3.9904704093933105\n",
      "Discriminator loss 0.8252391815185547\n",
      "Generator loss 3.9134135246276855\n",
      "Discriminator loss 0.96990966796875\n",
      "Generator loss 4.161799907684326\n",
      "Discriminator loss 0.7633484601974487\n",
      "Generator loss 4.1141676902771\n",
      "Discriminator loss 0.8032312393188477\n",
      "Generator loss 4.097766876220703\n",
      "Discriminator loss 0.9135798811912537\n",
      "Generator loss 3.9102020263671875\n",
      "Discriminator loss 0.8304834961891174\n",
      "Generator loss 4.079784870147705\n",
      "Discriminator loss 0.822422206401825\n",
      "Generator loss 3.8238139152526855\n",
      "Discriminator loss 1.0147185325622559\n",
      "Generator loss 3.841870069503784\n",
      "Discriminator loss 0.8587932586669922\n",
      "Generator loss 3.64772367477417\n",
      "Discriminator loss 0.817057728767395\n",
      "Generator loss 4.202803134918213\n",
      "Discriminator loss 0.7834569811820984\n",
      "Generator loss 3.838958501815796\n",
      "Epoch loss\n",
      "Discriminator loss 0.8917646408081055\n",
      "Generator loss 4.159454345703125\n",
      "Discriminator loss 0.9157710075378418\n",
      "Generator loss 3.2674853801727295\n",
      "Discriminator loss 0.8542308807373047\n",
      "Generator loss 4.088804244995117\n",
      "Discriminator loss 1.0445966720581055\n",
      "Generator loss 4.446335315704346\n",
      "Discriminator loss 0.8130091428756714\n",
      "Generator loss 4.124068260192871\n",
      "Discriminator loss 0.8773749470710754\n",
      "Generator loss 4.317526817321777\n",
      "Discriminator loss 0.9334479570388794\n",
      "Generator loss 4.001220226287842\n",
      "Discriminator loss 0.8532766103744507\n",
      "Generator loss 3.6102192401885986\n",
      "Discriminator loss 0.8437001705169678\n",
      "Generator loss 3.959425449371338\n",
      "Discriminator loss 0.9443600177764893\n",
      "Generator loss 3.630506753921509\n",
      "Discriminator loss 0.8383597135543823\n",
      "Generator loss 3.871263265609741\n",
      "Discriminator loss 0.7969028949737549\n",
      "Generator loss 4.14436674118042\n",
      "Discriminator loss 0.8282186985015869\n",
      "Generator loss 4.006189346313477\n",
      "Discriminator loss 0.9133354425430298\n",
      "Generator loss 3.9416630268096924\n",
      "Discriminator loss 0.7727441191673279\n",
      "Generator loss 3.936521530151367\n",
      "Discriminator loss 0.8055549263954163\n",
      "Generator loss 3.9517345428466797\n",
      "Discriminator loss 0.8774542808532715\n",
      "Generator loss 3.854145050048828\n",
      "Discriminator loss 0.8688508868217468\n",
      "Generator loss 4.287599086761475\n",
      "Discriminator loss 0.8939304947853088\n",
      "Generator loss 4.072854995727539\n",
      "Discriminator loss 0.9239366054534912\n",
      "Generator loss 4.218912124633789\n",
      "Discriminator loss 0.8254936337471008\n",
      "Generator loss 4.1312174797058105\n",
      "Discriminator loss 0.8140915036201477\n",
      "Generator loss 3.8272628784179688\n",
      "Discriminator loss 0.8132719993591309\n",
      "Generator loss 3.842576265335083\n",
      "Discriminator loss 0.8018515110015869\n",
      "Generator loss 4.043371200561523\n",
      "Discriminator loss 0.9135093092918396\n",
      "Generator loss 4.242956638336182\n",
      "Epoch loss\n",
      "Discriminator loss 0.8647618293762207\n",
      "Generator loss 4.449256420135498\n",
      "Discriminator loss 0.8661126494407654\n",
      "Generator loss 4.205870628356934\n",
      "Discriminator loss 0.908245325088501\n",
      "Generator loss 3.8721632957458496\n",
      "Discriminator loss 0.9736584424972534\n",
      "Generator loss 3.750514507293701\n",
      "Discriminator loss 0.8292115926742554\n",
      "Generator loss 3.684014320373535\n",
      "Discriminator loss 1.050209641456604\n",
      "Generator loss 3.9486119747161865\n",
      "Discriminator loss 0.7920726537704468\n",
      "Generator loss 3.5624194145202637\n",
      "Discriminator loss 0.7874382734298706\n",
      "Generator loss 3.900960683822632\n",
      "Discriminator loss 0.7599563598632812\n",
      "Generator loss 4.087462902069092\n",
      "Discriminator loss 0.8756941556930542\n",
      "Generator loss 3.6998543739318848\n",
      "Discriminator loss 0.8885993957519531\n",
      "Generator loss 3.464015007019043\n",
      "Discriminator loss 0.9090322256088257\n",
      "Generator loss 4.123322486877441\n",
      "Discriminator loss 0.8278439044952393\n",
      "Generator loss 4.3945393562316895\n",
      "Discriminator loss 0.83194500207901\n",
      "Generator loss 4.048671722412109\n",
      "Discriminator loss 0.9431488513946533\n",
      "Generator loss 3.8987786769866943\n",
      "Discriminator loss 0.8874289393424988\n",
      "Generator loss 3.687947988510132\n",
      "Discriminator loss 0.9310657382011414\n",
      "Generator loss 3.836099624633789\n",
      "Discriminator loss 0.9455442428588867\n",
      "Generator loss 3.923896312713623\n",
      "Discriminator loss 0.8716235756874084\n",
      "Generator loss 3.7381374835968018\n",
      "Discriminator loss 1.0226961374282837\n",
      "Generator loss 4.666882038116455\n",
      "Discriminator loss 0.891237199306488\n",
      "Generator loss 3.836519718170166\n",
      "Discriminator loss 0.9783356189727783\n",
      "Generator loss 3.931771993637085\n",
      "Discriminator loss 0.8142489194869995\n",
      "Generator loss 4.003909587860107\n",
      "Discriminator loss 0.8783194422721863\n",
      "Generator loss 4.298445701599121\n",
      "Discriminator loss 1.0046716928482056\n",
      "Generator loss 3.957653522491455\n",
      "Epoch loss\n",
      "Discriminator loss 0.9900005459785461\n",
      "Generator loss 3.9872636795043945\n",
      "Discriminator loss 1.0305360555648804\n",
      "Generator loss 4.2025628089904785\n",
      "Discriminator loss 0.9554017782211304\n",
      "Generator loss 4.14069938659668\n",
      "Discriminator loss 0.8636220693588257\n",
      "Generator loss 3.762336254119873\n",
      "Discriminator loss 1.0487929582595825\n",
      "Generator loss 3.9314208030700684\n",
      "Discriminator loss 0.9388974905014038\n",
      "Generator loss 3.510453462600708\n",
      "Discriminator loss 0.9069937467575073\n",
      "Generator loss 3.619211435317993\n",
      "Discriminator loss 0.9049902558326721\n",
      "Generator loss 4.05621337890625\n",
      "Discriminator loss 0.8249251246452332\n",
      "Generator loss 4.07356595993042\n",
      "Discriminator loss 0.8565552830696106\n",
      "Generator loss 3.2496795654296875\n",
      "Discriminator loss 0.9594744443893433\n",
      "Generator loss 3.836585521697998\n",
      "Discriminator loss 0.931867778301239\n",
      "Generator loss 3.966735363006592\n",
      "Discriminator loss 0.8739515542984009\n",
      "Generator loss 3.9477157592773438\n",
      "Discriminator loss 0.7599764466285706\n",
      "Generator loss 4.166583061218262\n",
      "Discriminator loss 0.8397704362869263\n",
      "Generator loss 4.058960914611816\n",
      "Discriminator loss 0.8926002979278564\n",
      "Generator loss 3.832484722137451\n",
      "Discriminator loss 1.0180081129074097\n",
      "Generator loss 3.8457162380218506\n",
      "Discriminator loss 0.9767234921455383\n",
      "Generator loss 3.735963821411133\n",
      "Discriminator loss 0.829030454158783\n",
      "Generator loss 3.375570297241211\n",
      "Discriminator loss 0.8481701016426086\n",
      "Generator loss 3.8439624309539795\n",
      "Discriminator loss 0.8383881449699402\n",
      "Generator loss 4.329076290130615\n",
      "Discriminator loss 0.811546266078949\n",
      "Generator loss 4.265355110168457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.9639863967895508\n",
      "Generator loss 3.806868076324463\n",
      "Discriminator loss 0.9103359580039978\n",
      "Generator loss 3.95699143409729\n",
      "Discriminator loss 0.9700859785079956\n",
      "Generator loss 4.014242172241211\n",
      "Epoch loss\n",
      "Discriminator loss 0.8822187185287476\n",
      "Generator loss 3.72245454788208\n",
      "Discriminator loss 0.8084152936935425\n",
      "Generator loss 4.4337239265441895\n",
      "Discriminator loss 0.9173102378845215\n",
      "Generator loss 4.2186808586120605\n",
      "Discriminator loss 0.8312455415725708\n",
      "Generator loss 3.782466173171997\n",
      "Discriminator loss 0.7835738062858582\n",
      "Generator loss 3.996608018875122\n",
      "Discriminator loss 0.9685297608375549\n",
      "Generator loss 3.826738119125366\n",
      "Discriminator loss 0.9943206906318665\n",
      "Generator loss 3.320082187652588\n",
      "Discriminator loss 0.8440293073654175\n",
      "Generator loss 3.775369167327881\n",
      "Discriminator loss 0.8289394378662109\n",
      "Generator loss 4.153435230255127\n",
      "Discriminator loss 0.7580209970474243\n",
      "Generator loss 3.8036797046661377\n",
      "Discriminator loss 0.9595737457275391\n",
      "Generator loss 4.028451919555664\n",
      "Discriminator loss 0.8384857773780823\n",
      "Generator loss 4.348397254943848\n",
      "Discriminator loss 0.888577938079834\n",
      "Generator loss 4.114696025848389\n",
      "Discriminator loss 0.8042718172073364\n",
      "Generator loss 3.96712064743042\n",
      "Discriminator loss 0.797359824180603\n",
      "Generator loss 4.350347518920898\n",
      "Discriminator loss 0.8627877831459045\n",
      "Generator loss 3.875650405883789\n",
      "Discriminator loss 0.9021098017692566\n",
      "Generator loss 4.19776725769043\n",
      "Discriminator loss 0.81758052110672\n",
      "Generator loss 3.718639850616455\n",
      "Discriminator loss 0.8445588946342468\n",
      "Generator loss 4.4836015701293945\n",
      "Discriminator loss 0.8945460915565491\n",
      "Generator loss 3.9223482608795166\n",
      "Discriminator loss 0.7844600677490234\n",
      "Generator loss 4.138020038604736\n",
      "Discriminator loss 0.950342059135437\n",
      "Generator loss 3.617710828781128\n",
      "Discriminator loss 0.8834366798400879\n",
      "Generator loss 3.6263580322265625\n",
      "Discriminator loss 0.8053290843963623\n",
      "Generator loss 3.7618331909179688\n",
      "Discriminator loss 0.8002347946166992\n",
      "Generator loss 3.691741466522217\n",
      "Epoch loss\n",
      "Discriminator loss 0.8596373796463013\n",
      "Generator loss 4.253681182861328\n",
      "Discriminator loss 0.9278244376182556\n",
      "Generator loss 3.274404764175415\n",
      "Discriminator loss 0.8217500448226929\n",
      "Generator loss 3.8928847312927246\n",
      "Discriminator loss 0.8931768536567688\n",
      "Generator loss 3.9738378524780273\n",
      "Discriminator loss 0.899581789970398\n",
      "Generator loss 3.958296060562134\n",
      "Discriminator loss 0.8613009452819824\n",
      "Generator loss 3.8266453742980957\n",
      "Discriminator loss 0.9166086912155151\n",
      "Generator loss 3.718700885772705\n",
      "Discriminator loss 0.8797816634178162\n",
      "Generator loss 4.099410057067871\n",
      "Discriminator loss 0.8094717264175415\n",
      "Generator loss 3.904146432876587\n",
      "Discriminator loss 0.9030710458755493\n",
      "Generator loss 4.3040924072265625\n",
      "Discriminator loss 0.8810490369796753\n",
      "Generator loss 3.9651143550872803\n",
      "Discriminator loss 1.1152491569519043\n",
      "Generator loss 3.8293232917785645\n",
      "Discriminator loss 0.8092027902603149\n",
      "Generator loss 3.0893423557281494\n",
      "Discriminator loss 0.7074028253555298\n",
      "Generator loss 4.268427848815918\n",
      "Discriminator loss 0.8361964821815491\n",
      "Generator loss 4.061975479125977\n",
      "Discriminator loss 0.7375034689903259\n",
      "Generator loss 4.217979431152344\n",
      "Discriminator loss 0.8483338356018066\n",
      "Generator loss 3.53212571144104\n",
      "Discriminator loss 0.8009733557701111\n",
      "Generator loss 4.437139511108398\n",
      "Discriminator loss 0.9407723546028137\n",
      "Generator loss 4.129750728607178\n",
      "Discriminator loss 0.9882522821426392\n",
      "Generator loss 4.217626571655273\n",
      "Discriminator loss 0.8564139604568481\n",
      "Generator loss 3.9736037254333496\n",
      "Discriminator loss 0.9604443311691284\n",
      "Generator loss 3.8770275115966797\n",
      "Discriminator loss 0.8197265267372131\n",
      "Generator loss 3.37648868560791\n",
      "Discriminator loss 0.8390034437179565\n",
      "Generator loss 3.788151502609253\n",
      "Discriminator loss 0.8265018463134766\n",
      "Generator loss 3.9080095291137695\n",
      "Epoch loss\n",
      "Discriminator loss 0.8655709028244019\n",
      "Generator loss 3.570432662963867\n",
      "Discriminator loss 0.8627134561538696\n",
      "Generator loss 3.8356528282165527\n",
      "Discriminator loss 0.8462091684341431\n",
      "Generator loss 3.8659188747406006\n",
      "Discriminator loss 0.9469063878059387\n",
      "Generator loss 3.7661402225494385\n",
      "Discriminator loss 0.9728447198867798\n",
      "Generator loss 3.8212108612060547\n",
      "Discriminator loss 0.789813220500946\n",
      "Generator loss 4.237346172332764\n",
      "Discriminator loss 0.8356795310974121\n",
      "Generator loss 3.668076515197754\n",
      "Discriminator loss 0.8203135132789612\n",
      "Generator loss 3.8726730346679688\n",
      "Discriminator loss 0.8513187766075134\n",
      "Generator loss 3.713611125946045\n",
      "Discriminator loss 0.9277330636978149\n",
      "Generator loss 3.7359914779663086\n",
      "Discriminator loss 0.8394981622695923\n",
      "Generator loss 3.581686496734619\n",
      "Discriminator loss 0.8873924612998962\n",
      "Generator loss 3.9051389694213867\n",
      "Discriminator loss 1.0087734460830688\n",
      "Generator loss 4.210641860961914\n",
      "Discriminator loss 0.8139677047729492\n",
      "Generator loss 3.8158071041107178\n",
      "Discriminator loss 0.9686827659606934\n",
      "Generator loss 4.070721626281738\n",
      "Discriminator loss 0.8874918222427368\n",
      "Generator loss 3.692436456680298\n",
      "Discriminator loss 0.7755059003829956\n",
      "Generator loss 4.116586208343506\n",
      "Discriminator loss 0.8141644597053528\n",
      "Generator loss 4.20503044128418\n",
      "Discriminator loss 0.8074012994766235\n",
      "Generator loss 4.159935474395752\n",
      "Discriminator loss 0.8958115577697754\n",
      "Generator loss 3.5342819690704346\n",
      "Discriminator loss 0.8167879581451416\n",
      "Generator loss 4.234218597412109\n",
      "Discriminator loss 0.7770112156867981\n",
      "Generator loss 3.437549114227295\n",
      "Discriminator loss 0.9003171920776367\n",
      "Generator loss 4.7588701248168945\n",
      "Discriminator loss 0.8653510808944702\n",
      "Generator loss 3.830876350402832\n",
      "Discriminator loss 0.8569526672363281\n",
      "Generator loss 3.9570510387420654\n",
      "Epoch loss\n",
      "Discriminator loss 0.9529469013214111\n",
      "Generator loss 3.891524314880371\n",
      "Discriminator loss 0.86654132604599\n",
      "Generator loss 3.504619836807251\n",
      "Discriminator loss 1.048259973526001\n",
      "Generator loss 3.9355509281158447\n",
      "Discriminator loss 0.8515833020210266\n",
      "Generator loss 3.693445920944214\n",
      "Discriminator loss 0.9030868411064148\n",
      "Generator loss 3.7073750495910645\n",
      "Discriminator loss 0.9031451344490051\n",
      "Generator loss 3.8607728481292725\n",
      "Discriminator loss 0.8834018111228943\n",
      "Generator loss 3.9337780475616455\n",
      "Discriminator loss 0.9054245352745056\n",
      "Generator loss 3.7417726516723633\n",
      "Discriminator loss 0.902232825756073\n",
      "Generator loss 4.176687240600586\n",
      "Discriminator loss 0.9516677260398865\n",
      "Generator loss 3.378498077392578\n",
      "Discriminator loss 0.9355694055557251\n",
      "Generator loss 4.005943775177002\n",
      "Discriminator loss 0.8124523162841797\n",
      "Generator loss 4.3326568603515625\n",
      "Discriminator loss 0.9349915385246277\n",
      "Generator loss 3.8291072845458984\n",
      "Discriminator loss 0.9099947214126587\n",
      "Generator loss 3.7605082988739014\n",
      "Discriminator loss 0.8313668370246887\n",
      "Generator loss 3.8826799392700195\n",
      "Discriminator loss 0.8525886535644531\n",
      "Generator loss 3.884396553039551\n",
      "Discriminator loss 0.8379868865013123\n",
      "Generator loss 4.133533000946045\n",
      "Discriminator loss 0.8263832926750183\n",
      "Generator loss 3.9621901512145996\n",
      "Discriminator loss 0.8142692446708679\n",
      "Generator loss 4.127402305603027\n",
      "Discriminator loss 0.7769150733947754\n",
      "Generator loss 3.6854026317596436\n",
      "Discriminator loss 0.8338552713394165\n",
      "Generator loss 3.7006139755249023\n",
      "Discriminator loss 0.8648119568824768\n",
      "Generator loss 3.560811996459961\n",
      "Discriminator loss 0.7795738577842712\n",
      "Generator loss 3.8785290718078613\n",
      "Discriminator loss 0.9462360739707947\n",
      "Generator loss 3.6918537616729736\n",
      "Discriminator loss 0.9208557605743408\n",
      "Generator loss 3.5297203063964844\n",
      "Epoch loss\n",
      "Discriminator loss 0.9596202373504639\n",
      "Generator loss 3.379744052886963\n",
      "Discriminator loss 0.7818686962127686\n",
      "Generator loss 3.895840644836426\n",
      "Discriminator loss 0.8104882836341858\n",
      "Generator loss 4.022192478179932\n",
      "Discriminator loss 0.8974341750144958\n",
      "Generator loss 3.627152919769287\n",
      "Discriminator loss 0.8536978960037231\n",
      "Generator loss 4.424307823181152\n",
      "Discriminator loss 0.8316678404808044\n",
      "Generator loss 4.059187889099121\n",
      "Discriminator loss 0.9606264233589172\n",
      "Generator loss 3.847555160522461\n",
      "Discriminator loss 0.8740127682685852\n",
      "Generator loss 3.807075262069702\n",
      "Discriminator loss 0.9454259276390076\n",
      "Generator loss 3.335132598876953\n",
      "Discriminator loss 0.7982637882232666\n",
      "Generator loss 3.9733452796936035\n",
      "Discriminator loss 0.8669010400772095\n",
      "Generator loss 3.8394222259521484\n",
      "Discriminator loss 0.972474992275238\n",
      "Generator loss 4.0290303230285645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.7992539405822754\n",
      "Generator loss 4.324155807495117\n",
      "Discriminator loss 0.8344258069992065\n",
      "Generator loss 3.8769829273223877\n",
      "Discriminator loss 0.9500479698181152\n",
      "Generator loss 4.375099182128906\n",
      "Discriminator loss 0.8961290121078491\n",
      "Generator loss 3.642651081085205\n",
      "Discriminator loss 0.9134323000907898\n",
      "Generator loss 3.7987656593322754\n",
      "Discriminator loss 0.8043338060379028\n",
      "Generator loss 3.849144458770752\n",
      "Discriminator loss 0.7904864549636841\n",
      "Generator loss 4.327998638153076\n",
      "Discriminator loss 0.8591514229774475\n",
      "Generator loss 3.990762710571289\n",
      "Discriminator loss 0.8254765272140503\n",
      "Generator loss 4.19397497177124\n",
      "Discriminator loss 0.7486838102340698\n",
      "Generator loss 3.979670763015747\n",
      "Discriminator loss 0.8276682496070862\n",
      "Generator loss 3.5105173587799072\n",
      "Discriminator loss 0.7644104957580566\n",
      "Generator loss 3.6335246562957764\n",
      "Discriminator loss 0.8823110461235046\n",
      "Generator loss 3.713639259338379\n",
      "Epoch loss\n",
      "Discriminator loss 0.7998500466346741\n",
      "Generator loss 3.710831880569458\n",
      "Discriminator loss 0.8898271918296814\n",
      "Generator loss 3.5235958099365234\n",
      "Discriminator loss 0.878612756729126\n",
      "Generator loss 3.67187237739563\n",
      "Discriminator loss 0.7444452047348022\n",
      "Generator loss 3.662075996398926\n",
      "Discriminator loss 0.7639009952545166\n",
      "Generator loss 4.177678108215332\n",
      "Discriminator loss 0.809816300868988\n",
      "Generator loss 3.8958234786987305\n",
      "Discriminator loss 0.9106304049491882\n",
      "Generator loss 4.37194299697876\n",
      "Discriminator loss 0.8600203394889832\n",
      "Generator loss 4.137141227722168\n",
      "Discriminator loss 0.8678253293037415\n",
      "Generator loss 3.6717491149902344\n",
      "Discriminator loss 0.8650600910186768\n",
      "Generator loss 3.938943386077881\n",
      "Discriminator loss 0.9376702308654785\n",
      "Generator loss 3.4819324016571045\n",
      "Discriminator loss 0.8475368022918701\n",
      "Generator loss 3.7817957401275635\n",
      "Discriminator loss 0.8408772945404053\n",
      "Generator loss 3.8390955924987793\n",
      "Discriminator loss 0.984027624130249\n",
      "Generator loss 3.882291316986084\n",
      "Discriminator loss 0.8860676288604736\n",
      "Generator loss 3.4723243713378906\n",
      "Discriminator loss 0.8148980140686035\n",
      "Generator loss 4.038335800170898\n",
      "Discriminator loss 0.8955709934234619\n",
      "Generator loss 3.9612698554992676\n",
      "Discriminator loss 0.8310912251472473\n",
      "Generator loss 3.660343885421753\n",
      "Discriminator loss 1.0007222890853882\n",
      "Generator loss 3.8150033950805664\n",
      "Discriminator loss 0.9094845056533813\n",
      "Generator loss 3.8571465015411377\n",
      "Discriminator loss 0.8411957025527954\n",
      "Generator loss 4.079889297485352\n",
      "Discriminator loss 0.9851923584938049\n",
      "Generator loss 4.209254264831543\n",
      "Discriminator loss 0.8386529684066772\n",
      "Generator loss 4.428948879241943\n",
      "Discriminator loss 0.7356963157653809\n",
      "Generator loss 3.646702766418457\n",
      "Discriminator loss 0.8541024923324585\n",
      "Generator loss 4.367311000823975\n",
      "Epoch loss\n",
      "Discriminator loss 0.9177671074867249\n",
      "Generator loss 3.6668171882629395\n",
      "Discriminator loss 0.7979890704154968\n",
      "Generator loss 4.02763032913208\n",
      "Discriminator loss 0.8287584781646729\n",
      "Generator loss 3.5706822872161865\n",
      "Discriminator loss 0.949359655380249\n",
      "Generator loss 3.6027352809906006\n",
      "Discriminator loss 0.8385789394378662\n",
      "Generator loss 4.079807758331299\n",
      "Discriminator loss 0.8487082719802856\n",
      "Generator loss 4.030588150024414\n",
      "Discriminator loss 0.8921196460723877\n",
      "Generator loss 3.9590632915496826\n",
      "Discriminator loss 0.8453776240348816\n",
      "Generator loss 3.754507303237915\n",
      "Discriminator loss 0.8547671437263489\n",
      "Generator loss 3.9958648681640625\n",
      "Discriminator loss 0.8108505606651306\n",
      "Generator loss 3.8785908222198486\n",
      "Discriminator loss 0.8814394474029541\n",
      "Generator loss 4.002936363220215\n",
      "Discriminator loss 0.8151743412017822\n",
      "Generator loss 4.0147385597229\n",
      "Discriminator loss 0.8530714511871338\n",
      "Generator loss 3.728510618209839\n",
      "Discriminator loss 0.856177806854248\n",
      "Generator loss 3.8705391883850098\n",
      "Discriminator loss 0.8680194020271301\n",
      "Generator loss 4.1620354652404785\n",
      "Discriminator loss 0.8992435932159424\n",
      "Generator loss 3.3016045093536377\n",
      "Discriminator loss 0.9789731502532959\n",
      "Generator loss 4.121013164520264\n",
      "Discriminator loss 0.8973067998886108\n",
      "Generator loss 4.152460098266602\n",
      "Discriminator loss 0.9425756931304932\n",
      "Generator loss 3.846123218536377\n",
      "Discriminator loss 0.8873385190963745\n",
      "Generator loss 3.57334303855896\n",
      "Discriminator loss 0.8605707883834839\n",
      "Generator loss 4.055823802947998\n",
      "Discriminator loss 0.8509200811386108\n",
      "Generator loss 4.24676513671875\n",
      "Discriminator loss 0.8443464040756226\n",
      "Generator loss 3.957542896270752\n",
      "Discriminator loss 0.9888384342193604\n",
      "Generator loss 4.027837753295898\n",
      "Discriminator loss 0.8668196797370911\n",
      "Generator loss 3.6272659301757812\n",
      "Epoch loss\n",
      "Discriminator loss 0.8772386312484741\n",
      "Generator loss 3.8210854530334473\n",
      "Discriminator loss 0.7494214773178101\n",
      "Generator loss 3.9007325172424316\n",
      "Discriminator loss 0.8709402084350586\n",
      "Generator loss 3.6796624660491943\n",
      "Discriminator loss 0.8592070937156677\n",
      "Generator loss 4.014143943786621\n",
      "Discriminator loss 0.8130167722702026\n",
      "Generator loss 3.801306962966919\n",
      "Discriminator loss 0.7705506682395935\n",
      "Generator loss 4.060114860534668\n",
      "Discriminator loss 0.8829469084739685\n",
      "Generator loss 3.8606972694396973\n",
      "Discriminator loss 0.8026189804077148\n",
      "Generator loss 3.591914653778076\n",
      "Discriminator loss 0.7556408643722534\n",
      "Generator loss 3.797299861907959\n",
      "Discriminator loss 0.9359821677207947\n",
      "Generator loss 3.6961541175842285\n",
      "Discriminator loss 0.9513264894485474\n",
      "Generator loss 4.100486755371094\n",
      "Discriminator loss 0.8976445198059082\n",
      "Generator loss 4.193251132965088\n",
      "Discriminator loss 0.820540726184845\n",
      "Generator loss 3.6669342517852783\n",
      "Discriminator loss 0.8365006446838379\n",
      "Generator loss 3.4784374237060547\n",
      "Discriminator loss 0.9580622315406799\n",
      "Generator loss 3.067559242248535\n",
      "Discriminator loss 0.8746665716171265\n",
      "Generator loss 4.133028984069824\n",
      "Discriminator loss 0.8412060737609863\n",
      "Generator loss 3.359602212905884\n",
      "Discriminator loss 0.8399516344070435\n",
      "Generator loss 4.005035877227783\n",
      "Discriminator loss 0.8876101970672607\n",
      "Generator loss 3.486701011657715\n",
      "Discriminator loss 1.0074775218963623\n",
      "Generator loss 3.97990083694458\n",
      "Discriminator loss 0.9098644256591797\n",
      "Generator loss 3.623065948486328\n",
      "Discriminator loss 0.8876399993896484\n",
      "Generator loss 3.7994625568389893\n",
      "Discriminator loss 0.8278501033782959\n",
      "Generator loss 3.9609062671661377\n",
      "Discriminator loss 0.7845786809921265\n",
      "Generator loss 3.716524362564087\n",
      "Discriminator loss 0.8008632063865662\n",
      "Generator loss 4.068911075592041\n",
      "Epoch loss\n",
      "Discriminator loss 0.9017018675804138\n",
      "Generator loss 3.961442470550537\n",
      "Discriminator loss 0.9329730868339539\n",
      "Generator loss 3.2310597896575928\n",
      "Discriminator loss 0.9617719650268555\n",
      "Generator loss 4.04240083694458\n",
      "Discriminator loss 0.8294929265975952\n",
      "Generator loss 3.9007391929626465\n",
      "Discriminator loss 0.8288938999176025\n",
      "Generator loss 4.151374816894531\n",
      "Discriminator loss 0.7984603643417358\n",
      "Generator loss 3.824530839920044\n",
      "Discriminator loss 0.8003271222114563\n",
      "Generator loss 3.822138786315918\n",
      "Discriminator loss 0.9400916695594788\n",
      "Generator loss 3.9124062061309814\n",
      "Discriminator loss 1.013380527496338\n",
      "Generator loss 3.7323195934295654\n",
      "Discriminator loss 0.8899297118186951\n",
      "Generator loss 3.5582380294799805\n",
      "Discriminator loss 1.0480892658233643\n",
      "Generator loss 4.015896797180176\n",
      "Discriminator loss 0.8921647071838379\n",
      "Generator loss 3.8757145404815674\n",
      "Discriminator loss 0.8869137763977051\n",
      "Generator loss 3.943107843399048\n",
      "Discriminator loss 0.814180314540863\n",
      "Generator loss 3.7493062019348145\n",
      "Discriminator loss 0.8486862182617188\n",
      "Generator loss 3.7382636070251465\n",
      "Discriminator loss 0.9572911858558655\n",
      "Generator loss 3.905339002609253\n",
      "Discriminator loss 0.9638640880584717\n",
      "Generator loss 3.741333484649658\n",
      "Discriminator loss 0.9344143867492676\n",
      "Generator loss 3.8895769119262695\n",
      "Discriminator loss 0.860939621925354\n",
      "Generator loss 3.877640724182129\n",
      "Discriminator loss 0.7494683861732483\n",
      "Generator loss 3.9746336936950684\n",
      "Discriminator loss 0.8241267204284668\n",
      "Generator loss 3.629868984222412\n",
      "Discriminator loss 0.7521126866340637\n",
      "Generator loss 3.8117752075195312\n",
      "Discriminator loss 0.8099387884140015\n",
      "Generator loss 3.7094757556915283\n",
      "Discriminator loss 0.8075762391090393\n",
      "Generator loss 3.7919058799743652\n",
      "Discriminator loss 0.7714953422546387\n",
      "Generator loss 3.291268825531006\n",
      "Epoch loss\n",
      "Discriminator loss 1.013322353363037\n",
      "Generator loss 3.813660144805908\n",
      "Discriminator loss 0.8694774508476257\n",
      "Generator loss 3.4820573329925537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.9367942214012146\n",
      "Generator loss 3.5070104598999023\n",
      "Discriminator loss 0.8363937139511108\n",
      "Generator loss 3.6252994537353516\n",
      "Discriminator loss 0.8270896673202515\n",
      "Generator loss 3.616921901702881\n",
      "Discriminator loss 0.7903828620910645\n",
      "Generator loss 3.90040922164917\n",
      "Discriminator loss 0.808282732963562\n",
      "Generator loss 3.37695050239563\n",
      "Discriminator loss 0.9116185903549194\n",
      "Generator loss 3.8726460933685303\n",
      "Discriminator loss 0.7978567481040955\n",
      "Generator loss 3.841647148132324\n",
      "Discriminator loss 0.9572657942771912\n",
      "Generator loss 3.8159584999084473\n",
      "Discriminator loss 0.8925184011459351\n",
      "Generator loss 3.8778514862060547\n",
      "Discriminator loss 0.9542231559753418\n",
      "Generator loss 4.354148864746094\n",
      "Discriminator loss 0.7513546943664551\n",
      "Generator loss 3.9334731101989746\n",
      "Discriminator loss 0.8360574841499329\n",
      "Generator loss 3.932943105697632\n",
      "Discriminator loss 1.057155966758728\n",
      "Generator loss 4.377377986907959\n",
      "Discriminator loss 0.7808809280395508\n",
      "Generator loss 3.3074116706848145\n",
      "Discriminator loss 0.80760258436203\n",
      "Generator loss 3.638059139251709\n",
      "Discriminator loss 0.9125885963439941\n",
      "Generator loss 3.856369733810425\n",
      "Discriminator loss 0.8649924397468567\n",
      "Generator loss 4.10318660736084\n",
      "Discriminator loss 0.856555163860321\n",
      "Generator loss 3.2699737548828125\n",
      "Discriminator loss 0.8827179074287415\n",
      "Generator loss 4.000199794769287\n",
      "Discriminator loss 0.921673595905304\n",
      "Generator loss 3.6703617572784424\n",
      "Discriminator loss 0.8240441083908081\n",
      "Generator loss 3.558422803878784\n",
      "Discriminator loss 0.9039053320884705\n",
      "Generator loss 3.6855432987213135\n",
      "Discriminator loss 0.7849974632263184\n",
      "Generator loss 3.5077016353607178\n",
      "Epoch loss\n",
      "Discriminator loss 0.7896220684051514\n",
      "Generator loss 3.4281187057495117\n",
      "Discriminator loss 0.756475567817688\n",
      "Generator loss 3.8030574321746826\n",
      "Discriminator loss 0.9001967906951904\n",
      "Generator loss 3.602736234664917\n",
      "Discriminator loss 0.7649000287055969\n",
      "Generator loss 3.6654324531555176\n",
      "Discriminator loss 0.8651585578918457\n",
      "Generator loss 4.325329780578613\n",
      "Discriminator loss 0.8650550246238708\n",
      "Generator loss 3.628913640975952\n",
      "Discriminator loss 0.8754113912582397\n",
      "Generator loss 3.817720413208008\n",
      "Discriminator loss 1.0158987045288086\n",
      "Generator loss 4.273175239562988\n",
      "Discriminator loss 0.8375692367553711\n",
      "Generator loss 3.4553561210632324\n",
      "Discriminator loss 0.8680713772773743\n",
      "Generator loss 3.617706537246704\n",
      "Discriminator loss 0.9115657806396484\n",
      "Generator loss 3.515096426010132\n",
      "Discriminator loss 0.8802014589309692\n",
      "Generator loss 3.82918119430542\n",
      "Discriminator loss 0.8628671169281006\n",
      "Generator loss 4.089421272277832\n",
      "Discriminator loss 0.9555666446685791\n",
      "Generator loss 3.650653839111328\n",
      "Discriminator loss 0.8238943815231323\n",
      "Generator loss 3.5313374996185303\n",
      "Discriminator loss 0.8174299597740173\n",
      "Generator loss 3.503085136413574\n",
      "Discriminator loss 0.8238309621810913\n",
      "Generator loss 4.003484725952148\n",
      "Discriminator loss 0.8200968503952026\n",
      "Generator loss 3.9939706325531006\n",
      "Discriminator loss 0.885998010635376\n",
      "Generator loss 3.517202138900757\n",
      "Discriminator loss 0.8617161512374878\n",
      "Generator loss 3.929288864135742\n",
      "Discriminator loss 0.7995681166648865\n",
      "Generator loss 3.6009230613708496\n",
      "Discriminator loss 0.9292078614234924\n",
      "Generator loss 3.4016201496124268\n",
      "Discriminator loss 0.7888503074645996\n",
      "Generator loss 3.698498010635376\n",
      "Discriminator loss 0.7887210845947266\n",
      "Generator loss 3.649214744567871\n",
      "Discriminator loss 0.8070106506347656\n",
      "Generator loss 4.399076461791992\n",
      "Epoch loss\n",
      "Discriminator loss 0.8685300946235657\n",
      "Generator loss 3.656203508377075\n",
      "Discriminator loss 0.9220294952392578\n",
      "Generator loss 3.360177993774414\n",
      "Discriminator loss 0.9030188322067261\n",
      "Generator loss 3.7535486221313477\n",
      "Discriminator loss 0.8391622304916382\n",
      "Generator loss 3.9215004444122314\n",
      "Discriminator loss 0.7951833009719849\n",
      "Generator loss 3.804145574569702\n",
      "Discriminator loss 0.953792154788971\n",
      "Generator loss 3.8329007625579834\n",
      "Discriminator loss 0.8476327061653137\n",
      "Generator loss 4.010369777679443\n",
      "Discriminator loss 0.8967046737670898\n",
      "Generator loss 3.7534542083740234\n",
      "Discriminator loss 0.8838329911231995\n",
      "Generator loss 4.192022323608398\n",
      "Discriminator loss 0.9176535606384277\n",
      "Generator loss 3.9698355197906494\n",
      "Discriminator loss 0.7798488140106201\n",
      "Generator loss 3.558011531829834\n",
      "Discriminator loss 0.8811702728271484\n",
      "Generator loss 3.6847431659698486\n",
      "Discriminator loss 0.8634757399559021\n",
      "Generator loss 3.782292366027832\n",
      "Discriminator loss 0.8026161789894104\n",
      "Generator loss 3.5916624069213867\n",
      "Discriminator loss 0.8859469890594482\n",
      "Generator loss 3.5903494358062744\n",
      "Discriminator loss 0.866563081741333\n",
      "Generator loss 3.7002410888671875\n",
      "Discriminator loss 0.8617043495178223\n",
      "Generator loss 3.856729507446289\n",
      "Discriminator loss 0.875699520111084\n",
      "Generator loss 4.106691360473633\n",
      "Discriminator loss 0.8362881541252136\n",
      "Generator loss 3.791574001312256\n",
      "Discriminator loss 0.9250196218490601\n",
      "Generator loss 4.24861478805542\n",
      "Discriminator loss 0.8070228099822998\n",
      "Generator loss 3.4928269386291504\n",
      "Discriminator loss 0.9851831197738647\n",
      "Generator loss 3.6904916763305664\n",
      "Discriminator loss 0.8523670434951782\n",
      "Generator loss 3.4663360118865967\n",
      "Discriminator loss 0.7864053249359131\n",
      "Generator loss 3.7278990745544434\n",
      "Discriminator loss 0.9303334951400757\n",
      "Generator loss 3.8086369037628174\n",
      "Epoch loss\n",
      "Discriminator loss 0.9146764278411865\n",
      "Generator loss 3.34273099899292\n",
      "Discriminator loss 0.8729410767555237\n",
      "Generator loss 3.5463998317718506\n",
      "Discriminator loss 0.8596965074539185\n",
      "Generator loss 3.740525722503662\n",
      "Discriminator loss 0.832413911819458\n",
      "Generator loss 3.529003620147705\n",
      "Discriminator loss 0.9547430276870728\n",
      "Generator loss 3.4663047790527344\n",
      "Discriminator loss 0.8639061450958252\n",
      "Generator loss 4.070255279541016\n",
      "Discriminator loss 0.8682520985603333\n",
      "Generator loss 3.4486656188964844\n",
      "Discriminator loss 0.895881175994873\n",
      "Generator loss 3.807868480682373\n",
      "Discriminator loss 0.862040638923645\n",
      "Generator loss 3.83622407913208\n",
      "Discriminator loss 0.9016721844673157\n",
      "Generator loss 3.887511968612671\n",
      "Discriminator loss 0.8205600380897522\n",
      "Generator loss 3.6683313846588135\n",
      "Discriminator loss 0.7868738770484924\n",
      "Generator loss 3.909482479095459\n",
      "Discriminator loss 0.7716941237449646\n",
      "Generator loss 3.758096694946289\n",
      "Discriminator loss 1.1754777431488037\n",
      "Generator loss 4.001724720001221\n",
      "Discriminator loss 1.0095463991165161\n",
      "Generator loss 3.7748241424560547\n",
      "Discriminator loss 0.7761529684066772\n",
      "Generator loss 3.6978611946105957\n",
      "Discriminator loss 0.8943860530853271\n",
      "Generator loss 4.279137134552002\n",
      "Discriminator loss 0.879309892654419\n",
      "Generator loss 3.587526321411133\n",
      "Discriminator loss 0.8236515522003174\n",
      "Generator loss 4.1236772537231445\n",
      "Discriminator loss 0.9015907645225525\n",
      "Generator loss 3.505378007888794\n",
      "Discriminator loss 0.9272399544715881\n",
      "Generator loss 3.62593150138855\n",
      "Discriminator loss 0.7874100208282471\n",
      "Generator loss 3.8645308017730713\n",
      "Discriminator loss 0.803371787071228\n",
      "Generator loss 3.983376979827881\n",
      "Discriminator loss 0.9556661248207092\n",
      "Generator loss 4.05745267868042\n",
      "Discriminator loss 0.7985892295837402\n",
      "Generator loss 3.615830898284912\n",
      "Epoch loss\n",
      "Discriminator loss 0.7903239727020264\n",
      "Generator loss 3.753024101257324\n",
      "Discriminator loss 0.8669701218605042\n",
      "Generator loss 3.968320846557617\n",
      "Discriminator loss 0.878287672996521\n",
      "Generator loss 3.916290044784546\n",
      "Discriminator loss 0.9424331784248352\n",
      "Generator loss 3.544492244720459\n",
      "Discriminator loss 0.9265102744102478\n",
      "Generator loss 3.2744197845458984\n",
      "Discriminator loss 0.843395471572876\n",
      "Generator loss 3.650393009185791\n",
      "Discriminator loss 0.8766782283782959\n",
      "Generator loss 3.487271547317505\n",
      "Discriminator loss 0.8916484117507935\n",
      "Generator loss 3.531243085861206\n",
      "Discriminator loss 0.7924197316169739\n",
      "Generator loss 3.9046709537506104\n",
      "Discriminator loss 0.8003346920013428\n",
      "Generator loss 3.816951274871826\n",
      "Discriminator loss 0.8352794647216797\n",
      "Generator loss 3.4326863288879395\n",
      "Discriminator loss 0.8565526604652405\n",
      "Generator loss 3.349701166152954\n",
      "Discriminator loss 0.8705019950866699\n",
      "Generator loss 3.6389007568359375\n",
      "Discriminator loss 0.9553959369659424\n",
      "Generator loss 3.9827072620391846\n",
      "Discriminator loss 0.8742004036903381\n",
      "Generator loss 3.4791312217712402\n",
      "Discriminator loss 0.9189451336860657\n",
      "Generator loss 3.638185739517212\n",
      "Discriminator loss 1.0374724864959717\n",
      "Generator loss 3.6045491695404053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.9207714796066284\n",
      "Generator loss 3.79915189743042\n",
      "Discriminator loss 0.8859959840774536\n",
      "Generator loss 3.636233329772949\n",
      "Discriminator loss 0.9184311628341675\n",
      "Generator loss 3.668163537979126\n",
      "Discriminator loss 0.883436381816864\n",
      "Generator loss 3.720689535140991\n",
      "Discriminator loss 0.9050595164299011\n",
      "Generator loss 3.6683197021484375\n",
      "Discriminator loss 0.8975542783737183\n",
      "Generator loss 3.622169017791748\n",
      "Discriminator loss 0.9245493412017822\n",
      "Generator loss 4.027454853057861\n",
      "Discriminator loss 0.8355889320373535\n",
      "Generator loss 3.621022939682007\n",
      "Epoch loss\n",
      "Discriminator loss 0.8767271637916565\n",
      "Generator loss 3.9783222675323486\n",
      "Discriminator loss 0.8779594898223877\n",
      "Generator loss 3.60945987701416\n",
      "Discriminator loss 0.8208558559417725\n",
      "Generator loss 3.5040700435638428\n",
      "Discriminator loss 0.8142572641372681\n",
      "Generator loss 3.788276433944702\n",
      "Discriminator loss 0.8002505898475647\n",
      "Generator loss 3.77040958404541\n",
      "Discriminator loss 0.9564298987388611\n",
      "Generator loss 3.7545011043548584\n",
      "Discriminator loss 0.9524914026260376\n",
      "Generator loss 3.443889856338501\n",
      "Discriminator loss 0.9615499377250671\n",
      "Generator loss 3.713026523590088\n",
      "Discriminator loss 0.8946200609207153\n",
      "Generator loss 4.04078483581543\n",
      "Discriminator loss 0.8308476805686951\n",
      "Generator loss 3.835878610610962\n",
      "Discriminator loss 0.8815931081771851\n",
      "Generator loss 3.986642837524414\n",
      "Discriminator loss 0.7684383392333984\n",
      "Generator loss 3.928227424621582\n",
      "Discriminator loss 0.8228037357330322\n",
      "Generator loss 3.8533833026885986\n",
      "Discriminator loss 0.8439083695411682\n",
      "Generator loss 3.682699680328369\n",
      "Discriminator loss 0.8126084208488464\n",
      "Generator loss 4.037388324737549\n",
      "Discriminator loss 0.7975097894668579\n",
      "Generator loss 3.7026307582855225\n",
      "Discriminator loss 0.8896561861038208\n",
      "Generator loss 3.4784579277038574\n",
      "Discriminator loss 0.7695662379264832\n",
      "Generator loss 3.4801173210144043\n",
      "Discriminator loss 0.8997319936752319\n",
      "Generator loss 3.756561040878296\n",
      "Discriminator loss 0.7879505753517151\n",
      "Generator loss 3.389230251312256\n",
      "Discriminator loss 0.8453210592269897\n",
      "Generator loss 3.996185302734375\n",
      "Discriminator loss 0.7987929582595825\n",
      "Generator loss 3.7094566822052\n",
      "Discriminator loss 0.8327052593231201\n",
      "Generator loss 4.099745273590088\n",
      "Discriminator loss 0.8562340140342712\n",
      "Generator loss 3.8180809020996094\n",
      "Discriminator loss 0.8662071824073792\n",
      "Generator loss 4.160704135894775\n",
      "Epoch loss\n",
      "Discriminator loss 0.8088812828063965\n",
      "Generator loss 3.4296882152557373\n",
      "Discriminator loss 1.1277704238891602\n",
      "Generator loss 3.7849340438842773\n",
      "Discriminator loss 0.8318844437599182\n",
      "Generator loss 3.778892993927002\n",
      "Discriminator loss 0.900931179523468\n",
      "Generator loss 3.6036107540130615\n",
      "Discriminator loss 0.7788255214691162\n",
      "Generator loss 3.5104217529296875\n",
      "Discriminator loss 0.935469925403595\n",
      "Generator loss 3.622772455215454\n",
      "Discriminator loss 0.7867773175239563\n",
      "Generator loss 3.8586301803588867\n",
      "Discriminator loss 0.8993476629257202\n",
      "Generator loss 3.5017659664154053\n",
      "Discriminator loss 0.9313722252845764\n",
      "Generator loss 3.7810897827148438\n",
      "Discriminator loss 0.7281789183616638\n",
      "Generator loss 3.989743709564209\n",
      "Discriminator loss 0.8951795697212219\n",
      "Generator loss 3.6093671321868896\n",
      "Discriminator loss 0.8717125654220581\n",
      "Generator loss 4.067458152770996\n",
      "Discriminator loss 0.921543538570404\n",
      "Generator loss 3.8340253829956055\n",
      "Discriminator loss 0.9013572335243225\n",
      "Generator loss 4.153688430786133\n",
      "Discriminator loss 0.8442710638046265\n",
      "Generator loss 3.1547598838806152\n",
      "Discriminator loss 0.8018032312393188\n",
      "Generator loss 4.382541656494141\n",
      "Discriminator loss 0.8371000289916992\n",
      "Generator loss 3.6054272651672363\n",
      "Discriminator loss 0.8388599753379822\n",
      "Generator loss 3.966902256011963\n",
      "Discriminator loss 0.8193455338478088\n",
      "Generator loss 3.4663124084472656\n",
      "Discriminator loss 0.985338032245636\n",
      "Generator loss 3.8812949657440186\n",
      "Discriminator loss 0.8476207852363586\n",
      "Generator loss 3.3189187049865723\n",
      "Discriminator loss 1.0384011268615723\n",
      "Generator loss 3.7471652030944824\n",
      "Discriminator loss 0.9906437397003174\n",
      "Generator loss 3.712177276611328\n",
      "Discriminator loss 0.847245454788208\n",
      "Generator loss 3.5373425483703613\n",
      "Discriminator loss 0.8303484916687012\n",
      "Generator loss 4.256297588348389\n",
      "Epoch loss\n",
      "Discriminator loss 1.0290892124176025\n",
      "Generator loss 4.069263935089111\n",
      "Discriminator loss 0.7928026914596558\n",
      "Generator loss 3.883190393447876\n",
      "Discriminator loss 0.8647566437721252\n",
      "Generator loss 3.9998855590820312\n",
      "Discriminator loss 0.902676522731781\n",
      "Generator loss 3.5731797218322754\n",
      "Discriminator loss 0.9891987442970276\n",
      "Generator loss 3.7640955448150635\n",
      "Discriminator loss 0.882915735244751\n",
      "Generator loss 3.6534371376037598\n",
      "Discriminator loss 0.9116865396499634\n",
      "Generator loss 3.8589234352111816\n",
      "Discriminator loss 1.019677996635437\n",
      "Generator loss 3.7174460887908936\n",
      "Discriminator loss 0.7790931463241577\n",
      "Generator loss 3.7795844078063965\n",
      "Discriminator loss 0.7737256288528442\n",
      "Generator loss 3.7241759300231934\n",
      "Discriminator loss 0.8231379985809326\n",
      "Generator loss 3.935142993927002\n",
      "Discriminator loss 0.8902939558029175\n",
      "Generator loss 4.048464775085449\n",
      "Discriminator loss 0.809726893901825\n",
      "Generator loss 4.30767297744751\n",
      "Discriminator loss 0.8382852673530579\n",
      "Generator loss 3.6566762924194336\n",
      "Discriminator loss 0.8014439344406128\n",
      "Generator loss 3.8050789833068848\n",
      "Discriminator loss 0.8391263484954834\n",
      "Generator loss 3.982296943664551\n",
      "Discriminator loss 0.881666898727417\n",
      "Generator loss 3.759211301803589\n",
      "Discriminator loss 0.8489074110984802\n",
      "Generator loss 3.786281108856201\n",
      "Discriminator loss 0.8171192407608032\n",
      "Generator loss 3.770151376724243\n",
      "Discriminator loss 0.7739522457122803\n",
      "Generator loss 3.6745903491973877\n",
      "Discriminator loss 0.8686960935592651\n",
      "Generator loss 3.7580864429473877\n",
      "Discriminator loss 0.9946131110191345\n",
      "Generator loss 3.9785099029541016\n",
      "Discriminator loss 0.8327143788337708\n",
      "Generator loss 3.7310643196105957\n",
      "Discriminator loss 0.8889062404632568\n",
      "Generator loss 3.595104694366455\n",
      "Discriminator loss 0.869442880153656\n",
      "Generator loss 3.6247124671936035\n",
      "Epoch loss\n",
      "Discriminator loss 0.7695556282997131\n",
      "Generator loss 3.8113327026367188\n",
      "Discriminator loss 0.8260511159896851\n",
      "Generator loss 3.504225492477417\n",
      "Discriminator loss 0.9817879796028137\n",
      "Generator loss 3.740072727203369\n",
      "Discriminator loss 0.8666884899139404\n",
      "Generator loss 3.5655605792999268\n",
      "Discriminator loss 0.7222917079925537\n",
      "Generator loss 3.828091859817505\n",
      "Discriminator loss 0.9286113977432251\n",
      "Generator loss 3.8977370262145996\n",
      "Discriminator loss 0.9070907831192017\n",
      "Generator loss 4.077174663543701\n",
      "Discriminator loss 0.8963429927825928\n",
      "Generator loss 3.7005908489227295\n",
      "Discriminator loss 0.7663137316703796\n",
      "Generator loss 4.236300945281982\n",
      "Discriminator loss 0.8100886940956116\n",
      "Generator loss 3.57055401802063\n",
      "Discriminator loss 0.9142348170280457\n",
      "Generator loss 3.788665294647217\n",
      "Discriminator loss 0.9819461703300476\n",
      "Generator loss 3.7310023307800293\n",
      "Discriminator loss 0.8267149329185486\n",
      "Generator loss 3.526040554046631\n",
      "Discriminator loss 0.955641508102417\n",
      "Generator loss 4.014275074005127\n",
      "Discriminator loss 0.8387916684150696\n",
      "Generator loss 3.6886203289031982\n",
      "Discriminator loss 0.9281771183013916\n",
      "Generator loss 3.3737897872924805\n",
      "Discriminator loss 1.0118130445480347\n",
      "Generator loss 3.5171546936035156\n",
      "Discriminator loss 0.8527690172195435\n",
      "Generator loss 3.700349807739258\n",
      "Discriminator loss 0.911880612373352\n",
      "Generator loss 4.195763111114502\n",
      "Discriminator loss 0.8945087790489197\n",
      "Generator loss 3.7288694381713867\n",
      "Discriminator loss 0.8878345489501953\n",
      "Generator loss 3.163109302520752\n",
      "Discriminator loss 0.7456049919128418\n",
      "Generator loss 3.4539425373077393\n",
      "Discriminator loss 0.8177905678749084\n",
      "Generator loss 3.87359619140625\n",
      "Discriminator loss 0.8998507261276245\n",
      "Generator loss 3.370631456375122\n",
      "Discriminator loss 0.9788897633552551\n",
      "Generator loss 3.8202199935913086\n",
      "Epoch loss\n",
      "Discriminator loss 0.981597363948822\n",
      "Generator loss 3.6205904483795166\n",
      "Discriminator loss 0.7656345367431641\n",
      "Generator loss 3.9320123195648193\n",
      "Discriminator loss 0.7626292109489441\n",
      "Generator loss 3.7687082290649414\n",
      "Discriminator loss 0.7772090435028076\n",
      "Generator loss 3.6948983669281006\n",
      "Discriminator loss 0.7720942497253418\n",
      "Generator loss 3.689211368560791\n",
      "Discriminator loss 0.9233710765838623\n",
      "Generator loss 3.465221643447876\n",
      "Discriminator loss 0.8592013716697693\n",
      "Generator loss 3.5615508556365967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.8838524222373962\n",
      "Generator loss 3.213958978652954\n",
      "Discriminator loss 0.9615533351898193\n",
      "Generator loss 3.549539566040039\n",
      "Discriminator loss 0.7746264934539795\n",
      "Generator loss 4.032248020172119\n",
      "Discriminator loss 0.9052117466926575\n",
      "Generator loss 3.5352444648742676\n",
      "Discriminator loss 0.785595715045929\n",
      "Generator loss 3.6743314266204834\n",
      "Discriminator loss 0.9326210021972656\n",
      "Generator loss 3.8639276027679443\n",
      "Discriminator loss 0.9069137573242188\n",
      "Generator loss 3.3703408241271973\n",
      "Discriminator loss 0.9319928884506226\n",
      "Generator loss 3.5670166015625\n",
      "Discriminator loss 0.8693324327468872\n",
      "Generator loss 3.511361598968506\n",
      "Discriminator loss 0.8779751062393188\n",
      "Generator loss 3.6071012020111084\n",
      "Discriminator loss 0.8284542560577393\n",
      "Generator loss 3.2905972003936768\n",
      "Discriminator loss 0.8252081274986267\n",
      "Generator loss 3.49509596824646\n",
      "Discriminator loss 0.8977001905441284\n",
      "Generator loss 3.5623154640197754\n",
      "Discriminator loss 0.8482615947723389\n",
      "Generator loss 3.4484195709228516\n",
      "Discriminator loss 0.8435257077217102\n",
      "Generator loss 4.203797817230225\n",
      "Discriminator loss 0.8852041363716125\n",
      "Generator loss 3.583667039871216\n",
      "Discriminator loss 0.8979737758636475\n",
      "Generator loss 3.5010669231414795\n",
      "Discriminator loss 0.8602426052093506\n",
      "Generator loss 3.4810991287231445\n",
      "Epoch loss\n",
      "Discriminator loss 0.7911846041679382\n",
      "Generator loss 3.9743595123291016\n",
      "Discriminator loss 0.8821503520011902\n",
      "Generator loss 3.3600316047668457\n",
      "Discriminator loss 0.8738822340965271\n",
      "Generator loss 3.580294609069824\n",
      "Discriminator loss 0.7710007429122925\n",
      "Generator loss 3.717623233795166\n",
      "Discriminator loss 0.831246018409729\n",
      "Generator loss 3.5361759662628174\n",
      "Discriminator loss 0.9023513197898865\n",
      "Generator loss 3.9885430335998535\n",
      "Discriminator loss 0.8690140843391418\n",
      "Generator loss 3.60954213142395\n",
      "Discriminator loss 0.8079161047935486\n",
      "Generator loss 4.109932899475098\n",
      "Discriminator loss 0.8471565842628479\n",
      "Generator loss 3.90913462638855\n",
      "Discriminator loss 0.8183278441429138\n",
      "Generator loss 3.9815213680267334\n",
      "Discriminator loss 0.8944610357284546\n",
      "Generator loss 3.300180435180664\n",
      "Discriminator loss 0.8123817443847656\n",
      "Generator loss 4.127986431121826\n",
      "Discriminator loss 0.9820137023925781\n",
      "Generator loss 3.818124771118164\n",
      "Discriminator loss 0.8366202712059021\n",
      "Generator loss 3.638705015182495\n",
      "Discriminator loss 0.9039335250854492\n",
      "Generator loss 3.348416328430176\n",
      "Discriminator loss 0.8487331867218018\n",
      "Generator loss 3.8508083820343018\n",
      "Discriminator loss 0.9205039739608765\n",
      "Generator loss 3.5072107315063477\n",
      "Discriminator loss 0.8438665866851807\n",
      "Generator loss 3.5706868171691895\n",
      "Discriminator loss 0.8172582983970642\n",
      "Generator loss 3.714108943939209\n",
      "Discriminator loss 0.8442983627319336\n",
      "Generator loss 3.4942872524261475\n",
      "Discriminator loss 0.8137536644935608\n",
      "Generator loss 3.6180672645568848\n",
      "Discriminator loss 0.8953150510787964\n",
      "Generator loss 3.3102121353149414\n",
      "Discriminator loss 0.797125518321991\n",
      "Generator loss 4.074041843414307\n",
      "Discriminator loss 0.9385143518447876\n",
      "Generator loss 3.811563014984131\n",
      "Discriminator loss 0.8833335638046265\n",
      "Generator loss 3.696866989135742\n",
      "Epoch loss\n",
      "Discriminator loss 0.9651116132736206\n",
      "Generator loss 3.440807342529297\n",
      "Discriminator loss 0.8412446975708008\n",
      "Generator loss 4.2713165283203125\n",
      "Discriminator loss 0.8326592445373535\n",
      "Generator loss 3.3024895191192627\n",
      "Discriminator loss 0.8943040370941162\n",
      "Generator loss 3.706645965576172\n",
      "Discriminator loss 0.8172609806060791\n",
      "Generator loss 3.4056711196899414\n",
      "Discriminator loss 0.836767315864563\n",
      "Generator loss 3.3885247707366943\n",
      "Discriminator loss 0.8181806802749634\n",
      "Generator loss 3.715620994567871\n",
      "Discriminator loss 0.8516045212745667\n",
      "Generator loss 3.5273587703704834\n",
      "Discriminator loss 0.8137457966804504\n",
      "Generator loss 3.8288614749908447\n",
      "Discriminator loss 0.8454671502113342\n",
      "Generator loss 3.8426380157470703\n",
      "Discriminator loss 0.8543370962142944\n",
      "Generator loss 3.7801082134246826\n",
      "Discriminator loss 0.9327006340026855\n",
      "Generator loss 3.3644676208496094\n",
      "Discriminator loss 0.8855955600738525\n",
      "Generator loss 3.6607754230499268\n",
      "Discriminator loss 0.8763046264648438\n",
      "Generator loss 3.6535837650299072\n",
      "Discriminator loss 0.896242082118988\n",
      "Generator loss 3.602398157119751\n",
      "Discriminator loss 0.9322665929794312\n",
      "Generator loss 3.868290662765503\n",
      "Discriminator loss 0.8685275912284851\n",
      "Generator loss 4.06137752532959\n",
      "Discriminator loss 0.8044018149375916\n",
      "Generator loss 3.718989372253418\n",
      "Discriminator loss 0.9455585479736328\n",
      "Generator loss 3.7779107093811035\n",
      "Discriminator loss 0.8494595885276794\n",
      "Generator loss 4.177797317504883\n",
      "Discriminator loss 1.001727819442749\n",
      "Generator loss 3.422168254852295\n",
      "Discriminator loss 0.8806294798851013\n",
      "Generator loss 3.3944895267486572\n",
      "Discriminator loss 0.8616585731506348\n",
      "Generator loss 3.560392141342163\n",
      "Discriminator loss 0.8269274830818176\n",
      "Generator loss 3.6461093425750732\n",
      "Discriminator loss 0.9075596332550049\n",
      "Generator loss 3.868835210800171\n",
      "Epoch loss\n",
      "Discriminator loss 0.7948664426803589\n",
      "Generator loss 3.7496092319488525\n",
      "Discriminator loss 0.9465895891189575\n",
      "Generator loss 3.929703950881958\n",
      "Discriminator loss 0.7788811326026917\n",
      "Generator loss 3.4671123027801514\n",
      "Discriminator loss 0.9037009477615356\n",
      "Generator loss 4.0483269691467285\n",
      "Discriminator loss 0.8340135216712952\n",
      "Generator loss 3.4204530715942383\n",
      "Discriminator loss 0.9632694721221924\n",
      "Generator loss 3.106374979019165\n",
      "Discriminator loss 0.8873887062072754\n",
      "Generator loss 3.568913698196411\n",
      "Discriminator loss 0.8400083184242249\n",
      "Generator loss 3.832559823989868\n",
      "Discriminator loss 0.8815485239028931\n",
      "Generator loss 3.6240592002868652\n",
      "Discriminator loss 0.9048364162445068\n",
      "Generator loss 3.4793789386749268\n",
      "Discriminator loss 0.8488835692405701\n",
      "Generator loss 3.764253616333008\n",
      "Discriminator loss 0.9570360779762268\n",
      "Generator loss 4.149734020233154\n",
      "Discriminator loss 1.0493110418319702\n",
      "Generator loss 3.7916269302368164\n",
      "Discriminator loss 0.8510711193084717\n",
      "Generator loss 3.861675262451172\n",
      "Discriminator loss 0.8623356223106384\n",
      "Generator loss 3.6251397132873535\n",
      "Discriminator loss 0.8292516469955444\n",
      "Generator loss 3.5337822437286377\n",
      "Discriminator loss 0.9348449110984802\n",
      "Generator loss 3.3141298294067383\n",
      "Discriminator loss 0.7934779524803162\n",
      "Generator loss 3.483731508255005\n",
      "Discriminator loss 0.9196731448173523\n",
      "Generator loss 4.123622894287109\n",
      "Discriminator loss 0.910271167755127\n",
      "Generator loss 3.422368049621582\n",
      "Discriminator loss 0.8778360486030579\n",
      "Generator loss 3.5543177127838135\n",
      "Discriminator loss 0.8926776647567749\n",
      "Generator loss 3.456923723220825\n",
      "Discriminator loss 0.8450014591217041\n",
      "Generator loss 3.475306749343872\n",
      "Discriminator loss 0.8030656576156616\n",
      "Generator loss 3.5512502193450928\n",
      "Discriminator loss 0.9894108176231384\n",
      "Generator loss 3.4638824462890625\n",
      "Epoch loss\n",
      "Discriminator loss 0.8155893087387085\n",
      "Generator loss 3.8927645683288574\n",
      "Discriminator loss 0.9034544229507446\n",
      "Generator loss 3.7343454360961914\n",
      "Discriminator loss 0.8658409714698792\n",
      "Generator loss 3.6035404205322266\n",
      "Discriminator loss 0.8738047480583191\n",
      "Generator loss 3.8347339630126953\n",
      "Discriminator loss 0.8970794677734375\n",
      "Generator loss 3.531529426574707\n",
      "Discriminator loss 0.7507683634757996\n",
      "Generator loss 4.024960994720459\n",
      "Discriminator loss 0.8929241299629211\n",
      "Generator loss 3.8595423698425293\n",
      "Discriminator loss 0.8445509076118469\n",
      "Generator loss 3.538433313369751\n",
      "Discriminator loss 0.802586019039154\n",
      "Generator loss 3.7433888912200928\n",
      "Discriminator loss 0.8983904123306274\n",
      "Generator loss 3.334688425064087\n",
      "Discriminator loss 0.7565912008285522\n",
      "Generator loss 3.505354166030884\n",
      "Discriminator loss 0.9557084441184998\n",
      "Generator loss 3.834383964538574\n",
      "Discriminator loss 0.8782308101654053\n",
      "Generator loss 4.094898223876953\n",
      "Discriminator loss 0.849612832069397\n",
      "Generator loss 3.7049760818481445\n",
      "Discriminator loss 0.8953548669815063\n",
      "Generator loss 3.6206934452056885\n",
      "Discriminator loss 0.7645557522773743\n",
      "Generator loss 3.789716958999634\n",
      "Discriminator loss 0.9576616883277893\n",
      "Generator loss 3.7580721378326416\n",
      "Discriminator loss 0.8631947040557861\n",
      "Generator loss 3.87680721282959\n",
      "Discriminator loss 0.7439800500869751\n",
      "Generator loss 3.550328493118286\n",
      "Discriminator loss 0.8228105306625366\n",
      "Generator loss 3.7833900451660156\n",
      "Discriminator loss 0.8548269867897034\n",
      "Generator loss 3.4997899532318115\n",
      "Discriminator loss 0.9209520220756531\n",
      "Generator loss 3.744246482849121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.9086679220199585\n",
      "Generator loss 3.436648368835449\n",
      "Discriminator loss 0.8684447407722473\n",
      "Generator loss 4.202897071838379\n",
      "Discriminator loss 0.9020928144454956\n",
      "Generator loss 3.2396562099456787\n",
      "Epoch loss\n",
      "Discriminator loss 0.9445146322250366\n",
      "Generator loss 3.7503585815429688\n",
      "Discriminator loss 0.8163470029830933\n",
      "Generator loss 3.6431562900543213\n",
      "Discriminator loss 0.9309095144271851\n",
      "Generator loss 3.921928882598877\n",
      "Discriminator loss 0.8102192878723145\n",
      "Generator loss 3.6953206062316895\n",
      "Discriminator loss 0.8194392323493958\n",
      "Generator loss 3.7871592044830322\n",
      "Discriminator loss 0.8050995469093323\n",
      "Generator loss 3.7039029598236084\n",
      "Discriminator loss 0.8209240436553955\n",
      "Generator loss 3.395397424697876\n",
      "Discriminator loss 0.8956291675567627\n",
      "Generator loss 3.4245593547821045\n",
      "Discriminator loss 0.7991099953651428\n",
      "Generator loss 3.9707794189453125\n",
      "Discriminator loss 0.9596377611160278\n",
      "Generator loss 3.75978422164917\n",
      "Discriminator loss 0.8568439483642578\n",
      "Generator loss 3.8657474517822266\n",
      "Discriminator loss 0.8876162767410278\n",
      "Generator loss 3.6495070457458496\n",
      "Discriminator loss 0.9419834613800049\n",
      "Generator loss 4.0516180992126465\n",
      "Discriminator loss 0.8748424053192139\n",
      "Generator loss 3.552380084991455\n",
      "Discriminator loss 0.9731664061546326\n",
      "Generator loss 3.091087579727173\n",
      "Discriminator loss 0.8458210229873657\n",
      "Generator loss 3.40083909034729\n",
      "Discriminator loss 0.8506823182106018\n",
      "Generator loss 3.748170852661133\n",
      "Discriminator loss 0.8381274938583374\n",
      "Generator loss 3.3931937217712402\n",
      "Discriminator loss 0.7909088134765625\n",
      "Generator loss 3.934603691101074\n",
      "Discriminator loss 0.8138446807861328\n",
      "Generator loss 3.658482313156128\n",
      "Discriminator loss 0.8791545629501343\n",
      "Generator loss 3.3232932090759277\n",
      "Discriminator loss 0.9867154359817505\n",
      "Generator loss 3.436105251312256\n",
      "Discriminator loss 0.9212040901184082\n",
      "Generator loss 3.9144887924194336\n",
      "Discriminator loss 0.8826259970664978\n",
      "Generator loss 3.771026611328125\n",
      "Discriminator loss 0.9106169939041138\n",
      "Generator loss 3.407602310180664\n",
      "Epoch loss\n",
      "Discriminator loss 0.9168983101844788\n",
      "Generator loss 3.8061437606811523\n",
      "Discriminator loss 0.8128045797348022\n",
      "Generator loss 3.5011301040649414\n",
      "Discriminator loss 0.888029932975769\n",
      "Generator loss 3.6284120082855225\n",
      "Discriminator loss 0.9094427824020386\n",
      "Generator loss 3.4724855422973633\n",
      "Discriminator loss 0.8863527178764343\n",
      "Generator loss 3.429093599319458\n",
      "Discriminator loss 0.9070165157318115\n",
      "Generator loss 3.447317123413086\n",
      "Discriminator loss 0.9296517372131348\n",
      "Generator loss 4.165422439575195\n",
      "Discriminator loss 0.9585801362991333\n",
      "Generator loss 3.840489149093628\n",
      "Discriminator loss 0.8593467473983765\n",
      "Generator loss 3.5715460777282715\n",
      "Discriminator loss 0.9886999726295471\n",
      "Generator loss 3.7676198482513428\n",
      "Discriminator loss 1.0468047857284546\n",
      "Generator loss 4.698591232299805\n",
      "Discriminator loss 0.8737009763717651\n",
      "Generator loss 3.2809934616088867\n",
      "Discriminator loss 0.8142005801200867\n",
      "Generator loss 3.2709524631500244\n",
      "Discriminator loss 0.8550388813018799\n",
      "Generator loss 3.4569084644317627\n",
      "Discriminator loss 0.832947850227356\n",
      "Generator loss 3.280803918838501\n",
      "Discriminator loss 0.7975399494171143\n",
      "Generator loss 3.644531488418579\n",
      "Discriminator loss 0.8022333383560181\n",
      "Generator loss 3.8478176593780518\n",
      "Discriminator loss 0.837065577507019\n",
      "Generator loss 3.3327205181121826\n",
      "Discriminator loss 0.7744227051734924\n",
      "Generator loss 3.7400503158569336\n",
      "Discriminator loss 0.8942472338676453\n",
      "Generator loss 3.7339398860931396\n",
      "Discriminator loss 0.8282743692398071\n",
      "Generator loss 3.831195592880249\n",
      "Discriminator loss 0.9498857259750366\n",
      "Generator loss 3.5687437057495117\n",
      "Discriminator loss 0.9030712246894836\n",
      "Generator loss 3.5067853927612305\n",
      "Discriminator loss 0.761865496635437\n",
      "Generator loss 3.218980312347412\n",
      "Discriminator loss 0.8214178085327148\n",
      "Generator loss 3.3968281745910645\n",
      "Epoch loss\n",
      "Discriminator loss 0.9931277632713318\n",
      "Generator loss 3.853306293487549\n",
      "Discriminator loss 0.8580728769302368\n",
      "Generator loss 3.233677864074707\n",
      "Discriminator loss 0.821172833442688\n",
      "Generator loss 3.636495351791382\n",
      "Discriminator loss 0.7938641905784607\n",
      "Generator loss 3.521770477294922\n",
      "Discriminator loss 0.8491232395172119\n",
      "Generator loss 3.6302685737609863\n",
      "Discriminator loss 0.8875699639320374\n",
      "Generator loss 3.774684190750122\n",
      "Discriminator loss 0.8487135171890259\n",
      "Generator loss 3.5279102325439453\n",
      "Discriminator loss 0.810180127620697\n",
      "Generator loss 3.6611239910125732\n",
      "Discriminator loss 0.7903059720993042\n",
      "Generator loss 3.5080466270446777\n",
      "Discriminator loss 0.8600847721099854\n",
      "Generator loss 3.598635673522949\n",
      "Discriminator loss 0.9064506888389587\n",
      "Generator loss 3.4138693809509277\n",
      "Discriminator loss 0.9714869856834412\n",
      "Generator loss 3.7429652214050293\n",
      "Discriminator loss 0.9250249862670898\n",
      "Generator loss 3.3157804012298584\n",
      "Discriminator loss 0.779249906539917\n",
      "Generator loss 3.5487265586853027\n",
      "Discriminator loss 0.8191603422164917\n",
      "Generator loss 3.6506760120391846\n",
      "Discriminator loss 0.8743333220481873\n",
      "Generator loss 3.3388280868530273\n",
      "Discriminator loss 0.8987107872962952\n",
      "Generator loss 3.7104876041412354\n",
      "Discriminator loss 0.8928046822547913\n",
      "Generator loss 3.7495217323303223\n",
      "Discriminator loss 0.9753321409225464\n",
      "Generator loss 3.7439463138580322\n",
      "Discriminator loss 0.9087874889373779\n",
      "Generator loss 4.009655475616455\n",
      "Discriminator loss 0.8023645877838135\n",
      "Generator loss 3.7133073806762695\n",
      "Discriminator loss 0.9334442615509033\n",
      "Generator loss 3.4849789142608643\n",
      "Discriminator loss 0.8239914178848267\n",
      "Generator loss 3.79826283454895\n",
      "Discriminator loss 0.7705202102661133\n",
      "Generator loss 3.73189640045166\n",
      "Discriminator loss 0.9895654320716858\n",
      "Generator loss 3.8273673057556152\n",
      "Epoch loss\n",
      "Discriminator loss 0.9683099389076233\n",
      "Generator loss 3.6144514083862305\n",
      "Discriminator loss 0.917779266834259\n",
      "Generator loss 4.209698677062988\n",
      "Discriminator loss 0.8955684304237366\n",
      "Generator loss 3.638068437576294\n",
      "Discriminator loss 0.9163727164268494\n",
      "Generator loss 3.4801738262176514\n",
      "Discriminator loss 0.8449708223342896\n",
      "Generator loss 3.7288687229156494\n",
      "Discriminator loss 0.8572766184806824\n",
      "Generator loss 3.6437337398529053\n",
      "Discriminator loss 0.8267900943756104\n",
      "Generator loss 3.3698623180389404\n",
      "Discriminator loss 0.890343189239502\n",
      "Generator loss 3.6558279991149902\n",
      "Discriminator loss 0.9039665460586548\n",
      "Generator loss 3.751797914505005\n",
      "Discriminator loss 0.8759668469429016\n",
      "Generator loss 3.5280532836914062\n",
      "Discriminator loss 0.9768156409263611\n",
      "Generator loss 3.400061845779419\n",
      "Discriminator loss 0.8646931052207947\n",
      "Generator loss 3.4038164615631104\n",
      "Discriminator loss 0.9011775255203247\n",
      "Generator loss 3.562863349914551\n",
      "Discriminator loss 0.9873058199882507\n",
      "Generator loss 3.9727158546447754\n",
      "Discriminator loss 0.8486108183860779\n",
      "Generator loss 3.4307141304016113\n",
      "Discriminator loss 0.8453158140182495\n",
      "Generator loss 3.529097557067871\n",
      "Discriminator loss 0.8756593465805054\n",
      "Generator loss 4.129571437835693\n",
      "Discriminator loss 0.82142174243927\n",
      "Generator loss 3.6267051696777344\n",
      "Discriminator loss 0.8676443696022034\n",
      "Generator loss 3.847568988800049\n",
      "Discriminator loss 0.9067898392677307\n",
      "Generator loss 3.7217164039611816\n",
      "Discriminator loss 0.8451613187789917\n",
      "Generator loss 3.8679845333099365\n",
      "Discriminator loss 0.7913405895233154\n",
      "Generator loss 3.9177072048187256\n",
      "Discriminator loss 1.0023431777954102\n",
      "Generator loss 3.8173062801361084\n",
      "Discriminator loss 0.9015520811080933\n",
      "Generator loss 3.505476951599121\n",
      "Discriminator loss 0.7963109016418457\n",
      "Generator loss 3.9811673164367676\n",
      "Epoch loss\n",
      "Discriminator loss 0.8334650993347168\n",
      "Generator loss 3.672804117202759\n",
      "Discriminator loss 0.8558506965637207\n",
      "Generator loss 3.9122519493103027\n",
      "Discriminator loss 0.924257755279541\n",
      "Generator loss 3.621723175048828\n",
      "Discriminator loss 0.8462615013122559\n",
      "Generator loss 3.6806952953338623\n",
      "Discriminator loss 0.8746779561042786\n",
      "Generator loss 3.9483232498168945\n",
      "Discriminator loss 0.80921870470047\n",
      "Generator loss 3.3522908687591553\n",
      "Discriminator loss 0.8504767417907715\n",
      "Generator loss 3.4509310722351074\n",
      "Discriminator loss 0.9091582298278809\n",
      "Generator loss 4.035202503204346\n",
      "Discriminator loss 0.9592906832695007\n",
      "Generator loss 3.6976518630981445\n",
      "Discriminator loss 0.8503835201263428\n",
      "Generator loss 3.696833372116089\n",
      "Discriminator loss 0.8062366247177124\n",
      "Generator loss 3.549464702606201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.857810914516449\n",
      "Generator loss 3.850433349609375\n",
      "Discriminator loss 0.9287876486778259\n",
      "Generator loss 3.3704822063446045\n",
      "Discriminator loss 1.0227147340774536\n",
      "Generator loss 3.466710090637207\n",
      "Discriminator loss 0.8151729702949524\n",
      "Generator loss 3.791738271713257\n",
      "Discriminator loss 0.8177609443664551\n",
      "Generator loss 3.660410165786743\n",
      "Discriminator loss 0.8580186367034912\n",
      "Generator loss 3.9988112449645996\n",
      "Discriminator loss 0.8826937675476074\n",
      "Generator loss 3.3496763706207275\n",
      "Discriminator loss 0.9572956562042236\n",
      "Generator loss 3.9882144927978516\n",
      "Discriminator loss 0.9408408403396606\n",
      "Generator loss 3.157540798187256\n",
      "Discriminator loss 0.852774977684021\n",
      "Generator loss 3.7402536869049072\n",
      "Discriminator loss 0.8357288241386414\n",
      "Generator loss 3.5046777725219727\n",
      "Discriminator loss 1.0200018882751465\n",
      "Generator loss 3.4604716300964355\n",
      "Discriminator loss 0.7797439098358154\n",
      "Generator loss 3.7005257606506348\n",
      "Discriminator loss 0.9454212784767151\n",
      "Generator loss 3.7319581508636475\n",
      "Epoch loss\n",
      "Discriminator loss 0.7912830710411072\n",
      "Generator loss 3.354771375656128\n",
      "Discriminator loss 0.7711065411567688\n",
      "Generator loss 3.5320661067962646\n",
      "Discriminator loss 0.8384917974472046\n",
      "Generator loss 3.5271875858306885\n",
      "Discriminator loss 0.9269133806228638\n",
      "Generator loss 3.386723756790161\n",
      "Discriminator loss 0.7769599556922913\n",
      "Generator loss 3.718816041946411\n",
      "Discriminator loss 0.9549567103385925\n",
      "Generator loss 3.425994634628296\n",
      "Discriminator loss 0.8473680019378662\n",
      "Generator loss 3.616368532180786\n",
      "Discriminator loss 0.8448702096939087\n",
      "Generator loss 3.5708298683166504\n",
      "Discriminator loss 0.7993160486221313\n",
      "Generator loss 3.693978786468506\n",
      "Discriminator loss 0.8877600431442261\n",
      "Generator loss 3.300309181213379\n",
      "Discriminator loss 0.8448219895362854\n",
      "Generator loss 3.681501865386963\n",
      "Discriminator loss 0.8122125267982483\n",
      "Generator loss 3.6399872303009033\n",
      "Discriminator loss 0.8225292563438416\n",
      "Generator loss 3.808560848236084\n",
      "Discriminator loss 0.9625060558319092\n",
      "Generator loss 3.7463321685791016\n",
      "Discriminator loss 0.8700957298278809\n",
      "Generator loss 3.2388129234313965\n",
      "Discriminator loss 0.8662277460098267\n",
      "Generator loss 3.400792360305786\n",
      "Discriminator loss 0.831991970539093\n",
      "Generator loss 3.04665470123291\n",
      "Discriminator loss 0.821233868598938\n",
      "Generator loss 3.65305757522583\n",
      "Discriminator loss 0.8290873765945435\n",
      "Generator loss 3.7041678428649902\n",
      "Discriminator loss 0.8681240677833557\n",
      "Generator loss 3.9033145904541016\n",
      "Discriminator loss 0.8641908168792725\n",
      "Generator loss 3.5630006790161133\n",
      "Discriminator loss 0.9220210909843445\n",
      "Generator loss 3.9994730949401855\n",
      "Discriminator loss 0.867222785949707\n",
      "Generator loss 3.807769775390625\n",
      "Discriminator loss 0.8809962868690491\n",
      "Generator loss 3.914196014404297\n",
      "Discriminator loss 0.9822149872779846\n",
      "Generator loss 3.2065656185150146\n",
      "Epoch loss\n",
      "Discriminator loss 0.940731406211853\n",
      "Generator loss 3.478863477706909\n",
      "Discriminator loss 0.8482475280761719\n",
      "Generator loss 3.507199287414551\n",
      "Discriminator loss 0.9811573624610901\n",
      "Generator loss 3.6402509212493896\n",
      "Discriminator loss 0.9036489129066467\n",
      "Generator loss 3.6945080757141113\n",
      "Discriminator loss 0.896648108959198\n",
      "Generator loss 3.7513444423675537\n",
      "Discriminator loss 0.9628962278366089\n",
      "Generator loss 3.80938720703125\n",
      "Discriminator loss 0.9248045086860657\n",
      "Generator loss 3.5429258346557617\n",
      "Discriminator loss 0.8024406433105469\n",
      "Generator loss 3.6101582050323486\n",
      "Discriminator loss 0.9233769178390503\n",
      "Generator loss 3.3317837715148926\n",
      "Discriminator loss 0.9171128273010254\n",
      "Generator loss 3.777240514755249\n",
      "Discriminator loss 0.8853288292884827\n",
      "Generator loss 3.6085569858551025\n",
      "Discriminator loss 0.800301194190979\n",
      "Generator loss 3.4138505458831787\n",
      "Discriminator loss 0.8988023400306702\n",
      "Generator loss 3.6831746101379395\n",
      "Discriminator loss 0.9495874643325806\n",
      "Generator loss 3.6531169414520264\n",
      "Discriminator loss 0.9017936587333679\n",
      "Generator loss 3.9107189178466797\n",
      "Discriminator loss 0.8940012454986572\n",
      "Generator loss 3.74273419380188\n",
      "Discriminator loss 0.882707953453064\n",
      "Generator loss 3.490119457244873\n",
      "Discriminator loss 0.9011958837509155\n",
      "Generator loss 3.6078414916992188\n",
      "Discriminator loss 0.938681423664093\n",
      "Generator loss 3.694110631942749\n",
      "Discriminator loss 0.850775957107544\n",
      "Generator loss 3.4971258640289307\n",
      "Discriminator loss 0.793883740901947\n",
      "Generator loss 3.531101942062378\n",
      "Discriminator loss 0.8022921085357666\n",
      "Generator loss 3.591935634613037\n",
      "Discriminator loss 0.9943031668663025\n",
      "Generator loss 3.6116855144500732\n",
      "Discriminator loss 0.8900362849235535\n",
      "Generator loss 3.5332584381103516\n",
      "Discriminator loss 0.8439145684242249\n",
      "Generator loss 3.4833147525787354\n",
      "Epoch loss\n",
      "Discriminator loss 0.8758498430252075\n",
      "Generator loss 3.675509214401245\n",
      "Discriminator loss 0.8758398294448853\n",
      "Generator loss 3.51971173286438\n",
      "Discriminator loss 0.7837720513343811\n",
      "Generator loss 3.6277408599853516\n",
      "Discriminator loss 0.9287657737731934\n",
      "Generator loss 3.215649127960205\n",
      "Discriminator loss 0.754855751991272\n",
      "Generator loss 3.2780728340148926\n",
      "Discriminator loss 0.843687117099762\n",
      "Generator loss 3.944427967071533\n",
      "Discriminator loss 0.8546633124351501\n",
      "Generator loss 3.5763349533081055\n",
      "Discriminator loss 0.8564996123313904\n",
      "Generator loss 3.8177173137664795\n",
      "Discriminator loss 0.8323875665664673\n",
      "Generator loss 3.755014657974243\n",
      "Discriminator loss 0.814437985420227\n",
      "Generator loss 3.4176924228668213\n",
      "Discriminator loss 0.9518608450889587\n",
      "Generator loss 3.727994680404663\n",
      "Discriminator loss 0.896040141582489\n",
      "Generator loss 3.3965530395507812\n",
      "Discriminator loss 0.8847053050994873\n",
      "Generator loss 3.3187291622161865\n",
      "Discriminator loss 0.9127244353294373\n",
      "Generator loss 3.562718391418457\n",
      "Discriminator loss 0.973160982131958\n",
      "Generator loss 3.7173831462860107\n",
      "Discriminator loss 0.8866445422172546\n",
      "Generator loss 3.733083724975586\n",
      "Discriminator loss 0.8587324619293213\n",
      "Generator loss 3.7639636993408203\n",
      "Discriminator loss 0.8602530360221863\n",
      "Generator loss 3.821467876434326\n",
      "Discriminator loss 0.8954939842224121\n",
      "Generator loss 3.863612413406372\n",
      "Discriminator loss 0.8204853534698486\n",
      "Generator loss 3.5367965698242188\n",
      "Discriminator loss 0.7905818819999695\n",
      "Generator loss 3.7210614681243896\n",
      "Discriminator loss 0.7944473028182983\n",
      "Generator loss 3.3589882850646973\n",
      "Discriminator loss 0.8842361569404602\n",
      "Generator loss 3.597614288330078\n",
      "Discriminator loss 0.8373595476150513\n",
      "Generator loss 4.160713195800781\n",
      "Discriminator loss 0.81965172290802\n",
      "Generator loss 3.7980191707611084\n",
      "Epoch loss\n",
      "Discriminator loss 0.9087192416191101\n",
      "Generator loss 4.059057712554932\n",
      "Discriminator loss 0.9364375472068787\n",
      "Generator loss 3.829681396484375\n",
      "Discriminator loss 0.8475307822227478\n",
      "Generator loss 3.355471611022949\n",
      "Discriminator loss 0.8151029944419861\n",
      "Generator loss 3.4664742946624756\n",
      "Discriminator loss 0.9373456835746765\n",
      "Generator loss 3.7382616996765137\n",
      "Discriminator loss 0.8672559261322021\n",
      "Generator loss 4.059912204742432\n",
      "Discriminator loss 1.0029469728469849\n",
      "Generator loss 3.62371826171875\n",
      "Discriminator loss 0.8645124435424805\n",
      "Generator loss 3.426051139831543\n",
      "Discriminator loss 0.7938350439071655\n",
      "Generator loss 3.8234493732452393\n",
      "Discriminator loss 0.7604840993881226\n",
      "Generator loss 3.740205764770508\n",
      "Discriminator loss 0.9582570791244507\n",
      "Generator loss 3.9344122409820557\n",
      "Discriminator loss 0.8634336590766907\n",
      "Generator loss 3.50095534324646\n",
      "Discriminator loss 0.8733131289482117\n",
      "Generator loss 3.044844150543213\n",
      "Discriminator loss 0.8701671957969666\n",
      "Generator loss 4.029473304748535\n",
      "Discriminator loss 0.8314434289932251\n",
      "Generator loss 3.4524638652801514\n",
      "Discriminator loss 0.8668356537818909\n",
      "Generator loss 3.8731560707092285\n",
      "Discriminator loss 0.8612203001976013\n",
      "Generator loss 3.868584394454956\n",
      "Discriminator loss 0.7669692039489746\n",
      "Generator loss 3.3654346466064453\n",
      "Discriminator loss 0.8673751950263977\n",
      "Generator loss 3.6844427585601807\n",
      "Discriminator loss 0.9873723983764648\n",
      "Generator loss 3.312953472137451\n",
      "Discriminator loss 0.857681155204773\n",
      "Generator loss 3.5791592597961426\n",
      "Discriminator loss 0.9535456895828247\n",
      "Generator loss 3.0606296062469482\n",
      "Discriminator loss 0.883065402507782\n",
      "Generator loss 3.8108673095703125\n",
      "Discriminator loss 0.7924842238426208\n",
      "Generator loss 3.4058732986450195\n",
      "Discriminator loss 0.7908485531806946\n",
      "Generator loss 3.501284599304199\n",
      "Epoch loss\n",
      "Discriminator loss 0.8581447601318359\n",
      "Generator loss 3.3876373767852783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.7817903757095337\n",
      "Generator loss 3.1059775352478027\n",
      "Discriminator loss 0.9498932957649231\n",
      "Generator loss 3.220399856567383\n",
      "Discriminator loss 0.8436955809593201\n",
      "Generator loss 3.4448275566101074\n",
      "Discriminator loss 0.8454887270927429\n",
      "Generator loss 3.9549474716186523\n",
      "Discriminator loss 0.8181881904602051\n",
      "Generator loss 3.767639398574829\n",
      "Discriminator loss 1.01932692527771\n",
      "Generator loss 3.763336181640625\n",
      "Discriminator loss 0.9430885910987854\n",
      "Generator loss 3.4750938415527344\n",
      "Discriminator loss 0.8710307478904724\n",
      "Generator loss 3.6152031421661377\n",
      "Discriminator loss 0.8412551283836365\n",
      "Generator loss 3.664475440979004\n",
      "Discriminator loss 0.8751433491706848\n",
      "Generator loss 3.80672025680542\n",
      "Discriminator loss 0.7742176055908203\n",
      "Generator loss 3.3419322967529297\n",
      "Discriminator loss 0.8107485771179199\n",
      "Generator loss 3.375361442565918\n",
      "Discriminator loss 0.9992763996124268\n",
      "Generator loss 3.203805685043335\n",
      "Discriminator loss 0.9469049572944641\n",
      "Generator loss 3.456881523132324\n",
      "Discriminator loss 0.8539457321166992\n",
      "Generator loss 3.9238357543945312\n",
      "Discriminator loss 0.8455253839492798\n",
      "Generator loss 3.849900245666504\n",
      "Discriminator loss 0.9016833305358887\n",
      "Generator loss 3.6384496688842773\n",
      "Discriminator loss 0.9130271077156067\n",
      "Generator loss 3.587913990020752\n",
      "Discriminator loss 0.819115400314331\n",
      "Generator loss 3.452244520187378\n",
      "Discriminator loss 0.8961391448974609\n",
      "Generator loss 3.6099233627319336\n",
      "Discriminator loss 0.8274316787719727\n",
      "Generator loss 3.9942946434020996\n",
      "Discriminator loss 0.9283323287963867\n",
      "Generator loss 3.5106747150421143\n",
      "Discriminator loss 0.8106005787849426\n",
      "Generator loss 3.3897130489349365\n",
      "Discriminator loss 0.8049103617668152\n",
      "Generator loss 3.486677646636963\n",
      "Epoch loss\n",
      "Discriminator loss 0.8162169456481934\n",
      "Generator loss 4.180237293243408\n",
      "Discriminator loss 0.8829338550567627\n",
      "Generator loss 3.5326125621795654\n",
      "Discriminator loss 0.838658332824707\n",
      "Generator loss 3.4643774032592773\n",
      "Discriminator loss 0.8246532082557678\n",
      "Generator loss 3.418999671936035\n",
      "Discriminator loss 0.8334843516349792\n",
      "Generator loss 3.433560848236084\n",
      "Discriminator loss 0.8258168697357178\n",
      "Generator loss 3.4524776935577393\n",
      "Discriminator loss 0.8577346801757812\n",
      "Generator loss 3.622497081756592\n",
      "Discriminator loss 0.8321436643600464\n",
      "Generator loss 3.714907169342041\n",
      "Discriminator loss 0.844982385635376\n",
      "Generator loss 3.5103955268859863\n",
      "Discriminator loss 0.8889762163162231\n",
      "Generator loss 3.8621346950531006\n",
      "Discriminator loss 0.8879992365837097\n",
      "Generator loss 3.4750287532806396\n",
      "Discriminator loss 1.052769660949707\n",
      "Generator loss 3.4743776321411133\n",
      "Discriminator loss 0.7856321334838867\n",
      "Generator loss 3.6871461868286133\n",
      "Discriminator loss 0.8813459277153015\n",
      "Generator loss 3.4573616981506348\n",
      "Discriminator loss 0.7761983871459961\n",
      "Generator loss 3.6869940757751465\n",
      "Discriminator loss 0.8386526703834534\n",
      "Generator loss 3.5331788063049316\n",
      "Discriminator loss 0.9178817868232727\n",
      "Generator loss 3.9746556282043457\n",
      "Discriminator loss 0.7901135683059692\n",
      "Generator loss 3.761651039123535\n",
      "Discriminator loss 0.9460853338241577\n",
      "Generator loss 3.7725963592529297\n",
      "Discriminator loss 0.8276764154434204\n",
      "Generator loss 3.6330339908599854\n",
      "Discriminator loss 0.8215506672859192\n",
      "Generator loss 3.336164951324463\n",
      "Discriminator loss 0.8651177883148193\n",
      "Generator loss 3.9129414558410645\n",
      "Discriminator loss 0.8522950410842896\n",
      "Generator loss 3.961789131164551\n",
      "Discriminator loss 0.9317767024040222\n",
      "Generator loss 3.761035919189453\n",
      "Discriminator loss 0.869369626045227\n",
      "Generator loss 4.110486030578613\n",
      "Epoch loss\n",
      "Discriminator loss 0.8953942060470581\n",
      "Generator loss 3.6016130447387695\n",
      "Discriminator loss 0.8350638151168823\n",
      "Generator loss 3.4763381481170654\n",
      "Discriminator loss 0.7314566373825073\n",
      "Generator loss 4.1719207763671875\n",
      "Discriminator loss 0.8197753429412842\n",
      "Generator loss 3.522669553756714\n",
      "Discriminator loss 0.7627547383308411\n",
      "Generator loss 3.585203170776367\n",
      "Discriminator loss 0.9805392622947693\n",
      "Generator loss 3.773998498916626\n",
      "Discriminator loss 0.8808994889259338\n",
      "Generator loss 3.387985944747925\n",
      "Discriminator loss 0.9306281208992004\n",
      "Generator loss 3.2372865676879883\n",
      "Discriminator loss 0.8368145823478699\n",
      "Generator loss 3.8315887451171875\n",
      "Discriminator loss 0.9434714913368225\n",
      "Generator loss 4.118824005126953\n",
      "Discriminator loss 0.8333783149719238\n",
      "Generator loss 4.048879146575928\n",
      "Discriminator loss 0.9613678455352783\n",
      "Generator loss 3.4106411933898926\n",
      "Discriminator loss 0.869596004486084\n",
      "Generator loss 3.685262441635132\n",
      "Discriminator loss 0.9212933778762817\n",
      "Generator loss 3.2444753646850586\n",
      "Discriminator loss 0.9541645050048828\n",
      "Generator loss 3.4793858528137207\n",
      "Discriminator loss 0.880154550075531\n",
      "Generator loss 3.870159864425659\n",
      "Discriminator loss 0.8537553548812866\n",
      "Generator loss 3.5904483795166016\n",
      "Discriminator loss 0.9274440407752991\n",
      "Generator loss 3.597306728363037\n",
      "Discriminator loss 0.8910222053527832\n",
      "Generator loss 4.0745625495910645\n",
      "Discriminator loss 0.7842926979064941\n",
      "Generator loss 3.4668221473693848\n",
      "Discriminator loss 0.8598331809043884\n",
      "Generator loss 3.7190349102020264\n",
      "Discriminator loss 0.841701090335846\n",
      "Generator loss 3.5574710369110107\n",
      "Discriminator loss 0.842017650604248\n",
      "Generator loss 3.466383934020996\n",
      "Discriminator loss 0.8896100521087646\n",
      "Generator loss 3.789961576461792\n",
      "Discriminator loss 0.9016223549842834\n",
      "Generator loss 3.5575218200683594\n",
      "Epoch loss\n",
      "Discriminator loss 0.8721542358398438\n",
      "Generator loss 3.476101875305176\n",
      "Discriminator loss 0.8540320992469788\n",
      "Generator loss 3.3772881031036377\n",
      "Discriminator loss 0.803254246711731\n",
      "Generator loss 4.02572774887085\n",
      "Discriminator loss 0.9856669902801514\n",
      "Generator loss 3.635188579559326\n",
      "Discriminator loss 0.8181921243667603\n",
      "Generator loss 3.5120749473571777\n",
      "Discriminator loss 0.909834086894989\n",
      "Generator loss 4.040203094482422\n",
      "Discriminator loss 0.8024559020996094\n",
      "Generator loss 3.6445744037628174\n",
      "Discriminator loss 0.9468411803245544\n",
      "Generator loss 3.987920045852661\n",
      "Discriminator loss 0.8164314031600952\n",
      "Generator loss 3.6069679260253906\n",
      "Discriminator loss 0.7571671009063721\n",
      "Generator loss 3.7597289085388184\n",
      "Discriminator loss 0.7659297585487366\n",
      "Generator loss 4.0032196044921875\n",
      "Discriminator loss 0.8658431768417358\n",
      "Generator loss 3.546837568283081\n",
      "Discriminator loss 0.8687836527824402\n",
      "Generator loss 3.7692692279815674\n",
      "Discriminator loss 0.9119350910186768\n",
      "Generator loss 3.2153239250183105\n",
      "Discriminator loss 0.8456835150718689\n",
      "Generator loss 3.7354989051818848\n",
      "Discriminator loss 0.8180602788925171\n",
      "Generator loss 3.957754135131836\n",
      "Discriminator loss 0.8685346245765686\n",
      "Generator loss 3.9575037956237793\n",
      "Discriminator loss 0.8943273425102234\n",
      "Generator loss 3.7509279251098633\n",
      "Discriminator loss 0.9000322818756104\n",
      "Generator loss 3.9152944087982178\n",
      "Discriminator loss 0.9506956934928894\n",
      "Generator loss 3.659482479095459\n",
      "Discriminator loss 0.7646708488464355\n",
      "Generator loss 3.623213291168213\n",
      "Discriminator loss 0.8882187604904175\n",
      "Generator loss 3.5465011596679688\n",
      "Discriminator loss 0.8523883819580078\n",
      "Generator loss 3.4823875427246094\n",
      "Discriminator loss 0.8793492317199707\n",
      "Generator loss 3.0225181579589844\n",
      "Discriminator loss 0.9031121730804443\n",
      "Generator loss 3.425679922103882\n",
      "Epoch loss\n",
      "Discriminator loss 0.8639867901802063\n",
      "Generator loss 3.8127636909484863\n",
      "Discriminator loss 0.9335423707962036\n",
      "Generator loss 4.019920825958252\n",
      "Discriminator loss 0.9232468605041504\n",
      "Generator loss 3.4492979049682617\n",
      "Discriminator loss 0.9729779958724976\n",
      "Generator loss 3.8158609867095947\n",
      "Discriminator loss 0.8251356482505798\n",
      "Generator loss 3.6963300704956055\n",
      "Discriminator loss 0.8492175340652466\n",
      "Generator loss 3.4125213623046875\n",
      "Discriminator loss 0.8690140247344971\n",
      "Generator loss 3.5559113025665283\n",
      "Discriminator loss 0.9340835213661194\n",
      "Generator loss 3.2941231727600098\n",
      "Discriminator loss 0.8696141839027405\n",
      "Generator loss 3.4761595726013184\n",
      "Discriminator loss 0.8860944509506226\n",
      "Generator loss 3.2669081687927246\n",
      "Discriminator loss 0.952266275882721\n",
      "Generator loss 3.7176525592803955\n",
      "Discriminator loss 0.9198593497276306\n",
      "Generator loss 3.2975761890411377\n",
      "Discriminator loss 0.9535314440727234\n",
      "Generator loss 3.6643147468566895\n",
      "Discriminator loss 0.853901207447052\n",
      "Generator loss 3.4443957805633545\n",
      "Discriminator loss 0.9573522806167603\n",
      "Generator loss 3.0173845291137695\n",
      "Discriminator loss 0.9773770570755005\n",
      "Generator loss 3.6406898498535156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss 0.8373547792434692\n",
      "Generator loss 3.4243569374084473\n",
      "Discriminator loss 0.8357905149459839\n",
      "Generator loss 3.7512664794921875\n",
      "Discriminator loss 0.8808266520500183\n",
      "Generator loss 3.3884544372558594\n",
      "Discriminator loss 0.8839499354362488\n",
      "Generator loss 3.124614953994751\n",
      "Discriminator loss 0.8496092557907104\n",
      "Generator loss 3.742976188659668\n",
      "Discriminator loss 0.8608877062797546\n",
      "Generator loss 3.5180888175964355\n",
      "Discriminator loss 0.7743508219718933\n",
      "Generator loss 3.688241958618164\n",
      "Discriminator loss 0.8543340563774109\n",
      "Generator loss 3.5990467071533203\n",
      "Discriminator loss 0.8539146184921265\n",
      "Generator loss 3.9295754432678223\n",
      "Epoch loss\n",
      "Discriminator loss 0.9370839595794678\n",
      "Generator loss 3.5384914875030518\n",
      "Discriminator loss 0.8123048543930054\n",
      "Generator loss 3.1350317001342773\n",
      "Discriminator loss 0.822693943977356\n",
      "Generator loss 3.5967013835906982\n",
      "Discriminator loss 0.7922903299331665\n",
      "Generator loss 3.4974372386932373\n",
      "Discriminator loss 1.016552209854126\n",
      "Generator loss 4.222273349761963\n",
      "Discriminator loss 0.8163912296295166\n",
      "Generator loss 3.5639913082122803\n",
      "Discriminator loss 0.8486895561218262\n",
      "Generator loss 3.69513201713562\n",
      "Discriminator loss 0.8751254081726074\n",
      "Generator loss 3.876983165740967\n",
      "Discriminator loss 0.8353931903839111\n",
      "Generator loss 3.707180976867676\n",
      "Discriminator loss 0.8542950749397278\n",
      "Generator loss 3.7784409523010254\n",
      "Discriminator loss 0.8726514577865601\n",
      "Generator loss 3.9316258430480957\n",
      "Discriminator loss 0.8369151949882507\n",
      "Generator loss 3.3955774307250977\n",
      "Discriminator loss 0.8748072385787964\n",
      "Generator loss 3.668571710586548\n",
      "Discriminator loss 0.8499268889427185\n",
      "Generator loss 3.326801300048828\n",
      "Discriminator loss 0.9183432459831238\n",
      "Generator loss 3.260073184967041\n",
      "Discriminator loss 0.9322353005409241\n",
      "Generator loss 3.477937936782837\n",
      "Discriminator loss 0.8494793176651001\n",
      "Generator loss 3.6072065830230713\n",
      "Discriminator loss 0.8411646485328674\n",
      "Generator loss 3.992323160171509\n",
      "Discriminator loss 0.8526262640953064\n",
      "Generator loss 3.842089891433716\n",
      "Discriminator loss 0.8231869339942932\n",
      "Generator loss 3.541388988494873\n",
      "Discriminator loss 0.8327988982200623\n",
      "Generator loss 3.4988481998443604\n",
      "Discriminator loss 0.8703866600990295\n",
      "Generator loss 3.3834118843078613\n",
      "Discriminator loss 0.8940820097923279\n",
      "Generator loss 3.8301897048950195\n",
      "Discriminator loss 0.7787876725196838\n",
      "Generator loss 4.084940433502197\n",
      "Discriminator loss 0.8451852798461914\n",
      "Generator loss 3.617201328277588\n",
      "Epoch loss\n",
      "Discriminator loss 0.8009256720542908\n",
      "Generator loss 3.4189226627349854\n",
      "Discriminator loss 0.8511685132980347\n",
      "Generator loss 3.23850679397583\n",
      "Discriminator loss 0.8194228410720825\n",
      "Generator loss 3.2956807613372803\n",
      "Discriminator loss 0.8420888185501099\n",
      "Generator loss 3.383502244949341\n",
      "Discriminator loss 0.8287805318832397\n",
      "Generator loss 3.335049629211426\n",
      "Discriminator loss 0.8366504907608032\n",
      "Generator loss 3.681199312210083\n",
      "Discriminator loss 0.8761492371559143\n",
      "Generator loss 3.6052536964416504\n",
      "Discriminator loss 0.9030254483222961\n",
      "Generator loss 3.8522419929504395\n",
      "Discriminator loss 0.8793736696243286\n",
      "Generator loss 3.1746230125427246\n",
      "Discriminator loss 0.8230085968971252\n",
      "Generator loss 3.531745672225952\n",
      "Discriminator loss 0.9631831645965576\n",
      "Generator loss 3.7656078338623047\n",
      "Discriminator loss 0.8274744153022766\n",
      "Generator loss 3.810634136199951\n",
      "Discriminator loss 0.8462889194488525\n",
      "Generator loss 3.733126163482666\n",
      "Discriminator loss 0.8669909834861755\n",
      "Generator loss 3.638094186782837\n",
      "Discriminator loss 0.8023181557655334\n",
      "Generator loss 3.5145983695983887\n",
      "Discriminator loss 0.8392789363861084\n",
      "Generator loss 3.3642258644104004\n",
      "Discriminator loss 1.0115735530853271\n",
      "Generator loss 3.9059834480285645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-491:\n",
      "Process Process-489:\n",
      "Process Process-492:\n",
      "Process Process-490:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/usr/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-db82f4c0e416>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss_D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer_disc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transform_list = [transforms.ToTensor()\n",
    "                  ]\n",
    "\n",
    "transform = transforms.Compose(transform_list)\n",
    "\n",
    "landmarkk_test = Image.open('face_dataset/celeba/Dulat_test.jpg').convert('L')\n",
    "landmarkk_test = landmarkk_test.resize((256, 128), Image.BICUBIC)\n",
    "landmarkk_test = transform(landmarkk_test)\n",
    "landmarkk_test = landmarkk_test.to(device)\n",
    "\n",
    "\n",
    "\n",
    "while epoch<200:\n",
    "    for en, x in enumerate(dataloader):\n",
    "        \n",
    "        image, label_map = x\n",
    "        image = image.to(device)\n",
    "        label_map = label_map.to(device)\n",
    "        \n",
    "        optimizer_gen.zero_grad()\n",
    "        optimizer_disc.zero_grad()  \n",
    "        \n",
    "        loss_D_fake, loss_D_real, loss_GAN, loss_FM, loss_VGG, fake_image = model(label_map, image)\n",
    "        \n",
    "        loss_D = (loss_D_fake + loss_D_real) * 0.5\n",
    "        loss_G = loss_GAN + loss_FM + loss_VGG\n",
    "\n",
    "        loss_G.backward(retain_graph=True)\n",
    "        optimizer_gen.step()\n",
    "\n",
    "        loss_D.backward()\n",
    "        optimizer_disc.step()\n",
    "        \n",
    "        if en%100==0:\n",
    "            print(\"Discriminator loss\", loss_D.item())\n",
    "            print(\"Generator loss\",     loss_G.item())\n",
    "    \n",
    "    img = torchvision.utils.make_grid([image[0].cpu(), label_map[0].repeat(3, 1, 1).cpu(), fake_image[0].cpu()], nrow=3)\n",
    "    save_image(img, filename=('pix2pixHD_outputs/edges/local/'+'fake_'+str(epoch)+'.jpg'))\n",
    "    \n",
    "    \n",
    "    fake_test = model.eval(landmarkk_test.unsqueeze(0))\n",
    "    save_image(fake_test.squeeze(0), filename=('pix2pixHD_outputs/edges/local/'+'fake_test'+str(epoch)+'.jpg'))\n",
    "    \n",
    "    \n",
    "    print(\"Epoch loss\")\n",
    "    epoch += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 128, 256])\n",
      "torch.Size([1, 128, 256])\n"
     ]
    }
   ],
   "source": [
    "transform_list = [transforms.ToTensor()\n",
    "                  ]\n",
    "\n",
    "transform = transforms.Compose(transform_list)\n",
    "\n",
    "landmarkk_test = Image.open('face_dataset/celeba/devushka_test.jpg').convert('L')\n",
    "landmarkk_test = landmarkk_test.resize((256, 128), Image.BICUBIC)\n",
    "landmarkk_test = transform(landmarkk_test)\n",
    "landmarkk_test = landmarkk_test.to(device)\n",
    "\n",
    "\n",
    "fake_test = model.eval(landmarkk_test.unsqueeze(0))\n",
    "print(fake_test.shape)\n",
    "print(landmarkk_test.shape)\n",
    "\n",
    "test_img = torchvision.utils.make_grid([landmarkk_test.repeat(3, 1, 1).cpu(), fake_test.squeeze(0).cpu()], nrow=3)\n",
    "save_image(test_img.squeeze(0), filename=('devushka_fake_test'+str(epoch)+'.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f72732de2e8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/resource_sharer.py\", line 58, in detach\n",
      "    return reduction.recv_handle(conn)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/reduction.py\", line 182, in recv_handle\n",
      "    return recvfds(s, 1)[0]\n",
      "  File \"/usr/lib/python3.6/multiprocessing/reduction.py\", line 153, in recvfds\n",
      "    msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_LEN(bytes_size))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(dataset, batch_size=1,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "for en, test in enumerate(test_dataloader):\n",
    "    image, label_map = test\n",
    "    generator.eval()\n",
    "    real_A = (image).to(device)\n",
    "    label_map = label_map.to(device)\n",
    "    fake_B = generator(label_map)\n",
    "    img = torchvision.utils.make_grid([label_map.squeeze(0), fake_B.squeeze(0)], nrow=2)\n",
    "    save_image(img, filename=('pix2pixHD_outputs/'+'fake_'+str(en)+'.jpg'))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
