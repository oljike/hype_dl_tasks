{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import glob\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.utils.data\n",
    "from torchvision.utils import save_image\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dir_A, dir_B, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "\n",
    "        \"\"\"\n",
    "        self.dir_A = glob.glob(os.path.join(dir_A, '*.jpg'))\n",
    "        self.dir_B = glob.glob(os.path.join(dir_B, '*.jpg'))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.dir_A), len(self.dir_B))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image_A = Image.open(self.dir_A[idx])\n",
    "\n",
    "        image_B = Image.open(self.dir_B[idx])\n",
    "        \n",
    "        if self.transform:\n",
    "            image_A = self.transform(image_A)\n",
    "            image_B = self.transform(image_B)\n",
    "\n",
    "        return image_A, image_B\n",
    "    \n",
    "class DatasetFromFolder(Dataset):\n",
    "    def __init__(self, photo_path, sketch_path, transform = None):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        \n",
    "        self.photo_path = photo_path\n",
    "        self.sketch_path = sketch_path\n",
    "\n",
    "        self.image_filenames = [x for x in os.listdir(self.photo_path) if \n",
    "                               any(x.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])]\n",
    "\n",
    "        transform_list = [transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "\n",
    "        self.transform = transforms.Compose(transform_list)\n",
    "    \n",
    "    \n",
    "    def load_img(self, filepath):\n",
    "        img = Image.open(filepath).convert('RGB')\n",
    "        #img = img.resize((256, 256), Image.BICUBIC)\n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Load Image}  \n",
    "   \n",
    "        input = self.load_img(os.path.join(self.photo_path, self.image_filenames[index]))\n",
    "        input = self.transform(input)\n",
    "        target = self.load_img(os.path.join(self.sketch_path, self.image_filenames[index]))\n",
    "        target = self.transform(target)\n",
    "\n",
    "        return input, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     1,
     78
    ]
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        #Downsampling layers\n",
    "        \n",
    "        #c7s1-32\n",
    "       \n",
    "        self.conv_1 = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False) \n",
    "        \n",
    "        #d64\n",
    "        self.conv_2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=True)\n",
    "        self.norm_2 = nn.BatchNorm2d(128, affine=True, track_running_stats=True)\n",
    "        \n",
    "        #d128\n",
    "        self.conv_3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=True)\n",
    "        self.norm_3 = nn.BatchNorm2d(256, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.conv_4 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=True)\n",
    "        self.norm_4 = nn.BatchNorm2d(512, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.conv_5 = nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1, bias=True)\n",
    "        self.norm_5 = nn.BatchNorm2d(512, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.conv_6 = nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1, bias=True)\n",
    "        self.norm_6 = nn.BatchNorm2d(512, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.conv_7 = nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1, bias=True)\n",
    "        self.norm_7 = nn.BatchNorm2d(512, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.conv_8 = nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1, bias=True)\n",
    "        self.norm_8 = nn.BatchNorm2d(512, affine=True, track_running_stats=True)\n",
    "           \n",
    "        #UpSampling\n",
    "        self.conv_9 = nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2,\n",
    "                                         padding=1,\n",
    "                                         bias=True)\n",
    "        self.norm_9 = nn.BatchNorm2d(512, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.conv_10 = nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2,\n",
    "                                         padding=1,\n",
    "                                         bias=True)\n",
    "        self.norm_10 = nn.BatchNorm2d(512, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.conv_11 = nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2,\n",
    "                                         padding=1,\n",
    "                                         bias=True)\n",
    "        self.norm_11 = nn.BatchNorm2d(512, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.conv_12 = nn.ConvTranspose2d(512, 512, kernel_size=4, stride=2,\n",
    "                                         padding=1,\n",
    "                                         bias=True)\n",
    "        self.norm_12 = nn.BatchNorm2d(512, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.conv_13 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2,\n",
    "                                         padding=1,\n",
    "                                         bias=True)\n",
    "        self.norm_13 = nn.BatchNorm2d(256, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.conv_14 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2,\n",
    "                                         padding=1,\n",
    "                                         bias=True)\n",
    "        self.norm_14 = nn.BatchNorm2d(128, affine=True, track_running_stats=True)\n",
    "          \n",
    "        #u32\n",
    "        self.conv_15 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2,\n",
    "                                         padding=1,\n",
    "                                         bias=True)\n",
    "        self.norm_15 = nn.BatchNorm2d(64, affine=True, track_running_stats=True)\n",
    "        \n",
    "        #output layer\n",
    "        \n",
    "        self.conv_out = nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Inputs:\n",
    "            x: (batch x 3 x 256 x 256)\n",
    "        Outputs:\n",
    "            image: (batch x 3 x 256 x 256)\n",
    "        '''\n",
    "    \n",
    "        x = self.conv_1(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        \n",
    "        x = self.conv_2(x)\n",
    "        x = self.norm_2(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        \n",
    "        x = self.conv_3(x)\n",
    "        x = self.norm_3(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "\n",
    "        x = self.conv_4(x)\n",
    "        x = self.norm_4(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        \n",
    "        x = self.conv_5(x)\n",
    "        x = self.norm_5(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        \n",
    "        x = self.conv_6(x)\n",
    "        x = self.norm_6(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        \n",
    "        x = self.conv_7(x)\n",
    "        x = self.norm_7(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        \n",
    "        x = self.conv_8(x)\n",
    "        x = self.norm_8(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        \n",
    "        #upsampling\n",
    "        x = self.conv_9(x)\n",
    "        x = self.norm_9(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv_10(x)\n",
    "        x = self.norm_10(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv_11(x)\n",
    "        x = self.norm_11(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv_12(x)\n",
    "        x = self.norm_12(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv_13(x)\n",
    "        x = self.norm_13(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv_14(x)\n",
    "        x = self.norm_14(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv_15(x)\n",
    "        x = self.norm_15(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        \n",
    "        x = self.conv_out(x)\n",
    "        x = torch.tanh(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(6, 64, kernel_size=4, stride=2, padding=1, bias=True)\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=True)\n",
    "        self.norm_2 = nn.InstanceNorm2d(128, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.conv_3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=True)\n",
    "        self.norm_3 = nn.InstanceNorm2d(256, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.conv_4 = nn.Conv2d(256, 512, kernel_size=4, padding=1, bias=True)\n",
    "        self.norm_4 = nn.InstanceNorm2d(512, affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.conv_output = nn.Conv2d(512, 1, kernel_size=4, padding=1, bias = False)\n",
    "       \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Inputs:\n",
    "            x: (batch, 3, 128, 128)\n",
    "        Output:\n",
    "            out: (batch, 1)\n",
    "        '''\n",
    "        \n",
    "        x = self.conv_1(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        \n",
    "        x = self.conv_2(x)\n",
    "        x = self.norm_2(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        \n",
    "        x = self.conv_3(x)\n",
    "        x = self.norm_3(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        \n",
    "        x = self.conv_4(x)\n",
    "        x = self.norm_4(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        \n",
    "        x = self.conv_output(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_l1 = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_gen = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "optimizer_disc = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "dataset = DatasetFromFolder('facades/train/a', 'facades/train/b')\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "input_A = Tensor(batch_size, 3, 256, 256)\n",
    "input_B = Tensor(batch_size, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input A is image of summer\n",
    "### Input B is image of winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generator loss 0.000000, discriminator Loss 0.111354\n",
      "The generator loss 0.000000, discriminator Loss 0.113223\n",
      "The generator loss 0.000000, discriminator Loss 0.119994\n",
      "The generator loss 0.000000, discriminator Loss 0.153607\n",
      "The generator loss 0.000000, discriminator Loss 0.097045\n",
      "The generator loss 0.000000, discriminator Loss 0.137227\n",
      "The generator loss 0.000000, discriminator Loss 0.176088\n",
      "The generator loss 0.000000, discriminator Loss 0.140484\n",
      "The generator loss 0.000000, discriminator Loss 0.131814\n",
      "The generator loss 0.000000, discriminator Loss 0.108552\n",
      "The generator loss 0.000000, discriminator Loss 0.121932\n",
      "The generator loss 0.000000, discriminator Loss 0.146454\n",
      "The generator loss 0.000000, discriminator Loss 0.121909\n",
      "The generator loss 0.000000, discriminator Loss 0.119357\n",
      "The generator loss 0.000000, discriminator Loss 0.118586\n",
      "The generator loss 0.000000, discriminator Loss 0.123267\n",
      "The generator loss 0.000000, discriminator Loss 0.104813\n",
      "The generator loss 0.000000, discriminator Loss 0.119134\n",
      "The generator loss 0.000000, discriminator Loss 0.119332\n",
      "The generator loss 0.000000, discriminator Loss 0.139205\n",
      "The generator loss 0.000000, discriminator Loss 0.102809\n",
      "The generator loss 0.000000, discriminator Loss 0.107474\n",
      "The generator loss 0.000000, discriminator Loss 0.128871\n",
      "The generator loss 0.000000, discriminator Loss 0.096358\n",
      "The generator loss 0.000000, discriminator Loss 0.124502\n",
      "The generator loss 0.000000, discriminator Loss 0.137025\n",
      "The generator loss 0.000000, discriminator Loss 0.125570\n",
      "The generator loss 0.000000, discriminator Loss 0.140355\n",
      "The generator loss 0.000000, discriminator Loss 0.115429\n",
      "The generator loss 0.000000, discriminator Loss 0.101425\n",
      "The generator loss 0.000000, discriminator Loss 0.105188\n",
      "The generator loss 0.000000, discriminator Loss 0.103178\n",
      "The generator loss 0.000000, discriminator Loss 0.094075\n",
      "The generator loss 0.000000, discriminator Loss 0.125434\n",
      "The generator loss 0.000000, discriminator Loss 0.126014\n",
      "The generator loss 0.000000, discriminator Loss 0.107476\n",
      "The generator loss 0.000000, discriminator Loss 0.104977\n",
      "The generator loss 0.000000, discriminator Loss 0.108688\n",
      "The generator loss 0.000000, discriminator Loss 0.118603\n",
      "The generator loss 0.000000, discriminator Loss 0.118607\n",
      "The generator loss 0.000000, discriminator Loss 0.134554\n",
      "The generator loss 0.000000, discriminator Loss 0.096821\n",
      "The generator loss 0.000000, discriminator Loss 0.107785\n",
      "The generator loss 0.000000, discriminator Loss 0.125315\n",
      "The generator loss 0.000000, discriminator Loss 0.117061\n",
      "The generator loss 0.000000, discriminator Loss 0.106508\n",
      "The generator loss 0.000000, discriminator Loss 0.118279\n",
      "The generator loss 0.000000, discriminator Loss 0.123986\n",
      "The generator loss 0.000000, discriminator Loss 0.111249\n",
      "The generator loss 0.000000, discriminator Loss 0.127942\n",
      "The generator loss 0.000000, discriminator Loss 0.120724\n",
      "The generator loss 0.000000, discriminator Loss 0.098496\n",
      "The generator loss 0.000000, discriminator Loss 0.095575\n",
      "The generator loss 0.000000, discriminator Loss 0.097588\n",
      "The generator loss 0.000000, discriminator Loss 0.100350\n",
      "The generator loss 0.000000, discriminator Loss 0.105515\n",
      "The generator loss 0.000000, discriminator Loss 0.097606\n",
      "The generator loss 0.000000, discriminator Loss 0.112413\n",
      "The generator loss 0.000000, discriminator Loss 0.095584\n",
      "The generator loss 0.000000, discriminator Loss 0.096330\n",
      "The generator loss 0.000000, discriminator Loss 0.107458\n",
      "The generator loss 0.000000, discriminator Loss 0.101874\n",
      "The generator loss 0.000000, discriminator Loss 0.092277\n",
      "The generator loss 0.000000, discriminator Loss 0.094858\n",
      "The generator loss 0.000000, discriminator Loss 0.119653\n",
      "The generator loss 0.000000, discriminator Loss 0.103363\n",
      "The generator loss 0.000000, discriminator Loss 0.078939\n",
      "The generator loss 0.000000, discriminator Loss 0.096786\n",
      "The generator loss 0.000000, discriminator Loss 0.085967\n",
      "The generator loss 0.000000, discriminator Loss 0.090647\n",
      "The generator loss 0.000000, discriminator Loss 0.094907\n",
      "The generator loss 0.000000, discriminator Loss 0.088135\n",
      "The generator loss 0.000000, discriminator Loss 0.097476\n",
      "The generator loss 0.000000, discriminator Loss 0.101779\n"
     ]
    }
   ],
   "source": [
    "\n",
    "while epoch<200:\n",
    "    for en, x in enumerate(dataloader):\n",
    "#         x = x.to(device)\n",
    "        \n",
    "        input_A = Tensor(x[0].size(0), 3, 256, 256)\n",
    "        input_B = Tensor(x[0].size(0), 3, 256, 256)\n",
    "        real_A = Variable(input_A.copy_(x[0])) \n",
    "        real_B = Variable(input_B.copy_(x[1]))\n",
    "        \n",
    "            \n",
    "        target_real = Variable(Tensor(real_A.size(0), 1, 30, 30).fill_(1.0), requires_grad=False)\n",
    "        target_fake = Variable(Tensor(real_B.size(0), 1, 30, 30).fill_(0.0), requires_grad=False)\n",
    "        \n",
    "        ### Disctiminator LOSS ###     \n",
    "        optimizer_disc.zero_grad()\n",
    "        \n",
    "        fake_B = generator(real_A)    \n",
    "        disc_fake = discriminator(torch.cat((real_A, fake_B.detach()), 1))\n",
    "        fake_loss = criterion_GAN(disc_fake, target_fake)\n",
    "        \n",
    "        disc_real = discriminator(torch.cat((real_B, real_A), 1))\n",
    "        real_loss = criterion_GAN(disc_real, target_real)\n",
    "        \n",
    "        disc_loss = (fake_loss + real_loss) * 0.5        \n",
    "        disc_loss.backward()\n",
    "        optimizer_disc.step()\n",
    "        \n",
    "\n",
    "        \n",
    "        ### Generator Loss ###\n",
    "        optimizer_gen.zero_grad()\n",
    "        \n",
    "        disc_out = discriminator(torch.cat((fake_B, real_A), 1))\n",
    "        gen_loss = criterion_GAN(disc_out, target_real) \n",
    "        \n",
    "          # L1 loss #\n",
    "        loss_l1 = criterion_l1(fake_B, real_B)\n",
    "        \n",
    "        all_gen_loss = loss_l1 + gen_loss\n",
    "        all_gen_loss.backward()\n",
    "        optimizer_gen.step()\n",
    "    \n",
    "        if en%100==0:          \n",
    "            img = torchvision.utils.make_grid(torch.cat([real_A, fake_B], dim=0), nrow=4)\n",
    "            save_image(img, filename=('pix2pix_outputs/'+str(epoch)+'_'+str(en)+'.jpg'))\n",
    "            \n",
    "    print(\"The generator loss {0:2f}, discriminator Loss {1:2f}\".format(disc_loss.item(), all_gen_loss.item()))\n",
    "    epoch += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generator_A2B' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-72063dfea2c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mgenerator_A2B\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mreal_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfake_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_A2B\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generator_A2B' is not defined"
     ]
    }
   ],
   "source": [
    "test_dataset = MyDataset('data/testA', 'data/testB', transform=transforms.Compose([\n",
    "                                               transforms.Resize(128),\n",
    "                                               transforms.ToTensor()]))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "for en, test in enumerate(dataloader):\n",
    "    generator_A2B.eval()\n",
    "    real_A = Variable(test[0]).to(device)\n",
    "    fake_B = generator_A2B(real_A)\n",
    "    img = torchvision.utils.make_grid([real_A.squeeze(0), fake_B.squeeze(0)], nrow=2)\n",
    "    save_image(img, filename=('test_outputs/'+'_'+str(en)+'.jpg'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
