{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "image_size = 32\n",
    "batch_size = 8\n",
    "\n",
    "dataset = datasets.CIFAR10(root=\"./data/cifar\", \n",
    "                           download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean=0.0, std=0.02)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        if m.weight is not None:\n",
    "            m.weight.data.normal_(mean=1.0, std=0.02)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#64  kernel_size 2 stride 2 padding 1\n",
    "#128 kernel_size 4 stride 2 padding 1\n",
    "#256 kernel_size 4 stride 2 padding 1\n",
    "#512 kernel_size 4 stride 2 padding 1\n",
    "#1   kernel_size 2 stride 1 padding 0\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(3, 64, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.batch_norm_1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(64, 128, kernel_size = 4, stride = 2, padding = 1)\n",
    "        self.batch_norm_2 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv_3 = nn.Conv2d(128, 256, kernel_size = 4, stride = 2, padding = 1)\n",
    "        self.batch_norm_3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv_4 = nn.Conv2d(256, 512, kernel_size = 4, stride = 2, padding = 1)\n",
    "        self.batch_norm_4 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        self.conv_5 = nn.Conv2d(512, 1, kernel_size = 2, stride = 1, padding = 0)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Inputs:\n",
    "            x: (batch x 3 x 32 x 32)\n",
    "        Outputs:\n",
    "            prob: (batch x 1)\n",
    "        '''\n",
    "        x = self.conv_1(x)\n",
    "        x = self.batch_norm_1(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        \n",
    "        x = self.conv_2(x)\n",
    "        x = self.batch_norm_2(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        \n",
    "        x = self.conv_3(x)\n",
    "        x = self.batch_norm_3(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        \n",
    "        x = self.conv_4(x)\n",
    "        x = self.batch_norm_4(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        \n",
    "        x = self.conv_5(x)\n",
    "\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = torch.sigmoid(x)\n",
    "\n",
    "        \n",
    "        return x\n",
    "\n",
    "          \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_size):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.conv_1 = nn.ConvTranspose2d(z_size, 512, kernel_size = 4, stride = 1, padding = 0, bias=False)     \n",
    "        \n",
    "        self.conv_2 = nn.ConvTranspose2d(512, 256, kernel_size = 4, stride = 2, padding = 1, bias=False)\n",
    "        self.batch_norm_2 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.conv_3 = nn.ConvTranspose2d(256, 128, kernel_size = 4, stride = 2, padding = 1, bias=False)\n",
    "        self.batch_norm_3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv_4 = nn.ConvTranspose2d(128, 3, kernel_size = 4, stride = 2, padding = 1, bias=False)\n",
    "#         self.batch_norm_4 = nn.BatchNorm2d(64)      \n",
    "        \n",
    "#         self.conv_5 = nn.ConvTranspose2d(64, 3, kernel_size = 4, stride = 2, padding = 1, bias=False)  \n",
    "        \n",
    "    \n",
    "    def forward(self, noise):\n",
    "        '''\n",
    "        Inputs:\n",
    "            noise: (batch x z_size)\n",
    "        Outputs:\n",
    "            image: (batch x 3 x 32 x 32)\n",
    "        '''\n",
    "        #code here\n",
    "        image = noise.view(noise.size(0), 100, 1, 1)\n",
    "\n",
    "        \n",
    "        image = self.conv_1(image)\n",
    "        image = F.leaky_relu(image, 0.2)\n",
    "        \n",
    "        image = self.conv_2(image)\n",
    "        image = self.batch_norm_2(image)\n",
    "        image = F.leaky_relu(image, 0.2)\n",
    "        \n",
    "        image = self.conv_3(image)\n",
    "        image = self.batch_norm_3(image)\n",
    "        image = F.leaky_relu(image, 0.2)\n",
    "        \n",
    "        image = self.conv_4(image)\n",
    "#         image = self.batch_norm_4(image)\n",
    "        image = F.tanh(image)\n",
    "        \n",
    "#         image = self.conv_5(image)\n",
    "#         image = F.leaky_relu(image, 0.2)\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 100\n",
    "\n",
    "generator     = Generator(latent_size).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "lr    = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "gen_optimizer = optim.Adam(generator.parameters(),     lr=lr, betas=(beta1, beta2))\n",
    "dis_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "epoch      = 0\n",
    "num_epochs = 25\n",
    "\n",
    "dis_losses = []\n",
    "gen_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(batch_size, latent_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-03e301712b61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mdis_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mdis_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while epoch < num_epochs:\n",
    "    for batch_idx, (image, _) in enumerate(dataloader):\n",
    "        image = image.to(device)\n",
    "        \n",
    "        real_labels = torch.ones(image.size(0), 1).to(device)\n",
    "        fake_labels = torch.zeros(image.size(0), 1).to(device)\n",
    "\n",
    "        generated_image = generator(torch.randn(image.size(0), 100).to(device))\n",
    "        \n",
    "        generator_loss = adversarial_loss(discriminator(generated_image), real_labels)\n",
    "        \n",
    "        gen_optimizer.zero_grad()\n",
    "        generator_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "        \n",
    "\n",
    "        #code here\n",
    "        disc_output = discriminator(image)\n",
    "        discriminator_loss = adversarial_loss(disc_output, real_labels) + adversarial_loss(discriminator(generated_image.detach()), fake_labels)\n",
    "            \n",
    "        dis_optimizer.zero_grad()\n",
    "        discriminator_loss.backward()\n",
    "        dis_optimizer.step()\n",
    "        \n",
    "        dis_losses.append(discriminator_loss.item()) \n",
    "        gen_losses.append(generator_loss.item())\n",
    "        \n",
    "        if batch_idx % 500 == 0:\n",
    "            torchvision.utils.save_image(generator(fixed_noise).data,\n",
    "                                          '%s/%d_%d.png' % ('./dcan', epoch, batch_idx),\n",
    "                                          normalize=True)\n",
    "        \n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
