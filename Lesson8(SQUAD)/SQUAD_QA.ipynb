{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import math\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, counter, sos, eos, pad, unk, min_freq=5):\n",
    "        self.sos = sos\n",
    "        self.eos = eos\n",
    "        self.pad = pad\n",
    "        self.unk = unk\n",
    "        \n",
    "        self.pad_idx = 0\n",
    "        self.unk_idx = 1\n",
    "        self.sos_idx = 2\n",
    "        self.eos_idx = 3\n",
    "        \n",
    "        self._token2idx = {\n",
    "            self.sos: self.sos_idx,\n",
    "            self.eos: self.eos_idx,\n",
    "            self.pad: self.pad_idx,\n",
    "            self.unk: self.unk_idx,\n",
    "        }\n",
    "        self._idx2token = {idx:token for token, idx in self._token2idx.items()}\n",
    "        \n",
    "        idx = len(self._token2idx)\n",
    "        min_freq = 0 if min_freq is None else min_freq\n",
    "        \n",
    "        for token, count in counter.items():\n",
    "            if count > min_freq:\n",
    "                self._token2idx[token] = idx\n",
    "                self._idx2token[idx]   = token\n",
    "                idx += 1\n",
    "        \n",
    "        self.vocab_size = len(self._token2idx)\n",
    "        self.tokens     = list(self._token2idx.keys())\n",
    "    \n",
    "    def token2idx(self, token):\n",
    "        return self._token2idx.get(token, self.pad_idx)\n",
    "    \n",
    "    def idx2token(self, idx):\n",
    "        return self._idx2token.get(idx, self.pad)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def pad_single_seq(sequence, pad_idx, max_length):\n",
    "    '''\n",
    "    Inputs:\n",
    "        sequence: list of tokens\n",
    "    '''    \n",
    "    return sequence + [pad_idx]*(max_length - len(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey', 'how', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.word_tokenize((\"HEy how are you?\").lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, path, val=False):\n",
    "\n",
    "        shuffle  = True\n",
    "        self.val = val\n",
    "        self.data = []\n",
    "        word_data = []       \n",
    "        words_counter = Counter()\n",
    "          \n",
    "        with open(path) as json_data:\n",
    "            json_data = json.load(json_data)\n",
    "            \n",
    "        for data_cell in json_data['data']:\n",
    "            for paragraph in data_cell['paragraphs']:\n",
    "                context = nltk.word_tokenize(paragraph['context'].lower())\n",
    "                for qa in paragraph['qas']:\n",
    "                    question = nltk.word_tokenize(qa['question'].lower())\n",
    "                    for ans in qa['answers']:\n",
    "                        \n",
    "                        answer = nltk.word_tokenize(ans['text'].lower())\n",
    "\n",
    "                        word_data.append((context, question, answer))\n",
    "\n",
    "                        for token in context:\n",
    "                            words_counter[token] += 1\n",
    "                        for token in question:\n",
    "                            words_counter[token] += 1\n",
    "                        for token in answer:\n",
    "                            words_counter[token] += 1  \n",
    "\n",
    "                \n",
    "        sos = \"<sos>\"\n",
    "        eos = \"<eos>\"\n",
    "        pad = \"<pad>\"\n",
    "        unk = \"<unk>\"\n",
    "\n",
    "        self.words_vocab = Vocab(words_counter, \n",
    "                            sos, eos, pad, unk)\n",
    "\n",
    " \n",
    "        if not val:\n",
    "            random.shuffle(self.data)\n",
    "        \n",
    "        for context, question, answer in word_data:\n",
    "\n",
    "            cell_context = [self.words_vocab.token2idx(item) for item in context]    \n",
    "            cell_question = [self.words_vocab.token2idx(item) for item in question]\n",
    "            cell_answer = [self.words_vocab.token2idx(item) for item in answer]\n",
    "            \n",
    "            self.data.append((cell_context, cell_question, cell_answer))\n",
    "                \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def get_batch(self, batch_size):\n",
    "        \n",
    "        random_ids = np.random.randint(0, len(self.data), batch_size)\n",
    "        if not self.val:\n",
    "            batch_data = [self.data[idx] for idx in random_ids]\n",
    "        else:\n",
    "            batch_data = self.data\n",
    "        \n",
    "        max_context_length = max([len(a) for (a, _, _) in batch_data])\n",
    "        max_question_length = max([len(b) for (_, b, _) in batch_data])\n",
    "        max_answer_length = max([len(c) for (_, _, c) in batch_data])\n",
    "\n",
    "        contexts = []\n",
    "        questions = []\n",
    "        answers = []\n",
    "        for a, b, c in batch_data:\n",
    "            \n",
    "            cell_context  = pad_single_seq(a, self.words_vocab.pad_idx, max_context_length)\n",
    "            cell_question = pad_single_seq(b, self.words_vocab.pad_idx, max_question_length)\n",
    "            cell_answer   = pad_single_seq(c, self.words_vocab.pad_idx, max_answer_length)  \n",
    "            \n",
    "            cell_context  = torch.LongTensor(cell_context).to(device)\n",
    "            cell_question = torch.LongTensor(cell_question).to(device)\n",
    "            cell_answer   = torch.LongTensor(cell_answer).to(device)\n",
    "            \n",
    "            contexts.append(cell_context)\n",
    "            questions.append(cell_question)\n",
    "            answers.append(cell_answer)\n",
    "            \n",
    "\n",
    "        contexts  = torch.stack(contexts, 0)\n",
    "        questions = torch.stack(questions, 0)      \n",
    "        answers   = torch.stack(answers, 0)#.squeeze(1)\n",
    "\n",
    "        return contexts, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset('train-v2.0.json')\n",
    "test_dataset  = Dataset('train-v2.0.json', val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60958"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of words in vocabulary\n",
    "len(train_dataset.words_vocab._token2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=5000):\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp((torch.arange(0, dim, 2, dtype=torch.float) *\n",
    "                             -(math.log(10000.0) / dim)))\n",
    "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, emb):\n",
    "        emb = emb * math.sqrt(self.dim)\n",
    "        emb = emb + self.pe[:,:emb.size(1),:]\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads, model_size):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert model_size % num_heads == 0 \n",
    "        \n",
    "        self.model_size = model_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size    = model_size // num_heads\n",
    "        \n",
    "        self.linear_query = nn.Linear(model_size, model_size, bias = False)\n",
    "        self.linear_key   = nn.Linear(model_size, model_size, bias = False)\n",
    "        self.linear_value = nn.Linear(model_size, model_size, bias = False)\n",
    "        self.linear_out = nn.Linear(model_size, model_size, bias = False)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        '''\n",
    "        Inputs:\n",
    "            query:   (batch, target_len, hidden)\n",
    "            key:     (batch, source_len, hidden)\n",
    "            value:   (batch, source_len, hidden)  \n",
    "            mask:    (batch, target_len, source_len)\n",
    "\n",
    "        Outputs:\n",
    "            output:  (batch, target_len, model_size)\n",
    "            weight:  (batch, num_heads, target_len, source_len)\n",
    "        '''\n",
    "        batch_size = query.size(0)        \n",
    "        \n",
    "        query = self.linear_query(query)\n",
    "        key   = self.linear_key(key)\n",
    "        value = self.linear_value(value)\n",
    "      \n",
    "        query = query.view(batch_size, -1, self.num_heads, self.head_size)\n",
    "        key   = key.view(batch_size, -1, self.num_heads, self.head_size)\n",
    "        value = value.view(batch_size, -1, self.num_heads, self.head_size)\n",
    "        \n",
    "        query = query.transpose(1, 2)\n",
    "        key = key.transpose(1, 2)\n",
    "        value = value.transpose(1, 2)\n",
    "             \n",
    "        key = key.transpose(2, 3)\n",
    "        \n",
    "        logits = torch.matmul(query, key)\n",
    "        logits = logits / math.sqrt(self.head_size)\n",
    "\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)#.repeat()\n",
    "            logits.masked_fill_(mask, -1e18)      \n",
    "        \n",
    "        weights = F.softmax(logits, dim = -1)\n",
    "\n",
    "        output = torch.matmul(weights, value)       \n",
    "        output = output.transpose(1,2).contiguous()\n",
    "        output = output.view(batch_size, -1, self.model_size)\n",
    "        output = self.linear_out(output)\n",
    "\n",
    "        return output, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, model_size, ff_size):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        \n",
    "        self.linear_1 = nn.Linear(model_size, ff_size)\n",
    "        self.linear_2 = nn.Linear(ff_size, model_size)\n",
    "        self.layer_norm = nn.LayerNorm(model_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Inputs:\n",
    "            x: (batch_size x seq_len x model-size)\n",
    "        Outputs:\n",
    "            output: \n",
    "        '''        \n",
    "        res = x\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.linear_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        \n",
    "        return x + res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, model_size, num_heads, ff_size):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attention = MultiHeadAttention(num_heads, model_size)\n",
    "        self.positionwise_ff = PositionWiseFeedForward(model_size, ff_size)\n",
    "        self.layer_norm = nn.LayerNorm(model_size)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        '''\n",
    "        Inputs:\n",
    "            x: (batch x seq_len x model_size)\n",
    "            mask: (batch x seq_len x seq_len)\n",
    "            \n",
    "        Outputs:\n",
    "            output : (batch x seq_len, model_size)\n",
    "        '''\n",
    "        \n",
    "        res  = x\n",
    "        x, _ = self.self_attention(x, x, x, mask)       \n",
    "        x    = x + res\n",
    "        x    = self.positionwise_ff(x)\n",
    "        x    = self.layer_norm(x)\n",
    "        \n",
    "        \n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                vocab_size,\n",
    "                num_layers, \n",
    "                model_size, \n",
    "                num_heads,\n",
    "                ff_size,\n",
    "                padding_idx):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, model_size, padding_idx=padding_idx)\n",
    "    \n",
    "        self.positional_enc = PositionalEncoding(model_size)\n",
    "\n",
    "        self.enc_blocks = nn.ModuleList(\n",
    "                            [TransformerEncoderLayer(model_size, num_heads, ff_size)\n",
    "                                        for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, source):\n",
    "        \n",
    "        '''\n",
    "        Inputs: \n",
    "            source: (batch_size, source_len)\n",
    "            \n",
    "        Outputs:\n",
    "            x: (batch, source_len, hidden)\n",
    "        '''\n",
    "        source_mask = source == self.padding_idx\n",
    "        mask = (source == self.padding_idx).unsqueeze(1).repeat(1, source.size(1), 1)\n",
    "\n",
    "        source_emb = self.embedding(source)\n",
    "        source_emb = self.positional_enc(source_emb)\n",
    "        \n",
    "        x = source_emb\n",
    "        for layer in self.enc_blocks:       \n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return x, source_mask   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, model_size, num_heads, ff_size):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attention = MultiHeadAttention(num_heads, model_size)\n",
    "        self.positionwise_ff = PositionWiseFeedForward(model_size, ff_size)\n",
    "        self.layer_norm = nn.LayerNorm(model_size)\n",
    "        \n",
    "    def forward(self, x, enc_outputs, enc_mask=None, subseq_mask=None):\n",
    "        '''\n",
    "        Inputs:\n",
    "            x: (batch x target_len x model_size)\n",
    "            enc_outputs: (batch x num_heads x model_size)\n",
    "            enc_mask : (batch x target_len x source_len)\n",
    "            subseq_mask: (batch x target_len x target_len)\n",
    "            \n",
    "        Outputs:\n",
    "            output : (batch x ? x ?)\n",
    "        '''      \n",
    "        res  = x\n",
    "        \n",
    "        x, _ = self.self_attention(x, x, x, subseq_mask)     \n",
    "        x    = x + res       \n",
    "        x    = self.layer_norm(x)       \n",
    "        \n",
    "        res2 = x      \n",
    "        #enc_mask = enc_mask.unsqueeze(1).repeat(1, res.size(1), 1)\n",
    "        x, _ = self.self_attention(x, enc_outputs, enc_outputs, enc_mask)\n",
    "        \n",
    "        x = res2 + x        \n",
    "        x = self.positionwise_ff(x)        \n",
    "        x = self.layer_norm(x)        \n",
    "        \n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, num_layers, pad_idx):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        self.padding_idx = pad_idx\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=pad_idx)\n",
    "        \n",
    "        self.positional_enc = PositionalEncoding(hidden_size)\n",
    "        \n",
    "        self.dec_blocks = nn.ModuleList(\n",
    "                            [TransformerDecoderLayer(hidden_size, num_heads, ff_size)\n",
    "                                        for _ in range(num_layers)])\n",
    "        self.linear_out = nn.Linear(hidden_size, vocab_size, bias = False)   \n",
    "        \n",
    "    def forward(self, target, enc_outputs, batch_words, val=False):        \n",
    "        '''\n",
    "        Inputs: \n",
    "            source: (batch_size, target_len)\n",
    "            \n",
    "        Outputs:\n",
    "            x: (batch, source_len, hidden)\n",
    "        '''\n",
    "\n",
    "        batch_size, target_len = target.size()\n",
    "        \n",
    "        subseq_mask = torch.triu(\n",
    "                torch.ones((target_len, target_len), device=target.device, dtype=torch.uint8), diagonal=1)\n",
    "        subseq_mask = subseq_mask.unsqueeze(0).expand(batch_size, -1, -1)  # b x ls x ls\n",
    "        \n",
    "        dec_mask_ = (target == 0).unsqueeze(1).repeat(1, target_len, 1)\n",
    "        \n",
    "        dec_mask = (subseq_mask + dec_mask_).gt(0)\n",
    "        \n",
    "        target_emb = self.embedding(target)\n",
    "        target_emb = self.positional_enc(target_emb) \n",
    "        \n",
    "        \n",
    "        enc_mask = batch_words == 0\n",
    "        enc_mask = enc_mask.unsqueeze(1).repeat(1, target_len, 1)\n",
    "        x = target_emb\n",
    "        for layer in self.dec_blocks:       \n",
    "            x = layer(x, enc_outputs, enc_mask, dec_mask)            \n",
    "        \n",
    "        \n",
    "        logits = self.linear_out(x)\n",
    "        print(logits.shape, 'logits')\n",
    "        \n",
    "        logits = logits.view(-1, self.vocab_size)\n",
    "        \n",
    "        #out = F.softmax(logits)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, encoder, decoder, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        \n",
    "    def forward(self, batch_context, batch_question,mask = None):\n",
    "        \n",
    "        enc_outputs, _ = self.encoder(batch_context)\n",
    "        out = self.decoder(batch_question, enc_outputs, batch_context, False)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "num_heads   = 8\n",
    "num_layers = 2\n",
    "ff_size = 128\n",
    "\n",
    "encoder = TransformerEncoder(len(train_dataset.words_vocab), num_layers, \n",
    "                hidden_size, \n",
    "                num_heads,\n",
    "                ff_size,\n",
    "                0).to(device)\n",
    "\n",
    "decoder = TransformerDecoder(len(train_dataset.words_vocab), hidden_size, hidden_size, num_layers, 0).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = Model(encoder, decoder, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, train_dataset, test_dataset, model, optimizer, criterion, batch_size):\n",
    "        \n",
    "        \n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        \n",
    "               \n",
    "    def train(self, n_epochs):\n",
    "        \n",
    "        mask_words = None\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            for batch_idx in range(len(self.train_dataset)//self.batch_size):\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                contexts, questions, answers = self.train_dataset.get_batch(32)\n",
    "                \n",
    "                logits = self.model(contexts, questions)\n",
    "                print(logits.shape)\n",
    "                print(answers.shape)\n",
    "\n",
    "                loss = self.criterion(logits, answers)                        \n",
    "                \n",
    "                loss.backward()\n",
    "                                \n",
    "                self.optimizer.step()\n",
    "\n",
    "\n",
    "                self.train_losses.append(loss.item())\n",
    "                \n",
    "                if batch_idx % 200 == 0:\n",
    "                    val_loss = self.eval_()\n",
    "                    self.val_losses.append(val_loss.item())\n",
    "                    self.plot(epoch, batch_idx, self.train_losses, self.val_losses)\n",
    "                  \n",
    "        \n",
    "    def eval_(self):\n",
    "        \n",
    "        val_words, val_trans_in, val_trans_out, val_words_lens, val_trans_lens = self.test_dataset.get_batch(len(self.test_dataset))\n",
    "        val_mask = val_words != 0\n",
    "        logits = self.model(val_words, val_trans_in, val_words_lens, val_mask)\n",
    "        val_trans_out = val_trans_out.view(-1)                \n",
    "\n",
    "        mask = val_trans_out != trans_vocab.pad_idx\n",
    "\n",
    "        loss = self.criterion(logits[mask], val_trans_out[mask])\n",
    "\n",
    "        return loss\n",
    "        \n",
    "    def plot(self, epoch, batch_idx, train_losses, val_losses):\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.subplot(131)\n",
    "        plt.title('epoch %s. | batch: %s | loss: %s' % (epoch, batch_idx, train_losses[-1]))\n",
    "        plt.plot(train_losses)\n",
    "        plt.subplot(132)\n",
    "        plt.title('epoch %s. | loss: %s' % (epoch, val_losses[-1]))\n",
    "        plt.plot(val_losses)\n",
    "        plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(train_dataset, test_dataset, model, optimizer, criterion, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 21, 60958]) logits\n",
      "torch.Size([672, 60958])\n",
      "torch.Size([32, 31])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (672) to match target batch_size (32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b3c9f33c4fbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-449880b695ef>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 862\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1403\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m-> 1405\u001b[0;31m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[1;32m   1406\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (672) to match target batch_size (32)."
     ]
    }
   ],
   "source": [
    "trainer.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
