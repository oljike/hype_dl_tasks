{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import math\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, counter, sos, eos, pad, unk, min_freq=5):\n",
    "        self.sos = sos\n",
    "        self.eos = eos\n",
    "        self.pad = pad\n",
    "        self.unk = unk\n",
    "        \n",
    "        self.pad_idx = 0\n",
    "        self.unk_idx = 1\n",
    "        self.sos_idx = 2\n",
    "        self.eos_idx = 3\n",
    "        \n",
    "        self._token2idx = {\n",
    "            self.sos: self.sos_idx,\n",
    "            self.eos: self.eos_idx,\n",
    "            self.pad: self.pad_idx,\n",
    "            self.unk: self.unk_idx,\n",
    "        }\n",
    "        self._idx2token = {idx:token for token, idx in self._token2idx.items()}\n",
    "        \n",
    "        idx = len(self._token2idx)\n",
    "        min_freq = 0 if min_freq is None else min_freq\n",
    "        \n",
    "        for token, count in counter.items():\n",
    "            if count > min_freq:\n",
    "                self._token2idx[token] = idx\n",
    "                self._idx2token[idx]   = token\n",
    "                idx += 1\n",
    "        \n",
    "        self.vocab_size = len(self._token2idx)\n",
    "        self.tokens     = list(self._token2idx.keys())\n",
    "    \n",
    "    def token2idx(self, token):\n",
    "        return self._token2idx.get(token, self.pad_idx)\n",
    "    \n",
    "    def idx2token(self, idx):\n",
    "        return self._idx2token.get(idx, self.pad)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     11
    ]
   },
   "outputs": [],
   "source": [
    "def pad_single_seq(sequence, pad_idx, max_length):\n",
    "    '''\n",
    "    Inputs:\n",
    "        sequence: list of tokens\n",
    "    '''    \n",
    "    return sequence + [pad_idx]*(max_length - len(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, path, val=False):\n",
    "\n",
    "        shuffle  = True\n",
    "        self.val = val\n",
    "        self.data = []\n",
    "        word_data = []       \n",
    "        words_counter = Counter()\n",
    "          \n",
    "        with open(path) as json_data:\n",
    "            json_data = json.load(json_data)\n",
    "            \n",
    "        for data_cell in json_data['data']:\n",
    "            for paragraph in data_cell['paragraphs']:\n",
    "                context = nltk.word_tokenize(paragraph['context'])\n",
    "                for qa in paragraph['qas']:\n",
    "                    question = nltk.word_tokenize(qa['question'])\n",
    "                    for ans in qa['answers']:\n",
    "                        \n",
    "                        answer = nltk.word_tokenize(ans['text'])\n",
    "\n",
    "                        word_data.append((context, question, answer))\n",
    "\n",
    "                        for token in context:\n",
    "                            words_counter[token] += 1\n",
    "                        for token in question:\n",
    "                            words_counter[token] += 1\n",
    "                        for token in answer:\n",
    "                            words_counter[token] += 1  \n",
    "\n",
    "                \n",
    "        sos = \"<sos>\"\n",
    "        eos = \"<eos>\"\n",
    "        pad = \"<pad>\"\n",
    "        unk = \"<unk>\"\n",
    "\n",
    "        self.words_vocab = Vocab(words_counter, \n",
    "                            sos, eos, pad, unk)\n",
    "\n",
    " \n",
    "        if not val:\n",
    "            random.shuffle(self.data)\n",
    "        \n",
    "        for context, question, answer in word_data:\n",
    "\n",
    "            cell_context = [self.words_vocab.token2idx(item) for item in context]    \n",
    "            cell_question = [self.words_vocab.token2idx(item) for item in question]\n",
    "            cell_answer = [self.words_vocab.token2idx(item) for item in answer]\n",
    "            \n",
    "            self.data.append((cell_context, cell_question, cell_answer))\n",
    "                \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def get_batch(self, batch_size, sort = False):\n",
    "        \n",
    "        random_ids = np.random.randint(0, len(self.data), batch_size)\n",
    "        if not self.val:\n",
    "            batch_data = [self.data[idx] for idx in random_ids]\n",
    "        else:\n",
    "            batch_data = self.data\n",
    "        \n",
    "        max_context_length = max([len(a) for (a, _, _) in batch_data])\n",
    "        max_question_length = max([len(b) for (_, b, _) in batch_data])\n",
    "        max_answer_length = max([len(c) for (_, _, c) in batch_data])\n",
    "\n",
    "        contexts = []\n",
    "        questions = []\n",
    "        answers = []\n",
    "        for a, b, c in batch_data:\n",
    "            \n",
    "            cell_context  = pad_single_seq(a, self.words_vocab.pad_idx, max_context_length)\n",
    "            cell_question = pad_single_seq(b, self.words_vocab.pad_idx, max_question_length)\n",
    "            cell_answer   = pad_single_seq(c, self.words_vocab.pad_idx, max_answer_length)  \n",
    "            \n",
    "            cell_context  = torch.LongTensor(cell_context).to(device)\n",
    "            cell_question = torch.LongTensor(cell_question).to(device)\n",
    "            cell_answer   = torch.LongTensor(cell_answer).to(device)\n",
    "            \n",
    "            contexts.append(cell_context)\n",
    "            questions.append(cell_question)\n",
    "            answers.append(cell_answer)\n",
    "            \n",
    "\n",
    "        contexts  = torch.stack(contexts, 0)\n",
    "        questions = torch.stack(questions, 0)      \n",
    "        answers   = torch.stack(answers, 0)#.squeeze(1)\n",
    "\n",
    "        return contexts, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset('train-v2.0.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model\n",
    "\n",
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, model_size, num_heads, ff_size):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attention = MultiHeadAttention(num_heads, model_size)\n",
    "        self.positionwise_ff = PositionWiseFeedForward(model_size, ff_size)\n",
    "        self.layer_norm = nn.LayerNorm(model_size)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        '''\n",
    "        Inputs:\n",
    "            x: (batch x seq_len x model_size)\n",
    "            mask: (batch x seq_len x seq_len)\n",
    "            \n",
    "        Outputs:\n",
    "            output : (batch x seq_len, model_size)\n",
    "        '''\n",
    "        \n",
    "        res  = x\n",
    "        x, _ = self.self_attention(x, x, x, mask)       \n",
    "        x    = x + res\n",
    "        x    = self.positionwise_ff(x)\n",
    "        x    = self.layer_norm(x)\n",
    "        \n",
    "        \n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                vocab_size,\n",
    "                num_layers, \n",
    "                model_size, \n",
    "                num_heads,\n",
    "                ff_size,\n",
    "                padding_idx):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, model_size, padding_idx=padding_idx)\n",
    "    \n",
    "        self.positional_enc = PositionalEncoding(model_size)\n",
    "\n",
    "        self.enc_blocks = nn.ModuleList(\n",
    "                            [TransformerEncoderLayer(model_size, num_heads, ff_size)\n",
    "                                        for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, source):\n",
    "        \n",
    "        '''\n",
    "        Inputs: \n",
    "            source: (batch_size, source_len)\n",
    "            \n",
    "        Outputs:\n",
    "            x: (batch, source_len, hidden)\n",
    "        '''\n",
    "        source_mask = source == self.padding_idx\n",
    "        mask = (source == self.padding_idx).unsqueeze(1).repeat(1, source.size(1), 1)\n",
    "\n",
    "        source_emb = self.embedding(source)\n",
    "        source_emb = self.positional_enc(source_emb)\n",
    "        \n",
    "        x = source_emb\n",
    "        for layer in self.enc_blocks:       \n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return x, source_mask   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, model_size, num_heads, ff_size):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attention = MultiHeadAttention(num_heads, model_size)\n",
    "        self.positionwise_ff = PositionWiseFeedForward(model_size, ff_size)\n",
    "        self.layer_norm = nn.LayerNorm(model_size)\n",
    "        \n",
    "    def forward(self, x, enc_outputs, enc_mask=None, subseq_mask=None):\n",
    "        '''\n",
    "        Inputs:\n",
    "            x: (batch x target_len x model_size)\n",
    "            enc_outputs: (batch x num_heads x model_size)\n",
    "            enc_mask : (batch x target_len x source_len)\n",
    "            subseq_mask: (batch x target_len x target_len)\n",
    "            \n",
    "        Outputs:\n",
    "            output : (batch x ? x ?)\n",
    "        '''      \n",
    "        res  = x\n",
    "        \n",
    "        x, _ = self.self_attention(x, x, x, subseq_mask)     \n",
    "        x    = x + res       \n",
    "        x    = self.layer_norm(x)       \n",
    "        \n",
    "        res2 = x      \n",
    "        #enc_mask = enc_mask.unsqueeze(1).repeat(1, res.size(1), 1)\n",
    "        x, _ = self.self_attention(x, enc_outputs, enc_outputs, enc_mask)\n",
    "        \n",
    "        x = res2 + x        \n",
    "        x = self.positionwise_ff(x)        \n",
    "        x = self.layer_norm(x)        \n",
    "        \n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, num_layers, pad_idx):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        self.padding_idx = pad_idx\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=pad_idx)\n",
    "        \n",
    "        self.positional_enc = PositionalEncoding(hidden_size)\n",
    "        \n",
    "        self.dec_blocks = nn.ModuleList(\n",
    "                            [TransformerDecoderLayer(hidden_size, num_heads, ff_size)\n",
    "                                        for _ in range(num_layers)])\n",
    "        self.linear_out = nn.Linear(hidden_size, vocab_size, bias = False)   \n",
    "        \n",
    "    def forward(self, target, enc_outputs, batch_words, val=False):        \n",
    "        '''\n",
    "        Inputs: \n",
    "            source: (batch_size, target_len)\n",
    "            \n",
    "        Outputs:\n",
    "            x: (batch, source_len, hidden)\n",
    "        '''\n",
    "\n",
    "        batch_size, target_len = target.size()\n",
    "        \n",
    "        subseq_mask = torch.triu(\n",
    "                torch.ones((target_len, target_len), device=target.device, dtype=torch.uint8), diagonal=1)\n",
    "        subseq_mask = subseq_mask.unsqueeze(0).expand(batch_size, -1, -1)  # b x ls x ls\n",
    "        \n",
    "        dec_mask_ = (target == 0).unsqueeze(1).repeat(1, target_len, 1)\n",
    "        \n",
    "        dec_mask = (subseq_mask + dec_mask_).gt(0)\n",
    "        \n",
    "        target_emb = self.embedding(target)\n",
    "        target_emb = self.positional_enc(target_emb) \n",
    "        \n",
    "        \n",
    "        enc_mask = batch_words == 0\n",
    "        enc_mask = enc_mask.unsqueeze(1).repeat(1, target_len, 1)\n",
    "        x = target_emb\n",
    "        for layer in self.dec_blocks:       \n",
    "            x = layer(x, enc_outputs, enc_mask, dec_mask)            \n",
    "        \n",
    "        \n",
    "        logits = self.linear_out(x)\n",
    "        \n",
    "        logits = logits.view(-1, self.vocab_size)\n",
    "        \n",
    "        #out = F.softmax(logits)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dataset, encoder, decoder, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        \n",
    "    def forward(self, batch_words, batch_trans_in, source_lens = None, mask = None):\n",
    "        \n",
    "        enc_outputs, _ = self.encoder(batch_words)\n",
    "        out = self.decoder(batch_trans_in, enc_outputs, batch_words, False)\n",
    "        \n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
