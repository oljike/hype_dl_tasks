{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "import nltk\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from itertools import dropwhile\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, counter, sos, eos, pad, unk, min_freq=10):\n",
    "\n",
    "        self.sos = sos\n",
    "        self.eos = eos\n",
    "        self.pad = pad\n",
    "        self.unk = unk\n",
    "        \n",
    "        self.pad_idx = 0\n",
    "        self.unk_idx = 1\n",
    "        self.sos_idx = 2\n",
    "        self.eos_idx = 3\n",
    "        \n",
    "        self._token2idx = {\n",
    "            self.pad: self.pad_idx,\n",
    "            self.unk: self.unk_idx,\n",
    "            self.eos: self.eos_idx,\n",
    "            self.sos: self.sos_idx\n",
    "            \n",
    "        }\n",
    "        self._idx2token = {idx:token for token, idx in self._token2idx.items()}\n",
    "        \n",
    "        idx = len(self._token2idx)\n",
    "        min_freq = 0 if min_freq is None else min_freq\n",
    "        \n",
    "        for token, count in counter.items():\n",
    "            if count > min_freq:\n",
    "                self._token2idx[token] = idx\n",
    "                self._idx2token[idx]   = token\n",
    "                idx += 1\n",
    "        \n",
    "        self.vocab_size = len(self._token2idx)\n",
    "        self.tokens     = list(self._token2idx.keys())\n",
    "    \n",
    "    def token2idx(self, token):\n",
    "        return self._token2idx.get(token, self.unk_idx)\n",
    "    \n",
    "    def idx2token(self, idx):\n",
    "        return self._idx2token.get(idx, self.unk)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     54
    ]
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, path, val=False):\n",
    "        \n",
    "        assert os.path.exists(path), 'Path does not exist'\n",
    "        \n",
    "        self.val  = val\n",
    "        files     = glob.glob(os.path.join(path, '*.txt'))\n",
    "        data      = []\n",
    "        sentences = []\n",
    "        for file_ in files:\n",
    "            with open(file_, 'r') as f:               \n",
    "                read = f.read()\n",
    "                read = read.lower()\n",
    "                sents = nltk.sent_tokenize(read)\n",
    "                for sent in sents: \n",
    "                    words = nltk.word_tokenize(sent)               \n",
    "                    data.extend(words)               \n",
    "                sentences.extend(sents)\n",
    "            break\n",
    "                    \n",
    "        words_counter = Counter()\n",
    "\n",
    "        for token in data:\n",
    "                    words_counter[token] += 1\n",
    "        \n",
    "        eos = \"<eos>\"\n",
    "        sos = \"<sos>\"\n",
    "        pad = \"<pad>\"\n",
    "        unk = \"<unk>\"\n",
    "        self.words_vocab= Vocab(words_counter, sos, eos, pad, unk)       \n",
    "        self.data_ids = [[self.words_vocab.token2idx(item) for item in nltk.word_tokenize(sent)] for sent in sentences]\n",
    "        \n",
    "    def get_batch(self, batch_size, train, noise, sos=True, eos=False):\n",
    "        \n",
    "        random_ids = np.random.randint(0, len(self.data_ids), batch_size)\n",
    "        if not self.val:\n",
    "            batch_data = [self.data_ids[idx] for idx in random_ids]\n",
    "        else:\n",
    "            batch_data = self.data_ids\n",
    "\n",
    "        \n",
    "        max_length = max([len(sent) for sent in batch_data])\n",
    "        pad_sents  = []\n",
    "        for sent in batch_data:\n",
    "            if train:\n",
    "                sent = self.apply_noise(sent)\n",
    "            pad_sent = self.pad_single_seq(sent, self.words_vocab.pad_idx, max_length)\n",
    "            pad_sent = torch.LongTensor(pad_sent).to(device)\n",
    "            pad_sents.append(pad_sent)\n",
    "            \n",
    "        pad_sents = torch.stack(pad_sents, 0)\n",
    "        \n",
    "        return pad_sents\n",
    "        \n",
    "    def pad_single_seq(self, sequence, pad_idx, max_length):\n",
    "        '''\n",
    "            Pad sequences to max_length\n",
    "        '''    \n",
    "        return sequence + [pad_idx]*(max_length - len(sequence))\n",
    "\n",
    "    def apply_noise(self, sent):\n",
    "        '''\n",
    "            Apply random swapping in inp according to their length\n",
    "        '''        \n",
    "        length = len(sent)\n",
    "        if length > 2:\n",
    "            for it in range(length//2):\n",
    "                j = random.randint(0, length-2)\n",
    "                sent[j], sent[j+1] = sent[j+1], sent[j]\n",
    "\n",
    "        return sent\n",
    "\n",
    "train_dataset = Dataset('kaz_rus/kaz/test')\n",
    "# test_dataset = Dataset('kaz_rus/kaz/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "btch = train_dataset.get_batch(2, train=True, noise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [
     62
    ]
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, num_layers, bidirectional):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vocab_size    = vocab_size\n",
    "        self.emb_size      = emb_size\n",
    "        self.hidden_size   = hidden_size\n",
    "        self.num_layers    = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        #self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
    "        self.gru       = nn.GRU(emb_size, hidden_size, num_layers, bidirectional=bidirectional, batch_first=True)\n",
    "\n",
    "        \n",
    "    def forward(self, sentence, enc_embedding, hidden=None):\n",
    "        '''\n",
    "        Input:\n",
    "            sentence: (batch x seq_len)\n",
    "        Output:\n",
    "            out:      (batch x hidden)\n",
    "        '''\n",
    "               \n",
    "        embeddings = enc_embedding(sentence)\n",
    "\n",
    "        if hidden is not None:\n",
    "            _, hidden = self.gru(embeddings, hidden)\n",
    "        else:\n",
    "            _, hidden = self.gru(embeddings)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            hidden = torch.stack([torch.cat((hidden[2*i], hidden[2*i+1]), dim=1) for i in range(self.num_layers)])\n",
    "\n",
    "        return hidden\n",
    "            \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, n_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size  = vocab_size\n",
    "        self.emb_size    = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
    "        self.gru       = GRULayers(emb_size, hidden_size, n_layers)\n",
    "        self.linear    = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inp, hidden):\n",
    "        '''\n",
    "        Input:\n",
    "            inp: (batch x 1)\n",
    "        Output:\n",
    "            logit: (batch x vocab_size)\n",
    "        '''    \n",
    "        \n",
    "        embedded = self.embedding(inp)\n",
    "        embedded = embedded.squeeze(1)\n",
    "\n",
    "        hidden = self.gru(embedded, hidden) \n",
    "\n",
    "        logit = self.linear(hidden[:, -1, :]) \n",
    "\n",
    "\n",
    "        return logit    \n",
    "\n",
    "class GRULayers(nn.Module):\n",
    "    def __init__(self, emb_size, hidden_size, n_layers):\n",
    "        super(GRULayers, self).__init__()\n",
    "        self.emb_size    = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers    = n_layers\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.layers.append(nn.GRUCell(emb_size, hidden_size))\n",
    "            emb_size = hidden_size\n",
    "    \n",
    "    def forward(self, inp, prev_hidden):\n",
    "        \n",
    "        hiddens = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            current_hidd = layer(inp, prev_hidden[i])\n",
    "            inp          = current_hidd\n",
    "            hiddens     += [current_hidd]\n",
    "        \n",
    "        hiddens = torch.stack(hiddens)\n",
    "        return hiddens           \n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, enc_embedding, dec_embedding, vocab_size, encoder, decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.enc_embedding = enc_embedding\n",
    "        self.dec_embedding = dec_embedding\n",
    "        self.vocab_size    = vocab_size\n",
    "        self.encoder       = encoder\n",
    "        self.decoder       = decoder\n",
    "        self.criterion     = nn.NLLLoss()\n",
    "        \n",
    "    def set_mode(self, mode):\n",
    "        self.enc_embedding.train(mode)\n",
    "        self.dec_embedding.train(mode)\n",
    "        #self.generator.train(mode)\n",
    "        self.encoder.train(mode)\n",
    "        self.decoder.train(mode)\n",
    "        self.criterion.train(mode)  \n",
    "       \n",
    "    def forward(self, input_, src_words, trg_words):\n",
    "        hidden = self.encoder(batch_words, self.enc_embedding)\n",
    "\n",
    "        logits = []\n",
    "        for t in range(2):\n",
    "            logit   = self.decoder(input_, hidden)\n",
    "            input_  = F.softmax(logit, dim=-1).topk(1)[1]#.item()\n",
    "            logits.append(logit)\n",
    "        \n",
    "        logits = torch.stack(logits, 1) \n",
    "        logits = logits.view(-1, self.vocab_size)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_enc_embeddings = nn.Embedding(5000, 30)\n",
    "trg_enc_embeddings = nn.Embedding(5000, 30)\n",
    "\n",
    "src_dec_embeddings = nn.Embedding(5000, 30)\n",
    "trg_dec_embeddings = nn.Embedding(5000, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder         = Encoder(10000, 30, 60, 2, bidirectional=True).to(device)\n",
    "decoder_one     = Decoder(5000, 30, 120, 2).to(device)\n",
    "decoder_two     = Decoder(5000, 30, 120, 2).to(device)\n",
    "\n",
    "model_one2one   = Model(5000, src_enc_embeddings, src_dec_embeddings, encoder, decoder_one).to(device)\n",
    "model_two2two   = Model(5000, trg_enc_embeddings, trg_dec_embeddings, encoder, decoder_two).to(device)\n",
    "model_one2two   = Model(5000, src_enc_embeddings, trg_dec_embeddings, encoder, decoder_two).to(device)\n",
    "model_two2one   = Model(5000, trg_enc_embeddings, src_dec_embeddings, encoder, decoder_one).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_one2one = optim.Adam(params=model_one2one.parameters(), lr=0.00002)\n",
    "optimizer_two2two = optim.Adam(params=model_two2two.parameters(), lr=0.00002)\n",
    "optimizer_one2two = optim.Adam(params=model_one2two.parameters(), lr=0.00002)\n",
    "optimizer_two2one = optim.Adam(params=model_two2one.parameters(), lr=0.00002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, batch_size):\n",
    "    len_data = min(len(train_dataset_one), len(train_dataset_two))\n",
    "       \n",
    "    for batch_idx in range(len_data//batch_size):\n",
    "        optimizer_one2one.zero_grad()\n",
    "        optimizer_two2two.zero_grad()\n",
    "        \n",
    "        batch_one = train_dataset_one.get_batch(batch_size)\n",
    "        batch_two = train_dataset_two.get_batch(batch_size)\n",
    "        \n",
    "        hidden_one = encoder(batch_one)\n",
    "        hidden_two = encoder(batch_two)\n",
    "        \n",
    "        ### Simple autoencoding ###\n",
    "        logits_one = decoder_one(hidden_one)      \n",
    "        logits_one = logits_one.view(-1)\n",
    "        mask = logits_one != train_dataset_one.word_vocab.pad_idx\n",
    "        loss_one = criterion(logits_one[mask], batch_one[mask])\n",
    "                \n",
    "        logits_two = decoder_two(hidden_two)   \n",
    "        logits_two = logits_two.view(-1)\n",
    "        mask = logits_two != train_dataset_one.word_vocab.pad_idx\n",
    "        loss_two = criterion(logits_two[mask], batch_two[mask])\n",
    "        \n",
    "        ### Back Translation ###\n",
    "        logits_cycle_two = decoder_two(hidden_one)      \n",
    "        logits_cycle_two = logits_cycle_two.view(-1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
