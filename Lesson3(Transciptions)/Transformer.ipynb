{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trans import Dataset\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset('data/transcriptions/train.csv')\n",
    "words_vocab = data.words_vocab\n",
    "trans_vocab = data.trans_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim, max_len=5000):\n",
    "        pe = torch.zeros(max_len, dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp((torch.arange(0, dim, 2, dtype=torch.float) *\n",
    "                             -(math.log(10000.0) / dim)))\n",
    "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, emb):\n",
    "        emb = emb * math.sqrt(self.dim)\n",
    "        emb = emb + self.pe[:,:emb.size(1),:]\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_heads, model_size):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert model_size % num_heads == 0 \n",
    "        \n",
    "        self.model_size = model_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size    = model_size // num_heads\n",
    "        \n",
    "        self.linear_query = nn.Linear(model_size, model_size, bias = False)\n",
    "        self.linear_key   = nn.Linear(model_size, model_size, bias = False)\n",
    "        self.linear_value = nn.Linear(model_size, model_size, bias = False)\n",
    "        self.linear_out = nn.Linear(model_size, model_size, bias = False)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        '''\n",
    "        Inputs:\n",
    "            query:   (batch, target_len, hidden)\n",
    "            key:     (batch, source_len, hidden)\n",
    "            value:   (batch, source_len, hidden)  \n",
    "            mask:    (batch, target_len, source_len)\n",
    "\n",
    "        Outputs:\n",
    "            output:  (batch, target_len, model_size)\n",
    "            weight:  (batch, num_heads, target_len, source_len)\n",
    "        '''\n",
    "        batch_size = query.size(0)        \n",
    "        \n",
    "        query = self.linear_query(query)\n",
    "        key   = self.linear_key(key)\n",
    "        value = self.linear_value(value)\n",
    "      \n",
    "        query = query.view(batch_size, -1, self.num_heads, self.head_size)\n",
    "        key   = key.view(batch_size, -1, self.num_heads, self.head_size)\n",
    "        value = value.view(batch_size, -1, self.num_heads, self.head_size)\n",
    "        \n",
    "        query = query.transpose(1, 2)\n",
    "        key = key.transpose(1, 2)\n",
    "        value = value.transpose(1, 2)\n",
    "             \n",
    "        key = key.transpose(2, 3)\n",
    "        \n",
    "        logits = torch.matmul(query, key)\n",
    "        logits = logits / math.sqrt(self.head_size)\n",
    "\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)#.repeat()\n",
    "            logits.masked_fill_(mask, -1e18)      \n",
    "        \n",
    "        weights = F.softmax(logits, dim = -1)\n",
    "\n",
    "        output = torch.matmul(weights, value)       \n",
    "        output = output.transpose(1,2).contiguous()\n",
    "        output = output.view(batch_size, -1, self.model_size)\n",
    "        output = self.linear_out(output)\n",
    "\n",
    "        return output, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, model_size, ff_size):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        \n",
    "        self.linear_1 = nn.Linear(model_size, ff_size)\n",
    "        self.linear_2 = nn.Linear(ff_size, model_size)\n",
    "        self.layer_norm = nn.LayerNorm(model_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Inputs:\n",
    "            x: (batch_size x seq_len x model-size)\n",
    "        Outputs:\n",
    "            output: \n",
    "        '''        \n",
    "        res = x\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.linear_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        \n",
    "        return x + res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, model_size, num_heads, ff_size):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attention = MultiHeadAttention(num_heads, model_size)\n",
    "        self.positionwise_ff = PositionWiseFeedForward(model_size, ff_size)\n",
    "        self.layer_norm = nn.LayerNorm(model_size)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        '''\n",
    "        Inputs:\n",
    "            x: (batch x seq_len x model_size)\n",
    "            mask: (batch x seq_len x seq_len)\n",
    "            \n",
    "        Outputs:\n",
    "            output : (batch x seq_len, model_size)\n",
    "        '''\n",
    "        \n",
    "        res  = x\n",
    "        x, _ = self.self_attention(x, x, x, mask)       \n",
    "        x    = x + res\n",
    "        x    = self.positionwise_ff(x)\n",
    "        x    = self.layer_norm(x)\n",
    "        \n",
    "        \n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                vocab_size,\n",
    "                num_layers, \n",
    "                model_size, \n",
    "                num_heads,\n",
    "                ff_size,\n",
    "                padding_idx):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        \n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, model_size, padding_idx=padding_idx)\n",
    "    \n",
    "        self.positional_enc = PositionalEncoding(model_size)\n",
    "\n",
    "        self.enc_blocks = nn.ModuleList(\n",
    "                            [TransformerEncoderLayer(model_size, num_heads, ff_size)\n",
    "                                        for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, source):\n",
    "        \n",
    "        '''\n",
    "        Inputs: \n",
    "            source: (batch_size, source_len)\n",
    "            \n",
    "        Outputs:\n",
    "            x: (batch, source_len, hidden)\n",
    "        '''\n",
    "        source_mask = source == self.padding_idx\n",
    "        mask = (source == self.padding_idx).unsqueeze(1).repeat(1, source.size(1), 1)\n",
    "\n",
    "        source_emb = self.embedding(source)\n",
    "        source_emb = self.positional_enc(source_emb)\n",
    "        \n",
    "        x = source_emb\n",
    "        for layer in self.enc_blocks:       \n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return x, source_mask   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class DecoderCell(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, pad_idx):\n",
    "        super(DecoderCell, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=pad_idx)\n",
    "\n",
    "        self.GRU_cell = nn.GRUCell(emb_size, hidden_size)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        self.attention = DotAttentionNoCell()\n",
    "        \n",
    "    def forward(self, hidden, target, mask = None):\n",
    "        \n",
    "        embedded = self.embedding(target)\n",
    "\n",
    "        hidden = self.GRU_cell(embedded, hidden)\n",
    "        \n",
    "        out = self.linear(hidden)\n",
    "        \n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, model_size, num_heads, ff_size):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attention = MultiHeadAttention(num_heads, model_size)\n",
    "        self.positionwise_ff = PositionWiseFeedForward(model_size, ff_size)\n",
    "        self.layer_norm = nn.LayerNorm(model_size)\n",
    "        \n",
    "    def forward(self, x, enc_outputs, enc_mask=None, subseq_mask=None):\n",
    "        '''\n",
    "        Inputs:\n",
    "            x: (batch x target_len x model_size)\n",
    "            enc_outputs: (batch x num_heads x model_size)\n",
    "            enc_mask : (batch x target_len x source_len)\n",
    "            subseq_mask: (batch x target_len x target_len)\n",
    "            \n",
    "        Outputs:\n",
    "            output : (batch x ? x ?)\n",
    "        '''      \n",
    "        res  = x\n",
    "        \n",
    "        x, _ = self.self_attention(x, x, x, subseq_mask)     \n",
    "        x    = x + res       \n",
    "        x    = self.layer_norm(x)       \n",
    "        \n",
    "        res2 = x      \n",
    "        #enc_mask = enc_mask.unsqueeze(1).repeat(1, res.size(1), 1)\n",
    "        x, _ = self.self_attention(x, enc_outputs, enc_outputs, enc_mask)\n",
    "        \n",
    "        x = res2 + x        \n",
    "        x = self.positionwise_ff(x)        \n",
    "        x = self.layer_norm(x)        \n",
    "        \n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, num_layers, pad_idx):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        \n",
    "        self.padding_idx = pad_idx\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=pad_idx)\n",
    "        \n",
    "        self.positional_enc = PositionalEncoding(hidden_size)\n",
    "        \n",
    "        self.dec_blocks = nn.ModuleList(\n",
    "                            [TransformerDecoderLayer(hidden_size, num_heads, ff_size)\n",
    "                                        for _ in range(num_layers)])\n",
    "        self.linear_out = nn.Linear(hidden_size, vocab_size, bias = False)   \n",
    "        \n",
    "    def forward(self, target, enc_outputs, batch_words, val=False):        \n",
    "        '''\n",
    "        Inputs: \n",
    "            source: (batch_size, target_len)\n",
    "            \n",
    "        Outputs:\n",
    "            x: (batch, source_len, hidden)\n",
    "        '''\n",
    "\n",
    "        batch_size, target_len = target.size()\n",
    "        \n",
    "        subseq_mask = torch.triu(\n",
    "                torch.ones((target_len, target_len), device=target.device, dtype=torch.uint8), diagonal=1)\n",
    "        subseq_mask = subseq_mask.unsqueeze(0).expand(batch_size, -1, -1)  # b x ls x ls\n",
    "        \n",
    "        dec_mask_ = (target == 0).unsqueeze(1).repeat(1, target_len, 1)\n",
    "        \n",
    "        dec_mask = (subseq_mask + dec_mask_).gt(0)\n",
    "        \n",
    "        target_emb = self.embedding(target)\n",
    "        target_emb = self.positional_enc(target_emb) \n",
    "        \n",
    "        \n",
    "        enc_mask = batch_words == 0\n",
    "        enc_mask = enc_mask.unsqueeze(1).repeat(1, target_len, 1)\n",
    "        x = target_emb\n",
    "        for layer in self.dec_blocks:       \n",
    "            x = layer(x, enc_outputs, enc_mask, dec_mask)            \n",
    "        \n",
    "        \n",
    "        logits = self.linear_out(x)\n",
    "        \n",
    "        logits = logits.view(-1, self.vocab_size)\n",
    "        \n",
    "        #out = F.softmax(logits)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dataset, encoder, decoder, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        \n",
    "    def forward(self, batch_words, batch_trans_in, source_lens = None, mask = None):\n",
    "        \n",
    "        enc_outputs, _ = self.encoder(batch_words)\n",
    "        out = self.decoder(batch_trans_in, enc_outputs, batch_words, False)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def generate(model, bos_idx, eos_idx, batch_words):\n",
    "        inp = [bos_idx]\n",
    "        enc_outputs, enc_mask = model.encoder(batch_words)\n",
    "        \n",
    "        \n",
    "        for _ in range(100):\n",
    "            inp_tensor = torch.LongTensor([inp]).to(batch_words.device)\n",
    "            logits = model.decoder(inp_tensor, enc_outputs, batch_words, True)\n",
    "            next_token = F.softmax(logits, dim=-1)[-1].topk(1)[1].item()\n",
    "            inp.append(next_token)\n",
    "            if next_token == eos_idx:\n",
    "                break\n",
    "        return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, sos_idx, eos_idx, batch_words):\n",
    "        '''\n",
    "        Inputs:\n",
    "            batch_words: (1 x seq_len)\n",
    "        Outputs:\n",
    "            tokens: example - [45, 30, 122, 4, 8, 5]\n",
    "        '''\n",
    "        outputs, _ = model.encoder(batch_words)\n",
    "        generated_word = [sos_idx]\n",
    "        seq_len = batch_words.size(1)\n",
    "        \n",
    "        for char in range(seq_len):\n",
    "            inp_tensor = torch.LongTensor([generated_word]).to(device)\n",
    "            logits = model.decoder(inp_tensor, outputs, batch_words.to(device))\n",
    "\n",
    "            #logits = logits.squeeze(0)\n",
    "            \n",
    "            logits = F.softmax(logits, dim=-1)\n",
    "            prob_next_token, next_token = logits[-1, :].max(0)\n",
    "            generated_word.append(next_token.item())\n",
    "            if next_token is eos_idx:\n",
    "                break\n",
    "        return generated_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "num_heads   = 8\n",
    "num_layers = 2\n",
    "ff_size = 128\n",
    "\n",
    "encoder = TransformerEncoder(len(words_vocab), num_layers, \n",
    "                hidden_size, \n",
    "                num_heads,\n",
    "                ff_size,\n",
    "                0).to(device)\n",
    "\n",
    "decoder = TransformerDecoder(len(trans_vocab), hidden_size, hidden_size, num_layers, 0).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = Model(data, encoder, decoder, hidden_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, dataset, model, optimizer, criterion, batch_size):\n",
    "        \n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        \n",
    "               \n",
    "    def train(self, n_epochs):\n",
    "        \n",
    "        mask_words = None\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            for batch_idx in range(len(self.dataset)//self.batch_size):\n",
    "\n",
    "                batch_words, batch_trans_in, batch_trans_out, words_lens, trans_lens = self.dataset.get_batch(32, sort = True)\n",
    "                mask_words  = batch_words != 0\n",
    "                \n",
    "                logits = self.model(batch_words, batch_trans_in, words_lens, mask_words)\n",
    "\n",
    "                batch_trans_out = batch_trans_out.view(-1)  \n",
    "                mask = batch_trans_out != self.dataset.trans_vocab.pad_idx\n",
    "                \n",
    "                print(logits.shape)\n",
    "                print(batch_trans_out.shape)\n",
    "                loss = self.criterion(logits[mask], batch_trans_out[mask])\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "\n",
    "                self.train_losses.append(loss.item())\n",
    "                \n",
    "                if batch_idx % 200 == 0:\n",
    "                    val_loss = self.eval_()\n",
    "                    self.val_losses.append(val_loss.item())\n",
    "                    self.plot(epoch, batch_idx, self.train_losses, self.val_losses)\n",
    "                  \n",
    "        \n",
    "    def eval_(self):\n",
    "        \n",
    "        val_words, val_trans_in, val_trans_out, val_words_lens, val_trans_lens = self.dataset.get_batch(32, sort = True, val = True)\n",
    "        val_mask = val_words != 0\n",
    "        logits = self.model(val_words, val_trans_in, val_words_lens, val_mask)\n",
    "        val_trans_out = val_trans_out.view(-1)                \n",
    "\n",
    "        mask = val_trans_out != trans_vocab.pad_idx\n",
    "\n",
    "        loss = self.criterion(logits[mask], val_trans_out[mask])\n",
    "\n",
    "        return loss\n",
    "        \n",
    "    def plot(self, epoch, batch_idx, train_losses, val_losses):\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.subplot(131)\n",
    "        plt.title('epoch %s. | batch: %s | loss: %s' % (epoch, batch_idx, train_losses[-1]))\n",
    "        plt.plot(train_losses)\n",
    "        plt.subplot(132)\n",
    "        plt.title('epoch %s. | loss: %s' % (epoch, val_losses[-1]))\n",
    "        plt.plot(val_losses)\n",
    "        plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(data, model, optimizer, criterion, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([320, 44])\n",
      "torch.Size([320])\n",
      "torch.Size([448, 44])\n",
      "torch.Size([448])\n",
      "torch.Size([480, 44])\n",
      "torch.Size([480])\n",
      "torch.Size([544, 44])\n",
      "torch.Size([544])\n",
      "torch.Size([352, 44])\n",
      "torch.Size([352])\n",
      "torch.Size([352, 44])\n",
      "torch.Size([352])\n",
      "torch.Size([352, 44])\n",
      "torch.Size([352])\n",
      "torch.Size([448, 44])\n",
      "torch.Size([448])\n",
      "torch.Size([448, 44])\n",
      "torch.Size([448])\n",
      "torch.Size([448, 44])\n",
      "torch.Size([448])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([480, 44])\n",
      "torch.Size([480])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([320, 44])\n",
      "torch.Size([320])\n",
      "torch.Size([352, 44])\n",
      "torch.Size([352])\n",
      "torch.Size([320, 44])\n",
      "torch.Size([320])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([448, 44])\n",
      "torch.Size([448])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([448, 44])\n",
      "torch.Size([448])\n",
      "torch.Size([352, 44])\n",
      "torch.Size([352])\n",
      "torch.Size([320, 44])\n",
      "torch.Size([320])\n",
      "torch.Size([352, 44])\n",
      "torch.Size([352])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([512, 44])\n",
      "torch.Size([512])\n",
      "torch.Size([448, 44])\n",
      "torch.Size([448])\n",
      "torch.Size([448, 44])\n",
      "torch.Size([448])\n",
      "torch.Size([320, 44])\n",
      "torch.Size([320])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([320, 44])\n",
      "torch.Size([320])\n",
      "torch.Size([320, 44])\n",
      "torch.Size([320])\n",
      "torch.Size([448, 44])\n",
      "torch.Size([448])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([448, 44])\n",
      "torch.Size([448])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([352, 44])\n",
      "torch.Size([352])\n",
      "torch.Size([320, 44])\n",
      "torch.Size([320])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([352, 44])\n",
      "torch.Size([352])\n",
      "torch.Size([448, 44])\n",
      "torch.Size([448])\n",
      "torch.Size([352, 44])\n",
      "torch.Size([352])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([608, 44])\n",
      "torch.Size([608])\n",
      "torch.Size([320, 44])\n",
      "torch.Size([320])\n",
      "torch.Size([480, 44])\n",
      "torch.Size([480])\n",
      "torch.Size([448, 44])\n",
      "torch.Size([448])\n",
      "torch.Size([448, 44])\n",
      "torch.Size([448])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([352, 44])\n",
      "torch.Size([352])\n",
      "torch.Size([448, 44])\n",
      "torch.Size([448])\n",
      "torch.Size([448, 44])\n",
      "torch.Size([448])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([480, 44])\n",
      "torch.Size([480])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([352, 44])\n",
      "torch.Size([352])\n",
      "torch.Size([320, 44])\n",
      "torch.Size([320])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([352, 44])\n",
      "torch.Size([352])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([448, 44])\n",
      "torch.Size([448])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([352, 44])\n",
      "torch.Size([352])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([352, 44])\n",
      "torch.Size([352])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([352, 44])\n",
      "torch.Size([352])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([352, 44])\n",
      "torch.Size([352])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([352, 44])\n",
      "torch.Size([352])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([448, 44])\n",
      "torch.Size([448])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([352, 44])\n",
      "torch.Size([352])\n",
      "torch.Size([352, 44])\n",
      "torch.Size([352])\n",
      "torch.Size([384, 44])\n",
      "torch.Size([384])\n",
      "torch.Size([416, 44])\n",
      "torch.Size([416])\n",
      "torch.Size([448, 44])\n",
      "torch.Size([448])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b3c9f33c4fbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-8669ea116879>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, n_epochs)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oljike/PycharmProjects/TopicModelling/venv/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def _print(val, model):\n",
    "    batch_words, batch_trans_in, batch_trans_out, words_lens, trans_lens = data.get_batch(1, sort=True, val=val)\n",
    "    batch_words     = batch_words.to(device)\n",
    "    batch_trans_out = batch_trans_out.to(device)\n",
    "\n",
    "\n",
    "    inp = translate(model, data.words_vocab.sos_idx, data.words_vocab.eos_idx, batch_words)\n",
    "            \n",
    "    tokens = [data.trans_vocab.idx2token(idx) for idx in inp if idx not in [data.trans_vocab.sos_idx,\n",
    "                                                                       data.trans_vocab.eos_idx,\n",
    "                                                                       data.trans_vocab.pad_idx]]\n",
    "    print('Src: ', ''.join([data.words_vocab.idx2token(idx) for idx in batch_words[0].tolist()]))\n",
    "    print('Pred:', ''.join(tokens))\n",
    "    print('Real:', ''.join([data.trans_vocab.idx2token(idx) for idx in batch_trans_out[0].tolist() if idx not in [data.trans_vocab.sos_idx,\n",
    "                                                                            data.trans_vocab.eos_idx,\n",
    "                                                                            data.trans_vocab.pad_idx]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    _print(True, model)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
